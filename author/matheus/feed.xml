<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/matheus/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-02-02T23:38:44+01:00</updated>
  <id>http://localhost:4000/author/matheus/feed.xml</id>

  
  
  

  
    <title type="html">Edge AI Guru | </title>
  

  
    <subtitle>The guidance through your Artificial Intelligence journey</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">O impacto da Inteligência Artificial no Controle de Qualidade</title>
      <link href="http://localhost:4000/Inspe%C3%A7%C3%A3o-usando-CV" rel="alternate" type="text/html" title="O impacto da Inteligência Artificial no Controle de Qualidade" />
      <published>2021-02-02T11:15:00+01:00</published>
      <updated>2021-02-02T11:15:00+01:00</updated>
      <id>http://localhost:4000/%20Inspe%C3%A7%C3%A3o%20usando%20CV</id>
      <content type="html" xml:base="http://localhost:4000/Inspe%C3%A7%C3%A3o-usando-CV">&lt;p&gt;Abordamos o impulsionamento da indústria com a integração da IA e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante,&lt;/p&gt;

&lt;p&gt;A Inteligência Artificial vem revolucionando a indústria nos últimos anos e resolvendo problemas, que antes eram onerosos em tempo e dinheiro, de maneira muito mais eficaz. Problemas de visão computacional, processamento de linguagem natural e diversas outras aplicações só são possíveis graças aos diversos avanços na área.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/ai-ml-dl.png&quot; alt=&quot;ai-ml-dl&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A inteligência artificial e suas subáreas.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Inteligência artificial pode ser definida como qualquer método que permite computadores imitar a inteligência humana. Já a parte de Aprendizado de Máquina (Machine Learning) é a subárea de Inteligência Artificial que aprende com os dados utilizando métodos estatísticos sem ser diretamente programada para isso. Parte do Aprendizado de Máquina, o Aprendizado Profundo (Deep Learning) é a área em que programas aprendem sozinhos sobre características importantes sobre os dados, viabilizando tarefas como reconhecimento de padrões em voz ou imagens.&lt;/p&gt;

&lt;p&gt;As Redes Neurais Artificiais (RNA) são um dos principais pilares dessa tecnologia. Inspiradas no cérebro humano, as RNA levam esse nome pois tem conexões e motivações biológicas. Assim como no cérebro humano, onde unidade mais básica de processamento é o neurônio, as RNA possuem um elemento que processa impulsos, ou entradas, e que também é chamado de neurônio ou nó.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inteligência Artificial na Indústria como forma de reduzir custos&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Podemos utilizar todo o espectro da Inteligência Artificial na indústria, desde Machine Learning até Aprendizado Profundo. Com ela, é possível otimizar processos com sistemas de automação inteligentes, aumentar o trabalho humano e impulsionar inovações. Sistemas de predição usando Machine Learning já são encontrados em áreas como manutenção preventiva, segurança de rede, processo de logística, gestão de inventário, gestão de ativos e gestão da cadeia de abastecimento. Por outro lado, recentemente com os avanços dos métodos de aprendizado profundo se tornaram mais comum em aplicações de visão computacional, principalmente no controle de qualidade. Essa tecnologia dá às máquinas sentidos mais humanos, como diferenciar e identificar objetos. Aqui estão alguns exemplos de aplicação de aprendizagem profunda na indústria que otimizam processos já existentes: identificação de produtos com defeito, detecção de anomalias e manutenção preditiva com redes neurais. Quando falamos diretamente de custos para o cliente, modelos são criados com o objetivo de realizar uma tarefa com precisão e também de otimizar um processo que permite a economia de dinheiro aos cofres da empresa. Segundo um &lt;a href=&quot;https://www.accenture.com/pt-pt/insight-ai-industry-growth&quot;&gt;estudo feito pela Accenture Research&lt;/a&gt;, o crescimento da margem de lucro na indústria americana vem diminuindo desde 2010. Isso acontece devido a fatores como como a falta de investimento em inovação e P&amp;amp;D. Por outro lado, a inovação é o que ajudará a indústria a voltar a lucrar. Com o aumento do número de sensores nas fábricas, a queda do preço de poder computacional e avanços em campos coma a Inteligências Artificial, a tecnologia será cada vez mais comum em processos industriais. De fato, na manufatura, a Inteligência Artificial encontra um ambiente favorável para inovar, já que essa área tem precursores como a Internet das Coisas (IoT).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/ind-grow.png&quot; alt=&quot;ind-grow&quot; /&gt;&lt;em&gt;Taxas de crescimento anuais em 2035 do valor bruto acrescentado (aproximadas ao PIB), comparando a linha base de crescimento em 2035 com um cenário de inteligência artificial, onde a IA foi absorvida pela economia. Fonte: Accenture e Frontier Economics&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Normalmente atribuído a humanos, trabalhos repetitivos são os que geram um maior número de erros e muita vezes também são os com maiores riscos. Assim, a substituição da mão de obra humana por processos automatizados ajuda reduzir significativamente o número e erros e os riscos na produção, diminuindo assim os custos. A pesquisa da Accenture ainda mostra que a IA tem potencial para aumentar as taxas de lucratividade em uma média de 38% até 2035 e levar a um aumento econômico de US $ 14 trilhões em 16 setores em 12 economias até 2035. O impacto das tecnologias de IA nos negócios é projetado para aumentar a produtividade do trabalho em até 40 por cento - e permitir que as pessoas façam um uso mais eficiente de seu tempo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aplicação de Deep Learning em Sistemas de Controle de Qualidade:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O uso da tecnologia de visão por máquina mostra-se como um verdadeiro aliado no controle de inspeção de qualidade de produtos em linhas de produção. Durante décadas, existem implementações que possibilitam analisar diversos produtos através de câmeras com algoritmos de processamento de imagem, onde um funcionário escolhe quais características são importantes para o problema, como arestas, cantos, cores, etc. A partir disso, definem-se regras através da extração manual de características que definem se há ou não problema no produto visualizado, tornando-se um método simples e consideravelmente eficaz.&lt;/p&gt;

&lt;p&gt;Com a revolução da Indústria 4.0, o desenvolvimento de sistemas autônomos e automatizados vem ganhando espaço. Dessa forma, foram desenvolvidos algoritmos de &lt;em&gt;Deep Learning&lt;/em&gt;, que é uma ferramenta poderosa que possibilita alavancar os sistemas de inspeção de qualidade. Esse método possui uma característica única: aprender a partir dos próprios dados. O processo utiliza das fotografias capturadas pelas câmeras, e cria regras próprias, customizadas e adaptadas para cada problema, sem necessidade da presença humana.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/process.png&quot; alt=&quot;process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Comparação entre sistemas de visão de máquina existentes e a simplificação através do uso de Deep Learning.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Além disso, essa tecnologia possui a flexibilidade de utilizar modelos treinados em problemas similares como um ponto de início para resolver um novo problema. Dessa forma, é possível otimizar as métricas de precisão e também economizar tempo de treino, visto que é necessário entregar ao modelo menos imagens para o aprendizado. Outro aliado da visão computacional são os algoritmos de detecção de objetos, que também partem de modelos pré-treinados em competições, e são necessários para localizar precisamente defeitos em objetos. Um exemplo de aplicação disso é o controle de qualidade em garrafas de plástico que implementamos na Fox IoT, em que o modelo consegue encontrar precisamente a localização de um rótulo defeituoso e consequentemente classifica-o corretamente.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/cola.png&quot; alt=&quot;cola&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Garrafa com rótulo defeituoso e inferência do algoritmo de inspeção visual.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Além do rótulo defeituoso, nossa inspeção de qualidade também é capaz de identificar quando a garrafa está amassada, sem tampa, sem rótulo ou com preenchimento de líquido incorreto, tornando uma solução completa para a indústria manufatureira. Devido à flexibilidade do serviço, podemos também fazer a contagem  de produtos na linha de produção e abrangir para outras manufaturas, desde integridade de caixas até falhas em confecções de tecidos.&lt;/p&gt;

&lt;p&gt;Assim explicado de forma geral o impulsionamento da indústria com a integração da inteligência artificial e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante, percebe-se que esses processos otimizados geram cada vez mais valor ao cliente final, possibilitando a redução de tempo e custos na produção.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Abordamos o impulsionamento da indústria com a integração da IA e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante,</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Radar Target Generation and Detection</title>
      <link href="http://localhost:4000/Radar" rel="alternate" type="text/html" title="Radar Target Generation and Detection" />
      <published>2020-08-03T12:15:00+02:00</published>
      <updated>2020-08-03T12:15:00+02:00</updated>
      <id>http://localhost:4000/Radar</id>
      <content type="html" xml:base="http://localhost:4000/Radar">&lt;p&gt;In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/radar/layout.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the following Radar Specifications, was defined Range and Velocity of target and generated a FMCW Waveform, simulating signal generation and a moving target. A Range Doppler Response was performed using a 2D FFT and CFAR on the generated RDM Range Doppler Map Generation was implemented to display the target.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Frequency of operation = 77GHz&lt;/li&gt;
  &lt;li&gt;Max Range = 200m&lt;/li&gt;
  &lt;li&gt;Range Resolution = 1 m&lt;/li&gt;
  &lt;li&gt;Max Velocity = 100 m/s&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;implementation-steps-for-the-2d-cfar-process&quot;&gt;Implementation steps for the 2D CFAR process.&lt;/h4&gt;

&lt;p&gt;To implement the 2D CFAR, we have to slide the window through the complete Range Doppler Map. This map was generated by running a 2DFFT on the mixed signal (beat signal) output. As we can see below the output of the 2D FFT is an image that has its response in the range and doppler FFT bins, achieved after converting the axis from bin sizes to range and doppler based on their max values and taking just one side of signal from Range dimension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/radar/2DFFT.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We then choose the number of Training Cells in both the dimensions and the number of Guard Cells in both dimensions around the Cell Under Test (CUT) for accurate estimation. A vector stores the noise level for each iteration on training cells, using a loop such that it slides the CUT across range doppler map by giving margins at the edges for Training and Guard Cells. Summing the signal level and converting the value from logarithmic to linear using db2pow function, we average the summed values for all of the training cells used and convert it back to logarithmic using pow2db. Further adding the offset to it to determine the threshold and comparing the signal under CUT with this threshold, we assign value of 1 if the CUT level is greater than the threshold, or equal to 0 otherwise. Using the RDM as the matrix from the output of 2D FFT for implementing CFAR we achieve the following CA-CFAR result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/radar/CA-CFAR.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;selection-of-training-guard-cells-and-offset&quot;&gt;Selection of Training, Guard cells and offset.&lt;/h4&gt;

&lt;p&gt;​	To slide through the complete Range Doppler Map, the following number of Training Cells in both the dimensions, the number of Guard Cells in both dimensions around the Cell Under Test (CUT) and the Offset threshold by SNR value in dB was chosen (&lt;em&gt;Lines 154~171&lt;/em&gt;). While the training values and the guard cell in the Range dimension was chosen after the project walk through video, the guard cell in the Doppler dimension  and the offset was chosen after some experimentations.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%% CFAR implementation

%Slide Window through the complete Range Doppler Map

% *%TODO* :
%Select the number of Training Cells in both the dimensions.
tr_cells = 10;
td_cells = 8;

% *%TODO* :
%Select the number of Guard Cells in both dimensions around the Cell under 
%test (CUT) for accurate estimation
gr_cells = 4;
gd_cells = 2;

% *%TODO* :
% offset the threshold by SNR value in dB
offset = 1.2;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;steps-taken-to-suppress-the-non-thresholded-cells-at-the-edges&quot;&gt;Steps taken to suppress the non-thresholded cells at the edges.&lt;/h4&gt;

&lt;p&gt;We generate a thresholded block, which is smaller than the Range Doppler Map as the CUT cannot be located at the edges of matrix. While adding the offset to it to determine the threshold and comparing the signal under CUT with this threshold, we assigned values of 1 if the CUT level is greater than the threshold, or equal to 0 otherwise. These few cells will not be thresholded and to keep the map size same we set those values to 0 using &lt;code class=&quot;highlighter-rouge&quot;&gt;RDM(RDM~=0 &amp;amp; RDM~=1) = 0;&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Tracking 3D objects with Lidar and Camera</title>
      <link href="http://localhost:4000/3D-Object-Tracking" rel="alternate" type="text/html" title="Tracking 3D objects with Lidar and Camera" />
      <published>2020-07-23T12:15:00+02:00</published>
      <updated>2020-07-23T12:15:00+02:00</updated>
      <id>http://localhost:4000/3D%20Object%20Tracking</id>
      <content type="html" xml:base="http://localhost:4000/3D-Object-Tracking">&lt;p&gt;Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)&lt;/p&gt;

&lt;p&gt;
    Autonomous vehicles technology has been on my radar (no pun intended) for a while now. I was excited to apply AI to some very complex problems and this project was for sure one of the most amazing I ever did. More than Machine and Deep Learning techniques, solving autonomous driving requires several technologies and methods. All this is only possible through fusing sensors to make sense of the environment.
&lt;/p&gt;
&lt;p&gt;
    To fusion two sensors was a completely new challenge for me and seeing the results is awesome. After some introduction on how camera technology works and how optics are applied in modern systems, some image processing, and computer vision applications were covered,  to better understand how to fusion Lidar and Camera data. From this point on, I tried to apply this knowledge to a collision avoidance project for vehicles. 
&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T2UWLYwpuv4?controls=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/structure.png&quot; alt=&quot;Structure&quot;&gt;&lt;/p&gt;

&lt;i&gt;Project Structure. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;



&lt;p&gt;
    From cameras we detect images key points, such as tail lights, extract these keypoints descriptors and match them between successive images and observe its distances, computing Camera TCC. From Lidar we take a 3D Point Cloud, crop the points(discard road and bad measurements) and cluster these into objects, identified using Neural Networks, and provided with bounding boxes. We than fusion both information, connecting them into space and tracking 3D object bounding boxes to compute the Lidar TCC on the object in front of the car. Conducting various tests with the framework, we tried to identify the most suitable detector/descriptor combination for TTC estimation and also to search for problems that can lead to faulty measurements by the camera or Lidar sensor.
&lt;/p&gt;

&lt;h2&gt;The Technology&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;	
    To make it happen, everything was coded in C++ using the &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti&quot;&gt;KITTI dataset&lt;/a&gt;. Besides, the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;github&lt;/a&gt;. This project is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;.
&lt;/p&gt;
&lt;blockquote cite=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;
    &lt;p&gt;Our recording platform is a Volkswagen Passat B6, which has been modified with actuators for the pedals (acceleration and brake) and the steering wheel. The data is recorded using an eight core i7 computer equipped with a RAID system, running Ubuntu Linux and a real-time database. We use the following sensors:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;1 Inertial Navigation System (GPS/IMU): OXTS RT 3003&lt;/li&gt;
        &lt;li&gt;1 Laserscanner: Velodyne HDL-64E&lt;/li&gt;
        &lt;li&gt;2 Grayscale cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3M-C)&lt;/li&gt;
        &lt;li&gt;2 Color cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3C-C)&lt;/li&gt;
        &lt;li&gt;4 Varifocal lenses, 4-8 mm: Edmund Optics NT59-917&lt;/li&gt;
    &lt;/ul&gt;
&lt;/blockquote&gt;


&lt;br&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/passat_sensors.png&quot; alt=&quot;Passat&quot;&gt;&lt;/p&gt;
&lt;i&gt;KITTI Passat Sensor Setup. Source: &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;





&lt;h2&gt;The Project&lt;/h2&gt;
&lt;br&gt;
&lt;p&gt;
    Cameras are one mature technology and the main sensor in autonomous vehicles. In a world made by humans, everything is adapted to our senses and driving would not be different. Therefore, its main advantage is to &quot;simulate&quot; the human vision being able to interpret 2D information as road signs and lanes. At the same time, it has challenges as we humans: bad performance in darkness or bad weather conditions. Thus, alone in some applications, the camera will fail, hence the need to add a second sensor such as Radar or Lidar. To fill the gap, Lidar has high resolution and can reconstruct 3D objects while Radar has a greater range and can detect velocity more accurately.
&lt;/p&gt;
&lt;p&gt;
    In this project, the main objective was to estimate the Time to Collision(TCC) using a camera-based object classification to cluster Lidar points and from 3D bounding boxes compute TCC. Human reflection time is around 1 second, so the system has to warn us way before and starting breaking 2 to 3 seconds before the collision. But how do we tell a machine a collision is near? How do we compute such time? The first concern is to choose a Motion Model: Constant Velocity(CVM) or Constant Acceleration(CAM). While CAM models best real-world situations, CVM assumes that the velocity does not change and it was chosen for simplicity, leaving space for us to focus more on the computer vision aspects.  
&lt;/p&gt;
&lt;p&gt;
    From that, the problem can be divided into two main parts: estimate TTC from Lidar points and estimate it using camera successive images. The former is pretty straightforward, addressing only the preceding car and calculating the time from the X component in the Point Cloud.  Based on the CVM, we can compute the velocity based in two Lidar measurements over time, taking the closest Lidar point in the path of the car and observing how it changes in a time window. To avoid erroneous measurements that would lead to false TCC, we perform a &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule&quot;&gt;IQR range values&lt;/a&gt; to remove outliers. Due to some measurement accuracy based on the amount of light reflected from an object, when using Lidar for TCC is advised to remove measurement of the road surface and low reflectivity points, making it easy to measure the distance of the preceding vehicle. Furthermore, instead of estimating the velocity after computing the vehicle velocity and two distance measurements, that could be noisy, we could use a Radar sensor that measures directly the relative speed. 
&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/keypoints.jpg&quot; alt=&quot;Keypoints&quot;&gt;&lt;/p&gt;

&lt;i&gt;Keypoints detection and tracking using descriptors. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;    
    In another hand, it is hard to measure metric distances using only one camera, some companies achieve measuring distance using a stereo camera setup, like &lt;a href=&quot;http://www.6d-vision.com/&quot;&gt;Mercedes-Benz which pioneered in this technology&lt;/a&gt;. With two pictures of the same place, taken from two different points of view, is possible to locate points of interest in both images and calculate its distance using geometry and perspective projection. Using a mono camera we can estimate TCC only by observing relative height change (or scale change) in the preceding car, for example, without distance measurement. Here enters Deep Learning, allowing us to identify cars in images. As showed in the figure above, it is also used CV techniques to find key points and track them using descriptors from one frame to the next, estimating the scale change in the object. Tracking how these descriptors change over time, we can estimate TTC.
&lt;/p&gt;
&lt;p&gt;
    First, we focus on loading images, setting up data structures, and putting everything into a ring buffer to optimize memory load. Then, integrated several keypoint detectors such as HARRIS, FAST, BRISK, and SIFT and compare them with regard to the number of key points and speed. For the descriptor extraction and matching, we used brute force and also the FLANN approach and tested the various algorithms in different combinations and compare them with regard to some performance measures. 
    Counting the number of key points on the preceding vehicle for 10 images and taking note of the distribution of their neighborhood size, doing this for several detectors implemented, we noted that FAST, BRISK, and AKASE were the detectors that identified the larger number of keypoints.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/comparision_table.png&quot; alt=&quot;Table&quot;&gt;&lt;/p&gt;
&lt;i&gt;Comparision table between several detectors to identify keypoints&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;
    During the matching step, the Brute Force approach is used with the descriptor distance ratio set to 0.8, and the time it takes for keypoint detection and descriptor extraction was logged. A helper function was created to iterate through all the possible detectors and descriptors, saving its matches and times. From this was identified that the best Detector for this application is FAST, and the best Descriptors are BRIEF and ORB.
&lt;/p&gt;
&lt;p&gt;
    Then we detected and classify images using the YOLO framework, which gives us a set of bounding boxes and augmenting them associating each bounding box to its respective Lidar points. Finally, we track these bounding boxes over time to estimate Time To Collision with Lidar and Camera measurements. When taking into account both TCC, twenty pairs of Detector/Descriptors were analyzed and AZAKE/FREAK was chosen based on the average error when compared to Lidar's TCC.
&lt;/p&gt;
&lt;p&gt;
    To check the complete code with comments, tasks and the results, please check the &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Tracking 3D objects with Lidar and Camera</title>
      <link href="http://localhost:4000/Lex-Friedman" rel="alternate" type="text/html" title="Tracking 3D objects with Lidar and Camera" />
      <published>2020-07-23T12:15:00+02:00</published>
      <updated>2020-07-23T12:15:00+02:00</updated>
      <id>http://localhost:4000/%20Lex%20Friedman</id>
      <content type="html" xml:base="http://localhost:4000/Lex-Friedman">&lt;p&gt;Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)&lt;/p&gt;

&lt;p&gt;
    Autonomous vehicles technology has been on my radar (no pun intended) for a while now. I was excited to apply AI to some very complex problems and this project was for sure one of the most amazing I ever did. More than Machine and Deep Learning techniques, solving autonomous driving requires several technologies and methods. All this is only possible through fusing sensors to make sense of the environment.
&lt;/p&gt;
&lt;p&gt;
    To fusion two sensors was a completely new challenge for me and seeing the results is awesome. After some introduction on how camera technology works and how optics are applied in modern systems, some image processing, and computer vision applications were covered,  to better understand how to fusion Lidar and Camera data. From this point on, I tried to apply this knowledge to a collision avoidance project for vehicles. 
&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T2UWLYwpuv4?controls=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/structure.png&quot; alt=&quot;Structure&quot;&gt;&lt;/p&gt;

&lt;i&gt;Project Structure. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;



&lt;p&gt;
    From cameras we detect images key points, such as tail lights, extract these keypoints descriptors and match them between successive images and observe its distances, computing Camera TCC. From Lidar we take a 3D Point Cloud, crop the points(discard road and bad measurements) and cluster these into objects, identified using Neural Networks, and provided with bounding boxes. We than fusion both information, connecting them into space and tracking 3D object bounding boxes to compute the Lidar TCC on the object in front of the car. Conducting various tests with the framework, we tried to identify the most suitable detector/descriptor combination for TTC estimation and also to search for problems that can lead to faulty measurements by the camera or Lidar sensor.
&lt;/p&gt;

&lt;h2&gt;The Technology&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;	
    To make it happen, everything was coded in C++ using the &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti&quot;&gt;KITTI dataset&lt;/a&gt;. Besides, the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;github&lt;/a&gt;. This project is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;.
&lt;/p&gt;
&lt;blockquote cite=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;
    &lt;p&gt;Our recording platform is a Volkswagen Passat B6, which has been modified with actuators for the pedals (acceleration and brake) and the steering wheel. The data is recorded using an eight core i7 computer equipped with a RAID system, running Ubuntu Linux and a real-time database. We use the following sensors:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;1 Inertial Navigation System (GPS/IMU): OXTS RT 3003&lt;/li&gt;
        &lt;li&gt;1 Laserscanner: Velodyne HDL-64E&lt;/li&gt;
        &lt;li&gt;2 Grayscale cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3M-C)&lt;/li&gt;
        &lt;li&gt;2 Color cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3C-C)&lt;/li&gt;
        &lt;li&gt;4 Varifocal lenses, 4-8 mm: Edmund Optics NT59-917&lt;/li&gt;
    &lt;/ul&gt;
&lt;/blockquote&gt;


&lt;br&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/passat_sensors.png&quot; alt=&quot;Passat&quot;&gt;&lt;/p&gt;
&lt;i&gt;KITTI Passat Sensor Setup. Source: &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;





&lt;h2&gt;The Project&lt;/h2&gt;
&lt;br&gt;
&lt;p&gt;
    Cameras are one mature technology and the main sensor in autonomous vehicles. In a world made by humans, everything is adapted to our senses and driving would not be different. Therefore, its main advantage is to &quot;simulate&quot; the human vision being able to interpret 2D information as road signs and lanes. At the same time, it has challenges as we humans: bad performance in darkness or bad weather conditions. Thus, alone in some applications, the camera will fail, hence the need to add a second sensor such as Radar or Lidar. To fill the gap, Lidar has high resolution and can reconstruct 3D objects while Radar has a greater range and can detect velocity more accurately.
&lt;/p&gt;
&lt;p&gt;
    In this project, the main objective was to estimate the Time to Collision(TCC) using a camera-based object classification to cluster Lidar points and from 3D bounding boxes compute TCC. Human reflection time is around 1 second, so the system has to warn us way before and starting breaking 2 to 3 seconds before the collision. But how do we tell a machine a collision is near? How do we compute such time? The first concern is to choose a Motion Model: Constant Velocity(CVM) or Constant Acceleration(CAM). While CAM models best real-world situations, CVM assumes that the velocity does not change and it was chosen for simplicity, leaving space for us to focus more on the computer vision aspects.  
&lt;/p&gt;
&lt;p&gt;
    From that, the problem can be divided into two main parts: estimate TTC from Lidar points and estimate it using camera successive images. The former is pretty straightforward, addressing only the preceding car and calculating the time from the X component in the Point Cloud.  Based on the CVM, we can compute the velocity based in two Lidar measurements over time, taking the closest Lidar point in the path of the car and observing how it changes in a time window. To avoid erroneous measurements that would lead to false TCC, we perform a &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule&quot;&gt;IQR range values&lt;/a&gt; to remove outliers. Due to some measurement accuracy based on the amount of light reflected from an object, when using Lidar for TCC is advised to remove measurement of the road surface and low reflectivity points, making it easy to measure the distance of the preceding vehicle. Furthermore, instead of estimating the velocity after computing the vehicle velocity and two distance measurements, that could be noisy, we could use a Radar sensor that measures directly the relative speed. 
&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/keypoints.jpg&quot; alt=&quot;Keypoints&quot;&gt;&lt;/p&gt;

&lt;i&gt;Keypoints detection and tracking using descriptors. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;    
    In another hand, it is hard to measure metric distances using only one camera, some companies achieve measuring distance using a stereo camera setup, like &lt;a href=&quot;http://www.6d-vision.com/&quot;&gt;Mercedes-Benz which pioneered in this technology&lt;/a&gt;. With two pictures of the same place, taken from two different points of view, is possible to locate points of interest in both images and calculate its distance using geometry and perspective projection. Using a mono camera we can estimate TCC only by observing relative height change (or scale change) in the preceding car, for example, without distance measurement. Here enters Deep Learning, allowing us to identify cars in images. As showed in the figure above, it is also used CV techniques to find key points and track them using descriptors from one frame to the next, estimating the scale change in the object. Tracking how these descriptors change over time, we can estimate TTC.
&lt;/p&gt;
&lt;p&gt;
    First, we focus on loading images, setting up data structures, and putting everything into a ring buffer to optimize memory load. Then, integrated several keypoint detectors such as HARRIS, FAST, BRISK, and SIFT and compare them with regard to the number of key points and speed. For the descriptor extraction and matching, we used brute force and also the FLANN approach and tested the various algorithms in different combinations and compare them with regard to some performance measures. 
    Counting the number of key points on the preceding vehicle for 10 images and taking note of the distribution of their neighborhood size, doing this for several detectors implemented, we noted that FAST, BRISK, and AKASE were the detectors that identified the larger number of keypoints.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/comparision_table.png&quot; alt=&quot;Table&quot;&gt;&lt;/p&gt;
&lt;i&gt;Comparision table between several detectors to identify keypoints&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;
    During the matching step, the Brute Force approach is used with the descriptor distance ratio set to 0.8, and the time it takes for keypoint detection and descriptor extraction was logged. A helper function was created to iterate through all the possible detectors and descriptors, saving its matches and times. From this was identified that the best Detector for this application is FAST, and the best Descriptors are BRIEF and ORB.
&lt;/p&gt;
&lt;p&gt;
    Then we detected and classify images using the YOLO framework, which gives us a set of bounding boxes and augmenting them associating each bounding box to its respective Lidar points. Finally, we track these bounding boxes over time to estimate Time To Collision with Lidar and Camera measurements. When taking into account both TCC, twenty pairs of Detector/Descriptors were analyzed and AZAKE/FREAK was chosen based on the average error when compared to Lidar's TCC.
&lt;/p&gt;
&lt;p&gt;
    To check the complete code with comments, tasks and the results, please check the &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Lidar Obstacles Detection</title>
      <link href="http://localhost:4000/Lidar-Obstacles-Detection" rel="alternate" type="text/html" title="Lidar Obstacles Detection" />
      <published>2020-07-18T12:15:00+02:00</published>
      <updated>2020-07-18T12:15:00+02:00</updated>
      <id>http://localhost:4000/Lidar%20Obstacles%20Detection</id>
      <content type="html" xml:base="http://localhost:4000/Lidar-Obstacles-Detection">&lt;p&gt;In this project, we create a simple 3d highway enviroment using Point Cloud Library for exploring self-driving car sensors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/obstacledetectionfps.gif&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main objective was to implement three essential algorithms used to identify obstacles using Lidar: RANSAC, KD-Tree, and Euclidean clustering, as these are required as part of the processing pipeline: filtering, segmentation, clustering, and bounding boxes. And use it to detect car and trucks on a narrow street using lidar. Although PCL’s built in segmentation and clustering functions were not used, they were useful for testing.&lt;/p&gt;

&lt;p&gt;In this object detection problem, we were concern about bounding boxes enclosing appropriate objects, as vehicles and the poles, where there is only one box per detected object. And objects being consistently detected across frames in the video. Most bounding boxes can be followed through the lidar stream, and major objects don’t lose or gain bounding boxes in the middle of the lidar stream. Segmentation was also implemented in the project, the code used for segmentation uses the 3D RANSAC algorithm. As well as clustering, using for clustering the Euclidean clustering algorithm along with the KD-Tree.&lt;/p&gt;

&lt;p&gt;For code efficiency, was kept in mind not running the exact same calculation repeatedly when you can run it once, storing the value and then reuse the value later. Avoid loops that run too many times. And the creation of unnecessarily complex data structures when simpler structures work equivalently.&lt;/p&gt;

&lt;p&gt;C++ was used in this whole project, which is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt; and the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/Lidar_Obstacle_Detection&quot;&gt;Github&lt;/a&gt;. Images and text are credit to &lt;a href=&quot;https://github.com/awbrown90&quot;&gt;Aaron Brown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.cpp&lt;/code&gt; we can found the main function, with the city block creator and how camera should start. At &lt;code class=&quot;highlighter-rouge&quot;&gt;processPointClouds.cpp&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;processPointClouds.h&lt;/code&gt; all the necessary function are implemented:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;filtering the cloud, with voxel grid point reduction and region based filtering&lt;/li&gt;
  &lt;li&gt;separating cloud, one cloud with obstacles and other with segmented plane&lt;/li&gt;
  &lt;li&gt;segmenting plane, applying RANSAC, find inliers and segment the largest planar component&lt;/li&gt;
  &lt;li&gt;clustering, performing euclidean clustering to group detected obstacles, creating the KdTree object for the search method of the extraction&lt;/li&gt;
  &lt;li&gt;findind bounding box for one of the clusters&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementing-ransac&quot;&gt;Implementing RANSAC&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac-linie-animiert.gif&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RANSAC stands fors Randomly Sample Consensus, and is a method for detecting outliers in data. RANSAC runs a for a max number of iterations, and returns the model with the best fit. Apart from the 3D RANSAC implemented in this project, a  RANSAC for fitting a line in 2D point data with outliers were also tested. The code is located in &lt;code class=&quot;highlighter-rouge&quot;&gt;src/quiz/ransac2d.cpp&lt;/code&gt; and the function to fill out is &lt;code class=&quot;highlighter-rouge&quot;&gt;Ransac&lt;/code&gt; which takes in arguments for a point cloud, max iterations to run, and distance tolerance. The point cloud is actually &lt;code class=&quot;highlighter-rouge&quot;&gt;pcl::PointXYZ&lt;/code&gt; but the z component will be set to zero to make things easy to visualize. The image below shows how the data looks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac2d.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The data was generated by creating a line with points slightly scattered, and then outliers were added by randomly placing points in the scene. We want to be able to identify which points belong to the line that was originally generated and which points are outliers. To do this you will randomly sample two points from the cloud and fit a line between the points. A helpful line equation for this problem can be seen below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ax + By + C = 0
for point1 (x1, y1), and point2 (x2, y2)
(y1  y2)x + (x2  x1)y + (x1y2  x2y1) = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After fitting the line you can then iterate through all the points and determine if they are inliers by measuring how far away the point is from the line. You can do this for each iteration keeping track of which fitted line had the highest number of inliers, the line with the most inliers will be the best model. The equation for calculating distance between a point and line is shown below. For further details see &lt;a href=&quot;https://brilliant.org/wiki/dot-product-distance-between-point-and-a-line/&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;line is Ax + By + C = 0
point (x,y)
d = |A*x+B*y+C|/sqrt(A^2+B^2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the results of doing RANSAC to fit a line from the data above. Inliers are green while outliers are red, the function had a max iteration size of 50, and a distance tolerance of 0.5. The max iteration size depends on the ratio of inliers to the total number of points. The more inliers our data contains the higher the probability of selecting inliers to fit the line to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac2dfitted.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The method above was for fitting a line, you can do the same thing for fitting plane in a 3D point cloud by using the equation for a plane from three points, and the distance formula for a point to a plane.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plane is Ax + By + Cz + D = 0
for point1 (x1, y1, z1)
for point2 (x2, y2, z2)
for point3 (x3, y3, z3)

Use point1 as reference and define two vectors on the plane v1, and v2

vector v1 travels from point1 to point2
vector v2 travels from point1 to point3

v1 = &amp;lt; x2 - x1, y2 - y1, z2 - z1 &amp;gt;
v2 = &amp;lt; x3 - x1, y3 - y1, z3 - z1 &amp;gt;

Find normal vector of plane by taking cross product of v1 x v2.

v1 x v2 = &amp;lt;(y2-y1)(z3-z1)-(z2-z1)(y3-y1), (z2-z1)(x3-x1)-(x2-x1)(z3-z1), (x2-x1)(y3-y1)-(y2-y1)(x3-x1)&amp;gt;

To simplify notation we can write it in the form 
v1 x v2 = &amp;lt; i, j, k &amp;gt;

then ,

i(x-x1)+j(y-y1)+k(z-z1) = 0,
ix + jy + kz -( ix1 + jy1 + kz1 ) = 0

A = i
B = j
C = k
D = -( ix1 + jy1 + kz1 )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the distance formula from a point to the plane is then,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plane is Ax + By + Cz + D = 0
point (x,y,z)
d = |A*x+B*y+C*z+D|/sqrt(A^2+B^2+C^2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;implementing-kd-tree-and-euclidean-clustering&quot;&gt;Implementing KD-Tree and Euclidean Clustering&lt;/h2&gt;

&lt;p&gt;A KD-Tree is a binary tree that splits points between alternating axes. By seperating space by splitting regions, it can make it much faster to do nearest neighbor search when using an algorithm like euclidean clustering. Here we will be looking at a 2D example, so the the tree will be a 2D-Tree. The code can be found at &lt;code class=&quot;highlighter-rouge&quot;&gt;src/quiz/cluster/kdtree.h&lt;/code&gt; and using the function &lt;code class=&quot;highlighter-rouge&quot;&gt;insert&lt;/code&gt; which take a 2D point represented by a vector of 2 floats, and a point ID, this is just a way to uniquely identify points and a way to tell which index they are from the overall point cloud. To understand the &lt;code class=&quot;highlighter-rouge&quot;&gt;insert&lt;/code&gt; function let’s first talk about how a KD-Tree splits information.&lt;/p&gt;

&lt;h3 id=&quot;inserting-points-into-the-tree&quot;&gt;Inserting Points into the Tree&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/2dpoints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows what the 2d points look like, in this simple example there is only 11 points, and there is 3 clusters where points are in close proximity to each other that you will be finding. The image below shows what the tree looks like after all 11 points have been inserted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now lets talk about how exactly the tree is created. At the very beginning when the tree is empty, root is NULL, the point inserted becomes the root, and splits the x region. Here is what this visually looks like, after inserting the first point (-6.2, 7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree1.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next point is (-6.3,8.4), since -6.3 is less than -6.2 this Node will be created and be apart of root’s left node, and now the point (-6.3,8.4) will split the region in the y dimension. The root was at depth 0, and split the x region, the next point become the left child of root and had a depth of 1, and split the y region. A point at depth 2 will split the x region again, so the split can actually be calculated as depth % 2, where 2 is the number of dimensions we are working with. The image below show how the tree looks after inserting the second point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree2.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then here is what the tree looks like after inserting two more points (-5.2,7.1), (-5.7,6.3), and having another x split division from point (-5.7,6.3) being at depth 2 in the tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree4.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image below shows so far what the tree looks like after inserting those 4 points. The labeled nodes A, B, C, D, and E are all NULL but if the next point (7.2,6.1) is inserted, whill of those 5 nodes will it be assigned to ?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree5.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The answer is D. Let’s look at why this is. First the root (-6.2, 7) and the point(7.2, 6.1) x region will be compared. 7.2 is greater than -6.2 so the new point will branch off to the right to (-5.2, 7.1). Next the y region will be compared, 6.1 is less than 7.1 so the new point will branch off to the left to (-5.7,6.3). Last the x region will be compared again, 7.2 is greater than -5.7 so the new point will branch to the right and will be Node D.&lt;/p&gt;

&lt;p&gt;Logically this is how points are inserted, how about doing this in C++? Implementing a recursive helper function to insert points can be a very nice way to update Nodes. The basic idea is the tree is traversed until the Node is arrives at is NULL in which case a new Node is created and assigned at that current NULL Node. For assinging a Node, one idea is to use a double pointer, you could pass in a pointer to the node, starting at root, and then when you want to assign that node, derefrence pointer and assign it to the newly created Node. Another way of achieving this is by using a pointer reference as well.&lt;/p&gt;

&lt;h3 id=&quot;improving-the-tree-structure&quot;&gt;Improving the Tree Structure&lt;/h3&gt;

&lt;p&gt;Having a balanced tree that evenly splits regions improves the search time for finding points later. To improve the tree insert points that alternate between splitting the x region and the y region. To do this pick the median of sorted x and y points. For instance if you are inserting the first four points that we used above (-6.3,8.4),(-6.2,7),(-5.2,7.1),(-5.7,6.3) we would first insert (-5.2,7.1) since its the median for the x points, if there is an even number of elements the lower median is chosen. The next point to be insorted would be (-6.2,7) the median of the three points for y. Followed by (-5.7,6.3) the lower median between the two for x, and then finally (-6.3,8.4). This ordering will allow the tree to more evenly split the region space and improving searching later.&lt;/p&gt;

&lt;h4 id=&quot;searching-nearby-points-in-the-tree&quot;&gt;Searching Nearby Points in the Tree&lt;/h4&gt;

&lt;p&gt;Once points are able to be inserted into the tree, the next step is being able to search for nearby points (points within a distance of distanceTol) inside the tree compared to a given pivot point. The kd-tree is able to split regions and allows certain regions to be completly ruled out, speeding up the process of finding nearby neighbors. The naive approach of finding nearby neighbors is to go through every single point in the tree and compare its distance with the pivots, selecting point indices that fall with in the distance tolerance. Instead with the kd-tree we can compare distance within a boxed square that is 2 x distanceTol for length, centered around the pivot point. If the current node point is within this box then we can directly calculate the distance and see if we add it to the list of &lt;code class=&quot;highlighter-rouge&quot;&gt;ids&lt;/code&gt;. Then we see if our box crosses over the node division region and if it does compare that next node. We do this recursively, with the advantage being that if the box region is not inside some division region we completly skip that branch.&lt;/p&gt;

&lt;h3 id=&quot;euclidean-clustering&quot;&gt;Euclidean Clustering&lt;/h3&gt;

&lt;p&gt;Once the kd-tree method for searching for nearby points is implemented, its not difficult to implement a euclidean clustering method that groups indidual cluster indicies based on their proximity. Inside &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster.cpp&lt;/code&gt; there is a function called &lt;code class=&quot;highlighter-rouge&quot;&gt;euclideanCluster&lt;/code&gt; which returns a vector of vector ints, this is the list of each cluster’s indices. To perform the clustering, iterate through each point in the cloud and keep track of which points have been processed already. For each point add it to a cluster group then get a list of all the points in proximity to that point. For each point in proximity if it hasn’t already been processed add it to the cluster group and repeat the process of calling proximity points. Once the recursion stops for the first cluster group, create a new cluster and move through the point list. Once all the points have been processed there will be a certain number of cluster groups found.&lt;/p&gt;

&lt;p&gt;There are three clusters found, using a distance tolerance of 3.0. Each cluster group is colored a differently, red, green, and blue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/clusterkdtree.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we create a simple 3d highway enviroment using Point Cloud Library for exploring self-driving car sensors.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Image Captioning</title>
      <link href="http://localhost:4000/Image-Captioning" rel="alternate" type="text/html" title="Image Captioning" />
      <published>2020-07-07T12:15:00+02:00</published>
      <updated>2020-07-07T12:15:00+02:00</updated>
      <id>http://localhost:4000/Image%20Captioning</id>
      <content type="html" xml:base="http://localhost:4000/Image-Captioning">&lt;p&gt;In this project, we used a dataset of image-caption pairs to train a CNN-RNN model to automatically generate captions from images.&lt;/p&gt;

&lt;p&gt;The Dataset&lt;/p&gt;

&lt;p&gt;Downloaded using  &lt;a href=&quot;https://github.com/cocodataset/cocoapi&quot;&gt;COCO API&lt;/a&gt;, the Microsoft &lt;strong&gt;C&lt;/strong&gt;ommon &lt;strong&gt;O&lt;/strong&gt;bjects in &lt;strong&gt;CO&lt;/strong&gt;ntext (MS COCO) dataset is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms. Furthermore you can find more about the dataset on the &lt;a href=&quot;http://cocodataset.org/#home&quot;&gt;website&lt;/a&gt; or in the &lt;a href=&quot;https://arxiv.org/pdf/1405.0312.pdf&quot;&gt;research paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/img-captioning/coco-examples.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we used a dataset of image-caption pairs to train a CNN-RNN model to automatically generate captions from images.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Landmark Detection &amp;amp; Robot Tracking (SLAM)</title>
      <link href="http://localhost:4000/Slam" rel="alternate" type="text/html" title="Landmark Detection &amp; Robot Tracking (SLAM)" />
      <published>2020-06-25T12:15:00+02:00</published>
      <updated>2020-06-25T12:15:00+02:00</updated>
      <id>http://localhost:4000/Slam</id>
      <content type="html" xml:base="http://localhost:4000/Slam">&lt;p&gt;In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.&lt;/p&gt;

&lt;h2 id=&quot;simultaneous-localization-and-mapping&quot;&gt;Simultaneous Localization and Mapping&lt;/h2&gt;

&lt;p&gt;SLAM gives us a way to both localize a robot and build up a map of its environment as a robot moves and senses in real-time. This is an active area of research in the fields of robotics and autonomous systems. Since this localization and map-building relies on the visual sensing of landmarks, this is also a computer vision problem.&lt;/p&gt;

&lt;p&gt;To perform SLAM, we’ll collect a series of robot sensor measurements and motions, in that order, over a defined period of time. Then we’ll use only this data to re-construct the map of the world with the robot and landmark locations. Robot motion and sensors have some uncertainty associated with them. For example, imagine a car driving up hill and down hill; the speedometer reading will likely overestimate the speed of the car going up hill and underestimate the speed of the car going down hill because it cannot perfectly account for gravity. Similarly, we cannot perfectly predict the &lt;em&gt;motion&lt;/em&gt; of a robot. A robot is likely to slightly overshoot or undershoot a target location.&lt;/p&gt;

&lt;p&gt;First, we create a robot and move it around a 2D grid world and then, define a &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function for this robot that allows it to sense landmarks in a given world. It’s important that you understand how this robot moves, senses, and how it keeps track of different landmarks that it sees in a 2D grid world, so that you can work with it’s movement and sensor data.&lt;/p&gt;

&lt;p&gt;This project is part of the &lt;a href=&quot;https://www.udacity.com/course/computer-vision-nanodegree--nd891&quot;&gt;Udacity Computer Vision Nanodegree&lt;/a&gt; and all code can be found in &lt;a href=&quot;https://github.com/jacquesmats/slam_implementation&quot;&gt;my Github&lt;/a&gt;.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;  in robot class takes in a number of parameters including a world size and some values that indicate the sensing and movement capabilities of the robot. Furthermore, using the visualization function, &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt;, that will display a grid world in a plot and draw a red &lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt; at the location of our robot, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;. The details of how this function works can be found in the &lt;code class=&quot;highlighter-rouge&quot;&gt;helpers.py&lt;/code&gt; file in the home directory. In the following image, we define a small 10x10 square world, a measurement range that is half that of the world and small values for motion and measurement noise. These values will typically be about 10 times larger, but we want to demonstrate this behavior on a small scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/gridworld.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s create landmarks, which are measurable features in the map. You can think of landmarks as things like notable buildings, or something smaller such as a tree, rock, or other feature. The robot class has a function &lt;code class=&quot;highlighter-rouge&quot;&gt;make_landmarks&lt;/code&gt; which randomly generates locations for the number of specified landmarks. We have to pass these locations as a third argument to the &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt; function and the list of landmark locations is accessed similar to how we find the robot position &lt;code class=&quot;highlighter-rouge&quot;&gt;r.landmarks&lt;/code&gt;.  Each landmark is displayed as a purple &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; in the grid world, and we also print out the exact &lt;code class=&quot;highlighter-rouge&quot;&gt;[x, y]&lt;/code&gt; locations of these landmarks at the end of this cell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/landmark.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once we have some landmarks to sense, we need to be able to tell our robot to &lt;em&gt;try&lt;/em&gt; to sense how far they are away from it. It will be up t you to code the &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function in our robot class. The &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function uses only internal class parameters and returns a list of the the measured/sensed x and y distances to the landmarks it senses within the specified &lt;code class=&quot;highlighter-rouge&quot;&gt;measurement_range&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;''' This function does not take in any parameters, instead it references internal variables
            (such as self.landamrks) to measure the distance between the robot and any landmarks
            that the robot can see (that are within its measurement range).
            This function returns a list of landmark indices, and the measured distances (dx, dy)
            between the robot's position and said landmarks.
            This function should account for measurement_noise and measurement_range.
            One item in the returned list should be in the form: [landmark_index, dx, dy].
            '''&lt;/span&gt;
           
        &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;## Iterate through all of the landmarks in a world&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;                 
            &lt;span class=&quot;c&quot;&gt;## For each landmark&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 1. compute dx and dy, the distances between the robot and the landmark&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 2. account for measurement noise by *adding* a noise component to dx and dy&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;##    then we cannot record them; if they do fall in the range, then add them to the measurements list&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;##    as list.append([index, dx, dy]), this format is important for data creation done later&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;land_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;  
            
            &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dx: {}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; dy: {}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; range: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;omega-and-xi&quot;&gt;Omega and Xi&lt;/h2&gt;

&lt;p&gt;To implement Graph SLAM, a matrix and a vector (omega and xi, respectively) are introduced. The matrix is square and labeled with all the robot poses (xi) and all the landmarks (Li). Every time you make an observation, for example, as you move between two poses by some distance &lt;code class=&quot;highlighter-rouge&quot;&gt;dx&lt;/code&gt; and can relate those two positions, you can represent this as a numerical relationship in these matrices.&lt;/p&gt;

&lt;p&gt;It’s easiest to see how these work in an example. Below you can see a matrix representation of omega and a vector representation of xi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/omega_xi.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s look at a simple example that relates 3 poses to one another.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When you start out in the world most of these values are zeros or contain only values from the initial robot position&lt;/li&gt;
  &lt;li&gt;In this example, you have been given constraints, which relate these poses to one another&lt;/li&gt;
  &lt;li&gt;Constraints translate into matrix values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/omega_xi_constraints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have ever solved linear systems of equations before, this may look familiar.&lt;/p&gt;

&lt;h3 id=&quot;solving-for-x&quot;&gt;Solving for x&lt;/h3&gt;

&lt;p&gt;To “solve” for all these x values, we can use linear algebra; all the values of x are in the vector &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; which can be calculated as a product of the inverse of omega times xi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/solution.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;motion-constraints-and-landmarks&quot;&gt;Motion Constraints and Landmarks&lt;/h2&gt;

&lt;p&gt;Now, let’s look at how motion (and similarly, sensor measurements) can be used to create constraints and fill up the constraint matrices, omega and xi. Let’s start with empty/zero matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/initial_constraints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This example also includes relationships between poses and landmarks. Say we move from x0 to x1 with a displacement &lt;code class=&quot;highlighter-rouge&quot;&gt;dx&lt;/code&gt; of 5. Then we have created a motion constraint that relates x0 to x1, and we can start to fill up these matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/motion_constraint.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, the one constraint equation can be written in two ways. So, the motion constraint that relates x0 and x1 by the motion of 5 has affected the matrix, adding values for &lt;em&gt;all&lt;/em&gt; elements that correspond to x0 and x1.&lt;/p&gt;

&lt;h2 id=&quot;generating-an-environment&quot;&gt;Generating an environment&lt;/h2&gt;

&lt;p&gt;In a real SLAM problem, one may be given a map that contains information about landmark locations, and in this project, we will make our own data using the &lt;code class=&quot;highlighter-rouge&quot;&gt;make_data&lt;/code&gt; function, which generates a world grid with landmarks in it and then generates data by placing a robot in that world and moving and sensing over some number of time steps.  The data is collected as an instantiated robot moves and senses in a world. The SLAM function will take in this data as input. So, first we create this data and explore how it represents the movement and sensor measurements that our robot takes.&lt;/p&gt;

&lt;h3 id=&quot;create-the-world&quot;&gt;Create the world&lt;/h3&gt;

&lt;p&gt;We used the &lt;code class=&quot;highlighter-rouge&quot;&gt;make_data&lt;/code&gt; function to generate a world of a specified size with randomly generated landmark locations. &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; holds the sensors measurements and motion of your robot over time. It stores the measurements as &lt;code class=&quot;highlighter-rouge&quot;&gt;data[i][0]&lt;/code&gt; and the motion as &lt;code class=&quot;highlighter-rouge&quot;&gt;data[i][1]&lt;/code&gt;. One of the most challenging tasks were to create and modify the constraint matrix and vector: omega and xi. In the images above, you saw an example of how omega and xi could hold all the values the define the relationships between robot poses &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; and landmark positions &lt;code class=&quot;highlighter-rouge&quot;&gt;Li&lt;/code&gt; in a 1D world.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;this&lt;/em&gt; project, we implemented constraints for a 2D world. We are referring to robot poses as &lt;code class=&quot;highlighter-rouge&quot;&gt;Px, Py&lt;/code&gt; and landmark positions as &lt;code class=&quot;highlighter-rouge&quot;&gt;Lx, Ly&lt;/code&gt;, and one way to approach this challenge is to add &lt;em&gt;both&lt;/em&gt; x and y locations in the constraint matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/constraints2D.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;slam-inputs&quot;&gt;SLAM inputs&lt;/h3&gt;

&lt;p&gt;In addition to &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt;, the slam function will also take in:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;N -   The number of time steps that a robot will be moving and sensing&lt;/li&gt;
  &lt;li&gt;num_landmarks - The number of landmarks in the world&lt;/li&gt;
  &lt;li&gt;world_size - The size (w/h) of the world&lt;/li&gt;
  &lt;li&gt;motion_noise - The noise associated with motion; the update confidence for motion should be &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/motion_noise&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;measurement_noise - The noise associated with measurement/sensing; the update weight for measurement should be &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/measurement_noise&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recall that &lt;code class=&quot;highlighter-rouge&quot;&gt;omega&lt;/code&gt; holds the relative “strengths” or weights for each position variable, and you can update these weights by accessing the correct index in omega &lt;code class=&quot;highlighter-rouge&quot;&gt;omega[row][col]&lt;/code&gt; and &lt;em&gt;adding/subtracting&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/noise&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;noise&lt;/code&gt; is measurement or motion noise. &lt;code class=&quot;highlighter-rouge&quot;&gt;Xi&lt;/code&gt; holds actual position values, and so to update &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; you’ll do a similar addition process only using the actual value of a motion or measurement. So for a vector index &lt;code class=&quot;highlighter-rouge&quot;&gt;xi[row][0]&lt;/code&gt; you will end up adding/subtracting one measurement or motion divided by their respective &lt;code class=&quot;highlighter-rouge&quot;&gt;noise&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;implement-graph-slam&quot;&gt;Implement Graph SLAM&lt;/h3&gt;

&lt;p&gt;With a 2D omega and xi structure as shown above, we have to be mindful about how you update the values in these constraint matrices to account for motion and measurement constraints in the x and y directions. Recall that the solution to these matrices (which holds all values for robot poses &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; and landmark locations &lt;code class=&quot;highlighter-rouge&quot;&gt;L&lt;/code&gt;) is the vector, &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt;, which can be computed at the end of the construction of omega and xi as the inverse of omega times xi.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## slam takes in 6 arguments and returns mu, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;## Use initilization to create constraint matrices, omega and xi&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_constraints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Which index starts the Landmarks?&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;first_landm_at&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;## Iterate through each time step in the data&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## get all the motion and measurement data as it iterates&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## update the constraint matrix/vector to account for all *measurements*&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## this should be a series of additions that take into account the measurement noise &lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;    

            &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            
            &lt;span class=&quot;c&quot;&gt;### MAIN DIAGONAL&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# X and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Lx and Ly&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;### OTHER DIAGONAL&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Lx and X&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Ly and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;                     
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;



            &lt;span class=&quot;c&quot;&gt;# Xi for X and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Xi for Lx and Ly&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
                    
    &lt;span class=&quot;c&quot;&gt;## update the constraint matrix/vector to account for all *motion* and motion noise           &lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;#X_current&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# X_next&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Y_current&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#Y_next&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;## After iterating through all the data&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## Compute the best estimate of poses and landmark positions&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## using the formula, omega_inverse * Xi&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;omega_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;omega_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;run-slam&quot;&gt;Run SLAM&lt;/h2&gt;

&lt;p&gt;Once we completed the implementation of &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;, lets check what &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; it returns for different world sizes and different landmarks! We expect the robot to start with an estimated pose in the very center of your square world, whose size is defined by &lt;code class=&quot;highlighter-rouge&quot;&gt;world_size&lt;/code&gt;. We specify the number, &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;, the time steps that the robot was expected to move and the &lt;code class=&quot;highlighter-rouge&quot;&gt;num_landmarks&lt;/code&gt; in the world (which your implementation of &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;) should see and estimate a position for.&lt;/p&gt;

&lt;p&gt;With these values in mind, you should expect to see a result that displays two lists:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Estimated poses&lt;/strong&gt;, a list of (x, y) pairs that is exactly &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; in length since this is how many motions your robot has taken. The very first pose should be the center of your world, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;[50.000, 50.000]&lt;/code&gt; for a world that is 100.0 in square size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Estimated landmarks&lt;/strong&gt;, a list of landmark positions (x, y) that is exactly &lt;code class=&quot;highlighter-rouge&quot;&gt;num_landmarks&lt;/code&gt; in length.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For visualizing the constructed world, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt; code from the &lt;code class=&quot;highlighter-rouge&quot;&gt;helpers.py&lt;/code&gt; file (which was also used in the first notebook), we can actually visualize what you have coded with &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;: the final position of the robot and the positon of landmarks, created from only motion and measurement data!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/fullgrid.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To confirm that the slam code works, we run it on some test data and cases. The output should be &lt;strong&gt;close-to or exactly&lt;/strong&gt; identical to the given results. If there are minor discrepancies it could be a matter of floating point accuracy or in the calculation of the inverse matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/final_result.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Gradient Descent, Feedforward and Backpropagation</title>
      <link href="http://localhost:4000/Feedforward-and-Backpropagation" rel="alternate" type="text/html" title="Gradient Descent, Feedforward and Backpropagation" />
      <published>2020-06-15T12:15:00+02:00</published>
      <updated>2020-06-15T12:15:00+02:00</updated>
      <id>http://localhost:4000/Feedforward%20and%20Backpropagation</id>
      <content type="html" xml:base="http://localhost:4000/Feedforward-and-Backpropagation">&lt;p&gt;Let’s see three fundamentals concepts of Deep Learning: the Gradient Descent, Feedforward and Backpropagation steps.&lt;/p&gt;

&lt;p&gt;Versão em português disponível em 26 de Junho &lt;a href=&quot;http://edgeaiguru.com/404&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/pre-cover.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;Gradient Descent(GD)  in one of the most common algorithms that help the Neural Networks to reach the correct values of weights and bias. To illustrate this method, let’s take a look at a Logistic Regression model. In this model, we have a loss of L(ŷ,y), where it defines how far the predicted output is from the correct output. If we sum all the losses during the training phase and average them, we will get the Cost Function J(w,b).  What is usually used for regression problems is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/log-reg-loss-function.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since we want to train our model to be as much accurate as possible, it is natural that this method tries to minimize the Cost Function J(W,b) in each step. So the cost function measures how well the parameters W and b are performing in the training set, iteratively updating the weights and bias trying to reach the global minimum in such function. And what is a global minimum? As shown in the figure below, the global minimum of a function J(theta1,theta2) is the values of theta1 and theta2 that produce the minimum value on J. Here J is equivalent to the height of the plot. Notice that there is the so-called local minimum, which sometimes tricks the model making it believe it has achieved the global minimum. One can be seen at theta1 and theta2 equals 0.8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/gradient-descent.png&quot; alt=&quot;Gradient descent to minimize cost function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Minimizing the Cost Function, a Gradient Descent Illustration. Source: &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Stanford’s Andrew Ng’s MOOC Machine Learning Course&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So, the minimum is dependent on the cost function J chosen, and to achieve it, we must first define some starting point for W and b and try to go downhill in the direction of the steepest descent in the function. To make it easy to understand, let us take a look at just one variable. If we are trying to minimize the function J(w), as shown below, and we initialize W with a very high value on the X-axis, what are the next steps?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/cost-function-j-w-1.png&quot; alt=&quot;Cost Function 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To update the weight W and the bias b, the gradient descent rule is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/f1.png&quot; alt=&quot;math1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To move it to the minimum, we first need to know which direction we should follow. Should we increase or decrease W? This can be achieved with the derivative of the point. A derivative is a calculus tool that is equal to the slope of the function at a given point. In other words, it tells us exactly what we need to know: the direction we should follow. When the function is increasing the derivative is positive, when the function is decreasing it is negative and when it is zero it means that it reached a local or global minimum. In the following figure, the derivative in this point is positive and so the dW value will be subtracted from W, in the formula above, and we are going to move left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/cost-function-j-w-2.png&quot; alt=&quot;Cost Function 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, if W was a low number on the left side of the figure, the derivative would be negative and the number dW would be added to W, moving it to the right. The alpha term is called Learning Rate and it defines how big those steps would be. Defining it too big could make the function pass through the local minimum and start to diverge.&lt;/p&gt;

&lt;p&gt;This is a simple convex function analyzed in a 2D graph. Although the concepts and steps remain the same, depending on the problem these functions could get pretty complex and have multiple dimensions, and it is even harder to visualize it. As you may see later, the Gradient Descent is just one of several Optimization Algorithms such as Adagrad, Adadelta, RMSProp, Momentum, and Adam.&lt;/p&gt;

&lt;p&gt;So now we have an optimization algorithm to update W and b values to get our model output closer to the expected output. Through the &lt;strong&gt;Forward Propagation&lt;/strong&gt;, we will pass through the model and calculate an output value (the prediction), compare this value with the expected output, and find an error. With this, we will transmit this error backward using &lt;strong&gt;Backward Propagation&lt;/strong&gt; and with the gradients (derivative values) to update the values for W and b, making the whole model closer to the cost function’s global minimum.&lt;/p&gt;

&lt;h2 id=&quot;feedforward&quot;&gt;Feedforward&lt;/h2&gt;

&lt;p&gt;It is the phase we calculate the model output, so we can understand how accurate it is. For illustration, lets take a network with just one hidden layer. In practice we may have several hidden layers and multiple output values. We will have to calculate the hidden states and then find the output values based on this states. Here the hidden layer and the outputs are matrices and we denote the layer by adding square brackets, i.e. [1] it’s the first layer. To calculate the predict output (ŷ) in the following 2 layers network, we must first find hidden values here called z.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/feedforward.png&quot; alt=&quot;Feedforward&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each hidden neuron is divide into two operations: the linear function y = x*w + b and the application of an activation function sigma. These steps are named s and z, respectively. In this example, we are going to set b, or bias, to zero to simplify the explanation. We use activation functions to make sure these values do not increase too much. It will make sure the outputs are between 0 and 1 or -1 and 1 (check this post for more about activation functions). Furthermore, these functions allow the network to represent nonlinear relationships between inputs and outputs.&lt;/p&gt;

&lt;p&gt;So initializing randomly the weights W and choosing TanH as an activation function sigma, we have:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/random-weights-sigma.png&quot; alt=&quot;image-20200614105546474&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will assume that in this particular training step, we are inputing x1 and x2 equals 1 and are expecting the model to output 1 as well. As state before, the first step is to calculate the result of the linear function and find s1 and s2 from the first layer [1]. Then, the results are passed through an activation function to serve as inputs for the next layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/s-and-z-output.png&quot; alt=&quot;image-20200614112302494&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With z1 and z2 in hands, we can now calculate the input from the next layer, in this case, the output layer. Multiplying these values by the weights and passing through the tanh activation function, we finally find our predicted output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/predicted-output.png&quot; alt=&quot;image-20200614112507065&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the output layer, sometimes we use different activation functions, or none, depending on the application. If it was a classification problem, we could use a Softmax here.&lt;/p&gt;

&lt;p&gt;With the predicted output, we can now calculate the error. The error can be as simple as the predicted minus the expected output (ŷ - y), or the squared error = (y_pred - y)². The Mean Squared Error (MSE) is sometimes used in regression problems, while the cross-entropy is used for classification problems. This function is called the Loss Function and once again, it depends on the problem we are trying to solve. That represents how good our model is performing, lower the value, closer we are to the expected output value.&lt;/p&gt;

&lt;p&gt;We will use the MSE here for sake of simplicity since MSE makes the gradient descent not work well. In logistic regression is common to use the loss function demonstrated at the beginning of this post, because it gives us a convex optimization problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/mse.png&quot; alt=&quot;Mean Squared Error &quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/backpropagation.png&quot; alt=&quot;Backpropagation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is the phase we take the error and adjust the weights so in the next iteration we have a small error. We must then go back adjusting the weights based on this MSE error. The backpropagation step is the SGD with the Chain Rule from Calculus. With partial derivatives, we can understand how the error is impacted by each weight separately.&lt;/p&gt;

&lt;h3 id=&quot;the-chain-rule-and-partial-derivatives&quot;&gt;The chain rule and partial derivatives&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/chain-rule.png&quot; alt=&quot;Chain Rule&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chain rule is actually very intuitive: if we have A, which apply a function f to a variable x and B, where another function g applies a function over f(x), the chain rule says that if we want to find the partial derivative of B with respect to x we should multiply the intermediary partial derivatives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/partial-derivatives.png&quot; alt=&quot;Partial Derivatices&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;updating-the-weights-with-backpropagation&quot;&gt;Updating the weights with backpropagation&lt;/h3&gt;

&lt;p&gt;Our main focus in Machine Learning is to update the weights so that the model gives us better predictions. The general rule for updating the weights and bias was showed before, but for this example we are setting ‘b’ to zero, so we only have to update W:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/update-w.png&quot; alt=&quot;Update Rule GD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, to achieve this we need to find the partial derivative of the Loss Function L(ŷ, y) with respect to W, and this update function can be applied to any weight in the figure above. In this post, we are not going to dive deep in how derivatives works and I’ll show you only what is the value of certain derivative. Although we must understand how all this process works, the most common Deep Learning Frameworks already had this implement and would be very rare the case you are going to code this.&lt;/p&gt;

&lt;p&gt;For the sake of illustration, let’s find the new value to w12. And for that, lets state two things: we chose the Median Square Error as loss function and the hyperbolic tangent as activation function. The derivative of z[2] with respect of s[2] is equal the derivative of the activation function tanh, so:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/loss-function-mse.png&quot; alt=&quot;MSE Loss Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/derivative-tanh.png&quot; alt=&quot;Derivative of Hyperbolic Tangent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first step is to calculate how the loss was influenced by the activation step in z[2]. We achieve this finding the derivative of the loss function with respect to z[2]:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dz2.png&quot; alt=&quot;image-20200615210236714&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next we calculate how the loss was impacted by s[2], through the chain rule. This is equal to the partial derivative of the previous case, multiplied by the derivative of z[2] with respect of s[2], which is the derivative of the activation function state above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/ds2.png&quot; alt=&quot;image-20200615210913075&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And finally, to find the derivative we were looking for, to update w12, we keep following the chain to find the derivative of the cost function with respect of w12. Here ds/dw is equal to the derivative of the linear function (s) with respect a w, where x is the term from W is multiplied for. In the weight w[2]12 is z[1]1, and in the w[1]11 is x1, for exemple.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/ds-dw.png&quot; alt=&quot;image-20200615211707555&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12.png&quot; alt=&quot;image-20200615212010762&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have the term needed (dL(ŷ,y) / dW) to update the weight w[2]12. This value is called the gradient and tells us in which direction and how big this step should be.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12-num.png&quot; alt=&quot;image-20200615213452203&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And choosing a learning rate alpha of 0.1, the new value for the weight w12 would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12-updated.png&quot; alt=&quot;image-20200615214024309&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This would change the weight so that the error would be lower in the following iteration. Then we would update again this weight in a iterative process until the model as a whole has the right weights to make a prediction. I won’t be showing all the weights numerically, but here are all the derivatives needed to calculate all the weights:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/all-weights-derivatives.png&quot; alt=&quot;All the weights derivatives&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Awesome! Now you know a little bit more how Deep Learning models work under the hood. There are amazing resources online where you can continue to learn and create a deep understanding of these concepts. The &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning&quot;&gt;Andrew Ng MOOC&lt;/a&gt; is the go-to course to understand deeply the theoretical part of Deep Learning and have assignments to consolidate your knowledge through programming exercises.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="back-to-basics" />
      

      
        <summary type="html">Let’s see three fundamentals concepts of Deep Learning: the Gradient Descent, Feedforward and Backpropagation steps.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Facial Keypoint Detection</title>
      <link href="http://localhost:4000/Facial-Keypoint-Detection" rel="alternate" type="text/html" title="Facial Keypoint Detection" />
      <published>2020-06-15T12:15:00+02:00</published>
      <updated>2020-06-15T12:15:00+02:00</updated>
      <id>http://localhost:4000/Facial%20Keypoint%20Detection</id>
      <content type="html" xml:base="http://localhost:4000/Facial-Keypoint-Detection">&lt;p&gt;This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some examples of images and corresponding facial keypoints.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/key_pts_example.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and &lt;strong&gt;68 keypoints, with coordinates (x, y), for that face&lt;/strong&gt;.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. This keypoint extraction project is part of the &lt;a href=&quot;https://www.udacity.com/course/computer-vision-nanodegree--nd891&quot;&gt;Udacity Computer Vision Nanodegree.&lt;/a&gt; Here they are, numbered, and you can see that specific ranges of points match different portions of the face.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/landmarks_numbered.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;In this project, we defined and trained a CNN to identify and extract facial keypoints, founded in the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;. Coded using Python and Pytorch, first we get familiar with the dataset and do all the processing and transformation using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataloader&lt;/code&gt;. Using a Custom LeNet-5 Architecture, we trained a model to identify such keypoints in the faces located by a OpenCV’s pre-trained Haar Cascade classifier. &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;Code and tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;The first step in working with any dataset is to become familiar with your data; we’ll need to load in the images of faces and their keypoints and visualize them! This set of image data has been extracted from the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;, which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.&lt;/p&gt;

&lt;h4 id=&quot;training-and-testing-data&quot;&gt;Training and Testing Data&lt;/h4&gt;

&lt;p&gt;This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3462 of these images are training images, to create a model to predict keypoints.&lt;/li&gt;
  &lt;li&gt;2308 are test images, which will be used to test the accuracy of the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas&lt;/code&gt;. When we read the training CSV and get the annotations, they are in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).&lt;/p&gt;

&lt;p&gt;Below, in an output from the function &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt; that takes in an image and keypoints and displays them. As you look at this data, &lt;strong&gt;note that these images are not all of the same size&lt;/strong&gt;, and neither are the faces! To eventually train a neural network on these images, we’ll need to standardize their shape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/show_keypoints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this project, to prepare our data for training, we used PyTorch’s Dataset class. Much of this this code is a modified version of what can be found in the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/data_loading_tutorial.html&quot;&gt;PyTorch data loading tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;dataset-class&quot;&gt;Dataset class&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; is an abstract class representing a dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.&lt;/p&gt;

&lt;p&gt;Our custom dataset should inherit &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt; and override the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__len__&lt;/code&gt; so that &lt;code class=&quot;highlighter-rouge&quot;&gt;len(dataset)&lt;/code&gt; returns the size of the dataset.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt; to support the indexing such that &lt;code class=&quot;highlighter-rouge&quot;&gt;dataset[i]&lt;/code&gt; can be used to get the i-th sample of image/keypoint data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here we create a dataset class for our face keypoints dataset. We read the CSV file in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; but leave the reading of images to &lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt;. This is memory efficient because all the images are not stored in the memory at once but read as required.&lt;/p&gt;

&lt;p&gt;A sample of our dataset will be a dictionary &lt;code class=&quot;highlighter-rouge&quot;&gt;{'image': image, 'keypoints': key_pts}&lt;/code&gt;. Our dataset will take an
optional argument &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; so that any required processing can be applied on the sample.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FacialKeypointsDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;# if image has an alpha color channel, get rid of it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'float'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'keypoints'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After we’ve defined this class, we can instantiate the dataset and display some images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/sample0.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transforms&quot;&gt;Transforms&lt;/h3&gt;

&lt;p&gt;Now, the images in this dataset are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.&lt;/p&gt;

&lt;p&gt;Therefore, we will need to write some pre-processing code. The following four transforms were created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Normalize&lt;/code&gt;: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Rescale&lt;/code&gt;: to rescale an image to a desired size.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RandomCrop&lt;/code&gt;: to crop an image randomly.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ToTensor&lt;/code&gt;: to convert numpy images to torch images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called. For this, we just implemented &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; method and (if we require parameters to be passed in), the &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; method.  We can then use a transform like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tx = Transform(params)
transformed_sample = tx(sample)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Observe below how these transforms are generally applied to both the image and its keypoints. Let’s test these transforms out to make sure they behave as expected. As you look at each transform, note that, in this case, &lt;strong&gt;order does matter&lt;/strong&gt;. For example, you cannot crop a image using a value smaller than the original image (and the original images vary in size!), but, if you first rescale the original image, you can then crop it to any size smaller than the rescaled size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/transform.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After creating the transformed dataset, applying the transforms in order to get grayscale images of the same shape, verify that your transform works, we used &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt; iterator which provides features like Batch the data, Shuffle the data and Load the data in parallel using &lt;code class=&quot;highlighter-rouge&quot;&gt;multiprocessing&lt;/code&gt; workers.&lt;/p&gt;

&lt;p&gt;Now that we load and transform the data, we are ready to build a neural network to train on this data.&lt;/p&gt;

&lt;h2 id=&quot;the-convolutional-neural-network&quot;&gt;The Convolutional Neural Network&lt;/h2&gt;

&lt;p&gt;After looking at the data we’re working with and, in this case, know the shapes of the images and of the keypoints, we can define a convolutional neural network that can &lt;em&gt;learn&lt;/em&gt; from this data. Recall that CNN’s are defined by a few types of layers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convolutional layers&lt;/li&gt;
  &lt;li&gt;Maxpooling layers&lt;/li&gt;
  &lt;li&gt;Fully-connected layers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-neural-nets&quot;&gt;PyTorch Neural Nets&lt;/h3&gt;

&lt;p&gt;To define a neural network in PyTorch, we define the layers of a model in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and define the feedforward behavior of a network that employs those initialized layers in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt;, which takes in an input image tensor, &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;. Also note that during training, PyTorch will be able to perform backpropagation by keeping track of the network’s feedforward behavior and using autograd to calculate the update to the weights in the network.&lt;/p&gt;

&lt;p&gt;Best practice is to place any layers whose weights will change during the training process in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and refer to them in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function; any layers or functions that always behave in the same way, such as a pre-defined activation function, should appear &lt;em&gt;only&lt;/em&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;The model used in this project was a Custom LeNet-5 Architecture, which you can check in the &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;repository&lt;/a&gt; under the file &lt;code class=&quot;highlighter-rouge&quot;&gt;models.py&lt;/code&gt;, using 4 Convolutional Layers followed by a Poooling Layer and 2 Fully Connected. After reading the paper from &lt;a href=&quot;https://arxiv.org/pdf/1710.00977.pdf&quot;&gt;NaimishNet&lt;/a&gt;, I realized, from Fig.21, that LeNet-5 had a good performance while with fewer layers than NaimishNet. I started if LeNet-5 and change it until reaching the final mode. Adding two more Conv. Layers, replacing AvgPool by MaxPool (since were more common use avg back then) and Tanh activations by ReLU, improvements made in this 20 years.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=25600, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=136, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;feature-maps&quot;&gt;Feature maps&lt;/h3&gt;

&lt;p&gt;Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it’s convolutional kernels and when these kernels are applied to some input image, they produce a set of &lt;strong&gt;feature maps&lt;/strong&gt;. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/feature_map_ex.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;To prepare for training, create a transformed dataset of images and keypoints. In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so your model’s loss does not blow up during training, it is also suggested that you normalize the input images and keypoints. The necessary transforms have been defined in &lt;code class=&quot;highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, having defined the transformed dataset, we use PyTorch’s DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in &lt;a href=&quot;http://pytorch.org/docs/master/data.html&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also defined some functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;net_sample_output(): test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints.&lt;/li&gt;
  &lt;li&gt;show_all_keypoints(): displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided).&lt;/li&gt;
  &lt;li&gt;visualize_output(): this function’s main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors &lt;em&gt;and&lt;/em&gt; it undoes the keypoint normalization done in the Normalize() transform; it’s assumed that you applied these transformations when you loaded your test data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After testing the optimizer with Adam and SGD, I kept Adam because is faster and converges faster also. About the loss functions, MSE and L1Smooth had a similar performance with 5 epochs, so I kept because of the NaimishNet reference. In 60 epochs, the average loss was 0.000651, resulting in the following detection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download.png&quot; alt=&quot;image-20200612307150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download2.png&quot; alt=&quot;image-20200602137150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download3.png&quot; alt=&quot;image-20200607121350421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;face-and-facial-keypoint-detection&quot;&gt;Face and Facial Keypoint detection&lt;/h2&gt;

&lt;p&gt;After you’ve trained a neural network to detect facial keypoints, you can then apply this network to &lt;em&gt;any&lt;/em&gt; image that includes faces. The neural network expects a Tensor of a certain size as input and, so, to detect any face, we first have to do some pre-processing. The following image will be used for test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamas.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;detect-all-faces-in-an-image&quot;&gt;Detect all faces in an image&lt;/h3&gt;

&lt;p&gt;Here used one of OpenCV’s pre-trained Haar Cascade classifiers, all of which can be found in the &lt;code class=&quot;highlighter-rouge&quot;&gt;detector_architectures/&lt;/code&gt; directory, to find any faces in your selected image. In the figure below, we loop over each face in the original image and draw a red square on each face (in a copy of the original image, so as not to modify the original). You can even &lt;a href=&quot;https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html&quot;&gt;add eye detections&lt;/a&gt; while using Haar detectors. An example of face detection is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamamichele.png&quot; alt=&quot;image-2020060715210421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;keypoint-detection&quot;&gt;Keypoint detection&lt;/h3&gt;

&lt;p&gt;Now, we loop over each detected face in an image (again!) only this time, you’ll transform those faces in Tensors that your CNN can accept as input images. First we convert the face from RGB to grayscale and normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]. So we rescale the detected face to be the expected square size for your CNN (224x224, suggested) and reshape the numpy image into a torch image.&lt;/p&gt;

&lt;p&gt;And finally, after each face has been appropriately converted into an input Tensor for your network to see as input, we apply our &lt;code class=&quot;highlighter-rouge&quot;&gt;net&lt;/code&gt; to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be “un-normalized” for display, and we write a helper function like &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt;. We end up with an image like the following with facial keypoints that closely match the facial features on each individual face:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obama.png&quot; alt=&quot;image-20200123607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/michele.png&quot; alt=&quot;image-20200607131250421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AI May Highlights</title>
      <link href="http://localhost:4000/May-Highlights" rel="alternate" type="text/html" title="AI May Highlights" />
      <published>2020-05-28T12:15:00+02:00</published>
      <updated>2020-05-28T12:15:00+02:00</updated>
      <id>http://localhost:4000/May%20Highlights</id>
      <content type="html" xml:base="http://localhost:4000/May-Highlights">&lt;p&gt;Top 5 AI News and Trends I came across in May 2020.&lt;/p&gt;

&lt;p&gt;Versão em português &lt;a href=&quot;http://edgeaiguru.com/Destaques-de-Maio&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/google.gif&quot; alt=&quot;Google BiT General Visual Representation Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;BiT model being used after pre-training and applied to other tasks with few labeled examples&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-open-sources-bit-the-bert-for-computer-vision&quot;&gt;Google open-sources BiT, the BERT for Computer Vision&lt;/h2&gt;

&lt;p&gt;On May 21, Google’s AI Team open-sourced Big Transfer (BiT): A General Visual Representation Learning. This is an effort to help Computer Vision applications to advance through large scale pre-trained models. Using BiT is possible to achieve state-of-the-art performance in many applications just using few labeled examples. To address tunning this model they also introduced “BiT-HyperRule”, a heuristic approach to selecting hyperparameters such as learning-rate and weight-decay. After transfer learning, using  CIFAR-10 dataset, the model achieved 64% and 95% median accuracy in 1-shot and 5-shot, respectively. Google also mentioned that “BiT-L matches or surpasses state-of-the-art results […] in several standard computer vision benchmarks such as Oxford &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/pets/&quot;&gt;Pets&lt;/a&gt; and &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/flowers/&quot;&gt;Flowers&lt;/a&gt;, &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR&lt;/a&gt;, etc.”  You can check the &lt;a href=&quot;https://arxiv.org/abs/1912.11370&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/tf_logo_social.png&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-version-220-is-here&quot;&gt;Tensorflow Version 2.2.0 is here&lt;/h2&gt;

&lt;p&gt;After almost four months after TF 2.1.0, this month Google also released Tensorflow 2.2.0. Major updates include a new Profiler  for CPU/GPU/TPU to help identify performance bottlenecks, performance improvements in computation between devices through &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.distrbuite&lt;/code&gt;, a new scalar type for strings (from &lt;code class=&quot;highlighter-rouge&quot;&gt;std::string&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow::tstring&lt;/code&gt;) and the substitution from SWIG to pybind11 while exporting C++ functions.  We also had a few Keras improvements such as the use of custom training logics with &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.fit&lt;/code&gt; overwriting &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.train_step&lt;/code&gt; and the SavedModel format that now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers). Furthermore, another highlight is that they dropped Python 2 support, which is not maintained since January 2020. Last, in this release, TensorFlow requires gast version 0.3.3. Check the full text &lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/nvidia-a100.jpg&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-launches-a100-ai-gpu-the-ultimate-instrument-for-advancing-ai&quot;&gt;NVIDIA launches A100 AI GPU: the ultimate instrument for advancing AI&lt;/h2&gt;

&lt;p&gt;Announced on May 14 during the Nvidia GTC 2020 event, the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/a100/&quot;&gt;A100 AI Chip&lt;/a&gt; has 54 billion transistors and 5 petaflops of performance and is the most powerful end-to-end AI and HPC platform for data centers, following the trend of HPC after &lt;a href=&quot;https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion&quot;&gt;acquiring Mellanox in March&lt;/a&gt;. Based on the Ampere architecture, the 7-nanometer chip can be used for scientific computing, cloud graphics, and data analytics. Here are some numbers, 20 times more powerful than the previous Tesla V100 GPU the A100 achieves 19.5 computational power with its 6,912 CUDA cores, 40 GB of memory, 1.6 TB of bandwidth, and the third generation Tensor Cores. One application is the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;DGX&lt;/a&gt; system, which is based in 8 NVIDIA A100 and is aiming the Artificial Intelligence sector, costing US$ 1 million. We can expect to see the new AI Chip in cloud services shortly. Alibaba Cloud, AWS, Baidu Cloud, Google Cloud, Microsoft Azure, Oracle, and Tencent Cloud had partnered up with NVIDIA already.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/fb.jpeg&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;facebook-ai-announces-groknet-for-advancing-product-understanding&quot;&gt;Facebook AI announces GrokNet for advancing product understanding&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;“a universal computer vision system designed for shopping. It can identify fine-grained product attributes across billions of photos — in different categories, such as fashion, auto, and home decor.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unlikely most models which have only one category to learn, GrokNet was trained in seven datasets, with several types of supervision and 83 loss functions, been able to predict categories, attributes such as color and texture and likely search queries. The FB model is trained by a colossal database with about 100 million images, most of which, of course, is taken from the Marketplace. The company also states that the system is able to identify objects that were photographed with not very good lighting conditions and angles that do not favor a good product view. To give the model the ability to understand easy and difficult tasks, they gave tasks a combination of weight loss and images for training, that is, easier tasks don’t need much supervision and can be given a smaller weight. Compared to text-attribute systems the new FB model can predict home and garden listings with 90% accuracy, against the firsts 33%. Facebook expects that “in the future, GrokNet could be used to help customers easily find exactly what they’re looking for, receive personalized suggestions from storefronts on what products are most relevant to them, [and] which products are compatible”. Check the official announcement &lt;a href=&quot;https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/sudo.gif&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sudo-190-launched-with-security-updates&quot;&gt;Sudo 1.9.0 launched with security updates&lt;/h2&gt;

&lt;p&gt;Basic command in our daily works with our beloved Unix systems, sudo allows users to execute programs as root. In May sudo received an update to version 1.9.0 after 9 years in the 1.8 branch. This new version brings security updates such as, and centralizing logging through &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo_logsrvd&lt;/code&gt;, creating sudo with the option “–enable-openssl”, the data is transmitted through an encrypted communication channel (TLS). It is now possible to use thirty-party softwares to pull data from sudo sessions for auditing. And while TF drops Python 2 support, it arrives in Sudo 1.9, where plugins can be written in python now, which is activated during compilation with the “–enable-python” option. Check all the changes in the full release &lt;a href=&quot;https://www.sudo.ws/stable.html#1.9.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Top 5 AI News and Trends I came across in May 2020.</summary>
      

      
      
    </entry>
  
</feed>
