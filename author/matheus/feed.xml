<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/matheus/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-08-13T23:28:02-03:00</updated>
  <id>http://localhost:4000/author/matheus/feed.xml</id>

  
  
  

  
    <title type="html">Edge AI Guru | </title>
  

  
    <subtitle>The guidance through your Artificial Intelligence journey</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Tracking 3D objects with Lidar and Camera</title>
      <link href="http://localhost:4000/3D-Object-Tracking" rel="alternate" type="text/html" title="Tracking 3D objects with Lidar and Camera" />
      <published>2020-07-23T07:15:00-03:00</published>
      <updated>2020-07-23T07:15:00-03:00</updated>
      <id>http://localhost:4000/3D%20Object%20Tracking</id>
      <content type="html" xml:base="http://localhost:4000/3D-Object-Tracking">&lt;p&gt;Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)&lt;/p&gt;

&lt;p&gt;
    Autonomous veihcles technology has been in my radar (no pun intended) for a while now. I was excited to apply AI to some very complex problems and this project was for sure one of the most amazing I ever did. More than Machine and Deep Learning techniques, solving autonomous driving requires several technlogies and methods. All this is only possible through fusing sensors to make sense of the enviroment.
&lt;/p&gt;
&lt;p&gt;
    To fusion two sensors was a completly new challenge for me and seeing the results is awesome. After some introduction on how camera technology works and how optics are applied in modern systems, some image processing and computer vision applications was covered,  to better understand how to fusion Lidar and Camera data. From this point on, I tried to apply this knowledge to a collision avoidance project for  vehicles. 
&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T2UWLYwpuv4?controls=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/structure.png&quot; alt=&quot;Structure&quot;&gt;&lt;/p&gt;

&lt;i&gt;Project Structure. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;



&lt;p&gt;
    From cameras we detect images keypoints, such tail lights, extract these keypoints descriptors and match them between successive images and obsereve its distances, computing Camera TCC. From Lidar we take a 3D Point Cloud, crop the points(discart road and bad measurements) and cluster these into objects, identified using Neural Networks and provided with bounding boxes. We than fusion both information, connecting them into space and tracking 3D object bouding boxes to compute the Lidar TCC on the object in front of the car. Conducting various tests with the framework, we tried to identify the most suitable detector/descriptor combination for TTC estimation and also to search for problems that can lead to faulty measurements by the camera or Lidar sensor.
&lt;/p&gt;

&lt;h2&gt;The Technology&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;	
    To make it happen, everything was coded in C++ using the &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti&quot;&gt;KITTI dataset&lt;/a&gt;. Besides, the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;github&lt;/a&gt;. This project is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;.
&lt;/p&gt;
&lt;blockquote cite=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;
    &lt;p&gt;Our recording platform is a Volkswagen Passat B6, which has been modified with actuators for the pedals (acceleration and brake) and the steering wheel. The data is recorded using an eight core i7 computer equipped with a RAID system, running Ubuntu Linux and a real-time database. We use the following sensors:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;1 Inertial Navigation System (GPS/IMU): OXTS RT 3003&lt;/li&gt;
        &lt;li&gt;1 Laserscanner: Velodyne HDL-64E&lt;/li&gt;
        &lt;li&gt;2 Grayscale cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3M-C)&lt;/li&gt;
        &lt;li&gt;2 Color cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3C-C)&lt;/li&gt;
        &lt;li&gt;4 Varifocal lenses, 4-8 mm: Edmund Optics NT59-917&lt;/li&gt;
    &lt;/ul&gt;
&lt;/blockquote&gt;


&lt;br&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/passat_sensors.png&quot; alt=&quot;Passat&quot;&gt;&lt;/p&gt;
&lt;i&gt;KITTI Passat Sensor Setup. Source: &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;





&lt;h2&gt;The Project&lt;/h2&gt;
&lt;br&gt;
&lt;p&gt;
    Cameras are one mature technology and the main sensor in autonomous vehicles. In a world made by humans, everything is adapted to our senses and driving would not be different. Therefore, it's main advantage is to &quot;simulate&quot; the human vision being able to interpreate 2D information as road signs and lanes. At the same time, it has challenges as we humans: bad performance in darkness or bad wether conditions. Thus, alone in some applications the camera will fail, hence the need to add a second sensor such as Radar or Lidar. To fill the gap, Lidar has high resolution and can reconstruct 3D objects while Radar has a greater range and can detect velocity more accurate.
&lt;/p&gt;
&lt;p&gt;
    In this project the main objetive was to estimate the Time to Collision(TCC) using a camera-based object classification to cluster Lidar points and from 3D bounding boxes compute TCC. Human reflection time is around 1 second, so the system has to warn us way before and starting breaking 2 to 3 seconds before collision. But how do we tell a machine a collision is near? How do we compute such time? The first concern is to chose a Motion Model: Constant Velocity(CVM) or Constant Acceleration(CAM). While CAM models best real world situations, CVM assumes that the velocity does not change and it was chosen for simplicty, leaving space for us to focus more on the computer vision aspects.  
&lt;/p&gt;
&lt;p&gt;
    From that, the problem can be divided into two main parts: estimate TTC from Lidar points and estimate it using camera successive images. The former is pretty straightforward, addressing only the preceding car and calculating the time from the X component in the Point Cloud.  Based in the CVM, we can compute the velocity based in two Lidar measurments over time, taking the closest Lidar point in the path of the car and observing how it changes in a time window. To avoid errouneos measurments that would lead to false TCC, we perform a &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule&quot;&gt;IQR range values&lt;/a&gt; to remove outliers. Due some measurement accuracy based on the amount of light reflected from an object, when using Lidar for TCC is advised to remove measurement of the road surface and low reflectivity points, making it easy to measure the distance of the preceding vehicle. Furthermore, instead of estimating the velocity after computing the veihcles velocity and two distance measurements, that could be noisy, we could use a Radar sensor that measures directly the relative speed. 
&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/keypoints.jpg&quot; alt=&quot;Keypoints&quot;&gt;&lt;/p&gt;

&lt;i&gt;Keypoints detection and tracking using descriptors. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;    
    In other hand, it is hard to measure metric distances using only one camera, some companies achieve measuring distance using a stereo camera setup, like &lt;a href=&quot;http://www.6d-vision.com/&quot;&gt;Mercedes-Benz which pioneered in this technology&lt;/a&gt;. With two pictures of the same place, taken from two different points of view, is possible to locate points of interest in both images and calculate its distance using geometry and perspective projection. Using a mono camera we can estimate TCC only by observing relative height change (or scale change) in the preceding car, for example, without distance measurement. Here enters Deep Learning, allowing us to identify cars in images. As showed in the figure above, it is also used CV techniques to find keypoints and track them using descriptors from one frame to the next, estimating the scale change in the object. Tracking how these descriptors change over time, we can estimate TTC.
&lt;/p&gt;
&lt;p&gt;
    First, we focus on loading images, setting up data structures and putting everything into a ring buffer to optimize memory load. Then, integrated several keypoint detectors such as HARRIS, FAST, BRISK and SIFT and compare them with regard to number of keypoints and speed. For the descriptor extraction and matching, we used brute force and also the FLANN approach and tested the various algorithms in different combinations and compare them with regard to some performance measures. 
    Counting the number of keypoints on the preceding vehicle for 10 images and taking note of the distribution of their neighborhood size, doing this for several detectors implemented, we noted that FAST, BRISK and AKASE were the detectors that identified the larger number of keypoints.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/comparision_table.png&quot; alt=&quot;Table&quot;&gt;&lt;/p&gt;
&lt;i&gt;Comparision table between several detectors to identify keypoints&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;
    During the matching step, the Brute Force approach is used with the descriptor distance ratio set to 0.8 and the time it takes for keypoint detection and descriptor extraction was logged. A helper function was created to iterate through all the possible  detectors and descriptors, saving its matches and times. From this was identified that the best Detector for this application is FAST, and the best Descriptors are BRIEF and ORB.
&lt;/p&gt;
&lt;p&gt;
    Then we detected and classify images using the YOLO framework, which gives us a set of bounding boxes and augmenting them associating each bounding box to its respective Lidar points. Finnaly we track this bounding boxes over time to estimate Time To Collision with Lidar and Camera measurements. When taking into account both TCC, twenty pairs of Detector/Descriptors were analyzed and AZAKE/FREAK was chosen based on the average error when compared to Lidar's TCC.
&lt;/p&gt;
&lt;p&gt;
    To check the complete code with comments, tasks and the results, please check the &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Gradient Descent, Feedforward and Backpropagation</title>
      <link href="http://localhost:4000/Feedforward-and-Backpropagation" rel="alternate" type="text/html" title="Gradient Descent, Feedforward and Backpropagation" />
      <published>2020-06-15T07:15:00-03:00</published>
      <updated>2020-06-15T07:15:00-03:00</updated>
      <id>http://localhost:4000/Feedforward%20and%20Backpropagation</id>
      <content type="html" xml:base="http://localhost:4000/Feedforward-and-Backpropagation">&lt;p&gt;Let’s see three fundamentals concepts of Deep Learning: the Gradient Descent, Feedforward and Backpropagation steps.&lt;/p&gt;

&lt;p&gt;Versão em português disponível em 26 de Junho &lt;a href=&quot;http://edgeaiguru.com/404&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/pre-cover.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;Gradient Descent(GD)  in one of the most common algorithms that help the Neural Networks to reach the correct values of weights and bias. To illustrate this method, let’s take a look at a Logistic Regression model. In this model, we have a loss of L(ŷ,y), where it defines how far the predicted output is from the correct output. If we sum all the losses during the training phase and average them, we will get the Cost Function J(w,b).  What is usually used for regression problems is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/log-reg-loss-function.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since we want to train our model to be as much accurate as possible, it is natural that this method tries to minimize the Cost Function J(W,b) in each step. So the cost function measures how well the parameters W and b are performing in the training set, iteratively updating the weights and bias trying to reach the global minimum in such function. And what is a global minimum? As shown in the figure below, the global minimum of a function J(theta1,theta2) is the values of theta1 and theta2 that produce the minimum value on J. Here J is equivalent to the height of the plot. Notice that there is the so-called local minimum, which sometimes tricks the model making it believe it has achieved the global minimum. One can be seen at theta1 and theta2 equals 0.8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/gradient-descent.png&quot; alt=&quot;Gradient descent to minimize cost function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Minimizing the Cost Function, a Gradient Descent Illustration. Source: &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Stanford’s Andrew Ng’s MOOC Machine Learning Course&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So, the minimum is dependent on the cost function J chosen, and to achieve it, we must first define some starting point for W and b and try to go downhill in the direction of the steepest descent in the function. To make it easy to understand, let us take a look at just one variable. If we are trying to minimize the function J(w), as shown below, and we initialize W with a very high value on the X-axis, what are the next steps?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/cost-function-j-w-1.png&quot; alt=&quot;Cost Function 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To update the weight W and the bias b, the gradient descent rule is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/f1.png&quot; alt=&quot;math1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To move it to the minimum, we first need to know which direction we should follow. Should we increase or decrease W? This can be achieved with the derivative of the point. A derivative is a calculus tool that is equal to the slope of the function at a given point. In other words, it tells us exactly what we need to know: the direction we should follow. When the function is increasing the derivative is positive, when the function is decreasing it is negative and when it is zero it means that it reached a local or global minimum. In the following figure, the derivative in this point is positive and so the dW value will be subtracted from W, in the formula above, and we are going to move left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/cost-function-j-w-2.png&quot; alt=&quot;Cost Function 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, if W was a low number on the left side of the figure, the derivative would be negative and the number dW would be added to W, moving it to the right. The alpha term is called Learning Rate and it defines how big those steps would be. Defining it too big could make the function pass through the local minimum and start to diverge.&lt;/p&gt;

&lt;p&gt;This is a simple convex function analyzed in a 2D graph. Although the concepts and steps remain the same, depending on the problem these functions could get pretty complex and have multiple dimensions, and it is even harder to visualize it. As you may see later, the Gradient Descent is just one of several Optimization Algorithms such as Adagrad, Adadelta, RMSProp, Momentum, and Adam.&lt;/p&gt;

&lt;p&gt;So now we have an optimization algorithm to update W and b values to get our model output closer to the expected output. Through the &lt;strong&gt;Forward Propagation&lt;/strong&gt;, we will pass through the model and calculate an output value (the prediction), compare this value with the expected output, and find an error. With this, we will transmit this error backward using &lt;strong&gt;Backward Propagation&lt;/strong&gt; and with the gradients (derivative values) to update the values for W and b, making the whole model closer to the cost function’s global minimum.&lt;/p&gt;

&lt;h2 id=&quot;feedforward&quot;&gt;Feedforward&lt;/h2&gt;

&lt;p&gt;It is the phase we calculate the model output, so we can understand how accurate it is. For illustration, lets take a network with just one hidden layer. In practice we may have several hidden layers and multiple output values. We will have to calculate the hidden states and then find the output values based on this states. Here the hidden layer and the outputs are matrices and we denote the layer by adding square brackets, i.e. [1] it’s the first layer. To calculate the predict output (ŷ) in the following 2 layers network, we must first find hidden values here called z.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/feedforward.png&quot; alt=&quot;Feedforward&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each hidden neuron is divide into two operations: the linear function y = x*w + b and the application of an activation function sigma. These steps are named s and z, respectively. In this example, we are going to set b, or bias, to zero to simplify the explanation. We use activation functions to make sure these values do not increase too much. It will make sure the outputs are between 0 and 1 or -1 and 1 (check this post for more about activation functions). Furthermore, these functions allow the network to represent nonlinear relationships between inputs and outputs.&lt;/p&gt;

&lt;p&gt;So initializing randomly the weights W and choosing TanH as an activation function sigma, we have:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/random-weights-sigma.png&quot; alt=&quot;image-20200614105546474&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will assume that in this particular training step, we are inputing x1 and x2 equals 1 and are expecting the model to output 1 as well. As state before, the first step is to calculate the result of the linear function and find s1 and s2 from the first layer [1]. Then, the results are passed through an activation function to serve as inputs for the next layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/s-and-z-output.png&quot; alt=&quot;image-20200614112302494&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With z1 and z2 in hands, we can now calculate the input from the next layer, in this case, the output layer. Multiplying these values by the weights and passing through the tanh activation function, we finally find our predicted output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/predicted-output.png&quot; alt=&quot;image-20200614112507065&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the output layer, sometimes we use different activation functions, or none, depending on the application. If it was a classification problem, we could use a Softmax here.&lt;/p&gt;

&lt;p&gt;With the predicted output, we can now calculate the error. The error can be as simple as the predicted minus the expected output (ŷ - y), or the squared error = (y_pred - y)². The Mean Squared Error (MSE) is sometimes used in regression problems, while the cross-entropy is used for classification problems. This function is called the Loss Function and once again, it depends on the problem we are trying to solve. That represents how good our model is performing, lower the value, closer we are to the expected output value.&lt;/p&gt;

&lt;p&gt;We will use the MSE here for sake of simplicity since MSE makes the gradient descent not work well. In logistic regression is common to use the loss function demonstrated at the beginning of this post, because it gives us a convex optimization problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/mse.png&quot; alt=&quot;Mean Squared Error &quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/backpropagation.png&quot; alt=&quot;Backpropagation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is the phase we take the error and adjust the weights so in the next iteration we have a small error. We must then go back adjusting the weights based on this MSE error. The backpropagation step is the SGD with the Chain Rule from Calculus. With partial derivatives, we can understand how the error is impacted by each weight separately.&lt;/p&gt;

&lt;h3 id=&quot;the-chain-rule-and-partial-derivatives&quot;&gt;The chain rule and partial derivatives&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/chain-rule.png&quot; alt=&quot;Chain Rule&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chain rule is actually very intuitive: if we have A, which apply a function f to a variable x and B, where another function g applies a function over f(x), the chain rule says that if we want to find the partial derivative of B with respect to x we should multiply the intermediary partial derivatives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/partial-derivatives.png&quot; alt=&quot;Partial Derivatices&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;updating-the-weights-with-backpropagation&quot;&gt;Updating the weights with backpropagation&lt;/h3&gt;

&lt;p&gt;Our main focus in Machine Learning is to update the weights so that the model gives us better predictions. The general rule for updating the weights and bias was showed before, but for this example we are setting ‘b’ to zero, so we only have to update W:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/update-w.png&quot; alt=&quot;Update Rule GD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, to achieve this we need to find the partial derivative of the Loss Function L(ŷ, y) with respect to W, and this update function can be applied to any weight in the figure above. In this post, we are not going to dive deep in how derivatives works and I’ll show you only what is the value of certain derivative. Although we must understand how all this process works, the most common Deep Learning Frameworks already had this implement and would be very rare the case you are going to code this.&lt;/p&gt;

&lt;p&gt;For the sake of illustration, let’s find the new value to w12. And for that, lets state two things: we chose the Median Square Error as loss function and the hyperbolic tangent as activation function. The derivative of z[2] with respect of s[2] is equal the derivative of the activation function tanh, so:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/loss-function-mse.png&quot; alt=&quot;MSE Loss Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/derivative-tanh.png&quot; alt=&quot;Derivative of Hyperbolic Tangent&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first step is to calculate how the loss was influenced by the activation step in z[2]. We achieve this finding the derivative of the loss function with respect to z[2]:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dz2.png&quot; alt=&quot;image-20200615210236714&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next we calculate how the loss was impacted by s[2], through the chain rule. This is equal to the partial derivative of the previous case, multiplied by the derivative of z[2] with respect of s[2], which is the derivative of the activation function state above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/ds2.png&quot; alt=&quot;image-20200615210913075&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And finally, to find the derivative we were looking for, to update w12, we keep following the chain to find the derivative of the cost function with respect of w12. Here ds/dw is equal to the derivative of the linear function (s) with respect a w, where x is the term from W is multiplied for. In the weight w[2]12 is z[1]1, and in the w[1]11 is x1, for exemple.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/ds-dw.png&quot; alt=&quot;image-20200615211707555&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12.png&quot; alt=&quot;image-20200615212010762&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have the term needed (dL(ŷ,y) / dW) to update the weight w[2]12. This value is called the gradient and tells us in which direction and how big this step should be.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12-num.png&quot; alt=&quot;image-20200615213452203&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And choosing a learning rate alpha of 0.1, the new value for the weight w12 would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/dw12-updated.png&quot; alt=&quot;image-20200615214024309&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This would change the weight so that the error would be lower in the following iteration. Then we would update again this weight in a iterative process until the model as a whole has the right weights to make a prediction. I won’t be showing all the weights numerically, but here are all the derivatives needed to calculate all the weights:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/feed-back-prop/all-weights-derivatives.png&quot; alt=&quot;All the weights derivatives&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Awesome! Now you know a little bit more how Deep Learning models work under the hood. There are amazing resources online where you can continue to learn and create a deep understanding of these concepts. The &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning&quot;&gt;Andrew Ng MOOC&lt;/a&gt; is the go-to course to understand deeply the theoretical part of Deep Learning and have assignments to consolidate your knowledge through programming exercises.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="back-to-basics" />
      

      
        <summary type="html">Let’s see three fundamentals concepts of Deep Learning: the Gradient Descent, Feedforward and Backpropagation steps.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AI May Highlights</title>
      <link href="http://localhost:4000/May-Highlights" rel="alternate" type="text/html" title="AI May Highlights" />
      <published>2020-05-28T07:15:00-03:00</published>
      <updated>2020-05-28T07:15:00-03:00</updated>
      <id>http://localhost:4000/May%20Highlights</id>
      <content type="html" xml:base="http://localhost:4000/May-Highlights">&lt;p&gt;Top 5 AI News and Trends I came across in May 2020.&lt;/p&gt;

&lt;p&gt;Versão em português &lt;a href=&quot;http://edgeaiguru.com/Destaques-de-Maio&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/google.gif&quot; alt=&quot;Google BiT General Visual Representation Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;BiT model being used after pre-training and applied to other tasks with few labeled examples&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-open-sources-bit-the-bert-for-computer-vision&quot;&gt;Google open-sources BiT, the BERT for Computer Vision&lt;/h2&gt;

&lt;p&gt;On May 21, Google’s AI Team open-sourced Big Transfer (BiT): A General Visual Representation Learning. This is an effort to help Computer Vision applications to advance through large scale pre-trained models. Using BiT is possible to achieve state-of-the-art performance in many applications just using few labeled examples. To address tunning this model they also introduced “BiT-HyperRule”, a heuristic approach to selecting hyperparameters such as learning-rate and weight-decay. After transfer learning, using  CIFAR-10 dataset, the model achieved 64% and 95% median accuracy in 1-shot and 5-shot, respectively. Google also mentioned that “BiT-L matches or surpasses state-of-the-art results […] in several standard computer vision benchmarks such as Oxford &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/pets/&quot;&gt;Pets&lt;/a&gt; and &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/flowers/&quot;&gt;Flowers&lt;/a&gt;, &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR&lt;/a&gt;, etc.”  You can check the &lt;a href=&quot;https://arxiv.org/abs/1912.11370&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/tf_logo_social.png&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-version-220-is-here&quot;&gt;Tensorflow Version 2.2.0 is here&lt;/h2&gt;

&lt;p&gt;After almost four months after TF 2.1.0, this month Google also released Tensorflow 2.2.0. Major updates include a new Profiler  for CPU/GPU/TPU to help identify performance bottlenecks, performance improvements in computation between devices through &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.distrbuite&lt;/code&gt;, a new scalar type for strings (from &lt;code class=&quot;highlighter-rouge&quot;&gt;std::string&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow::tstring&lt;/code&gt;) and the substitution from SWIG to pybind11 while exporting C++ functions.  We also had a few Keras improvements such as the use of custom training logics with &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.fit&lt;/code&gt; overwriting &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.train_step&lt;/code&gt; and the SavedModel format that now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers). Furthermore, another highlight is that they dropped Python 2 support, which is not maintained since January 2020. Last, in this release, TensorFlow requires gast version 0.3.3. Check the full text &lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/nvidia-a100.jpg&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-launches-a100-ai-gpu-the-ultimate-instrument-for-advancing-ai&quot;&gt;NVIDIA launches A100 AI GPU: the ultimate instrument for advancing AI&lt;/h2&gt;

&lt;p&gt;Announced on May 14 during the Nvidia GTC 2020 event, the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/a100/&quot;&gt;A100 AI Chip&lt;/a&gt; has 54 billion transistors and 5 petaflops of performance and is the most powerful end-to-end AI and HPC platform for data centers, following the trend of HPC after &lt;a href=&quot;https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion&quot;&gt;acquiring Mellanox in March&lt;/a&gt;. Based on the Ampere architecture, the 7-nanometer chip can be used for scientific computing, cloud graphics, and data analytics. Here are some numbers, 20 times more powerful than the previous Tesla V100 GPU the A100 achieves 19.5 computational power with its 6,912 CUDA cores, 40 GB of memory, 1.6 TB of bandwidth, and the third generation Tensor Cores. One application is the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;DGX&lt;/a&gt; system, which is based in 8 NVIDIA A100 and is aiming the Artificial Intelligence sector, costing US$ 1 million. We can expect to see the new AI Chip in cloud services shortly. Alibaba Cloud, AWS, Baidu Cloud, Google Cloud, Microsoft Azure, Oracle, and Tencent Cloud had partnered up with NVIDIA already.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/fb.jpeg&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;facebook-ai-announces-groknet-for-advancing-product-understanding&quot;&gt;Facebook AI announces GrokNet for advancing product understanding&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;“a universal computer vision system designed for shopping. It can identify fine-grained product attributes across billions of photos — in different categories, such as fashion, auto, and home decor.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unlikely most models which have only one category to learn, GrokNet was trained in seven datasets, with several types of supervision and 83 loss functions, been able to predict categories, attributes such as color and texture and likely search queries. The FB model is trained by a colossal database with about 100 million images, most of which, of course, is taken from the Marketplace. The company also states that the system is able to identify objects that were photographed with not very good lighting conditions and angles that do not favor a good product view. To give the model the ability to understand easy and difficult tasks, they gave tasks a combination of weight loss and images for training, that is, easier tasks don’t need much supervision and can be given a smaller weight. Compared to text-attribute systems the new FB model can predict home and garden listings with 90% accuracy, against the firsts 33%. Facebook expects that “in the future, GrokNet could be used to help customers easily find exactly what they’re looking for, receive personalized suggestions from storefronts on what products are most relevant to them, [and] which products are compatible”. Check the official announcement &lt;a href=&quot;https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/sudo.gif&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sudo-190-launched-with-security-updates&quot;&gt;Sudo 1.9.0 launched with security updates&lt;/h2&gt;

&lt;p&gt;Basic command in our daily works with our beloved Unix systems, sudo allows users to execute programs as root. In May sudo received an update to version 1.9.0 after 9 years in the 1.8 branch. This new version brings security updates such as, and centralizing logging through &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo_logsrvd&lt;/code&gt;, creating sudo with the option “–enable-openssl”, the data is transmitted through an encrypted communication channel (TLS). It is now possible to use thirty-party softwares to pull data from sudo sessions for auditing. And while TF drops Python 2 support, it arrives in Sudo 1.9, where plugins can be written in python now, which is activated during compilation with the “–enable-python” option. Check all the changes in the full release &lt;a href=&quot;https://www.sudo.ws/stable.html#1.9.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Top 5 AI News and Trends I came across in May 2020.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introduction to Neural Networks</title>
      <link href="http://localhost:4000/Introduction-to-Neural-Networks" rel="alternate" type="text/html" title="Introduction to Neural Networks" />
      <published>2020-05-03T07:18:00-03:00</published>
      <updated>2020-05-03T07:18:00-03:00</updated>
      <id>http://localhost:4000/Introduction%20to%20Neural%20Networks</id>
      <content type="html" xml:base="http://localhost:4000/Introduction-to-Neural-Networks">&lt;p&gt;Learn the main concepts behind Neural Networks, one of Deep Learning’s pillars.&lt;/p&gt;

&lt;p&gt;Versão em português &lt;a href=&quot;http://edgeaiguru.com/Introdução-a-Redes-Neurais&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Artificial Intelligence has been revolutionizing the industry in recent years and solving problems, which previously were costly in time and money, much more effectively. Computer vision problems, natural language processing, and several other applications are only possible thanks to advances in Deep Learning.&lt;/p&gt;

&lt;p&gt;Artificial Neural Networks (ANN) are one of the main pillars of this technology. Inspired by the human brain, ANN carries this name because of its biological connections and motivations. Just as in the human brain, where the most basic processing unit is the neuron, ANNs have an element that processes impulses, or inputs, which is also called a neuron or node.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/artificial-vs-biological-neuron.png&quot; alt=&quot;Artificial vs Biological Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Biological Neuron vs Artificial Neuron. Source: [Keras Tutorial: Deep Learning in Python] (https://www.datacamp.com/community/tutorials/deep-learning-python)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Both structures share the same function for transferring information: they receive an input (impulse) that is carried through the node (cell body) and activate a certain output (axon terminals). Just as in biological neurons, this impulse that fires neurons is reproduced in ANNs through activation functions.&lt;/p&gt;

&lt;p&gt;Therefore, this basic element of neural networks can be represented by the following figure, taken from the course &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning&quot;&gt;Neural Networks and Deep Learning&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-neuron.png&quot; alt=&quot;Generic Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where, through the example of the necessity to predict the price of houses based on their size, we can determine a function that can represent this problem. In this example, a ReLU function fits the data perfectly. So, the minimum representation of a neuron would be input the house’s area and, based on the mathematical function stored “inside” the neuron, we can estimate a price for that residence.&lt;/p&gt;

&lt;p&gt;In this way, we train each neuron to be activated when a certain pattern appears. Thus, grouping several neurons in series and parallel, allows Neural Networks to learn to recognize patterns in images, texts, audios, and in the most distinct forms of data.&lt;/p&gt;

&lt;p&gt;In this article, the main components of Artificial Neural Networks, some of the main architectures and the most common activation functions will be presented.&lt;/p&gt;

&lt;h2 id=&quot;artificial-neural-networks-ann&quot;&gt;Artificial Neural Networks (ANN)&lt;/h2&gt;

&lt;p&gt;Although neural networks have some similarities to neurons in the human brain, they are infinitely simpler than their biological counterpart. These architectures are composed of mathematical blocks that can be explained using algebra and calculus, very different from the different parts of the brain that are not yet understood.&lt;/p&gt;

&lt;p&gt;The main components of ANNs are input layers, hidden layers, and output layers. These layers are activated through weighted connections, which defines how important the connection is to the network. In addition, as we saw earlier, at the output of each neuron there is an activation function that defines whether the neuron will be fired or not.&lt;/p&gt;

&lt;h2 id=&quot;neural-networks-building-blocks&quot;&gt;Neural Networks Building Blocks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-2-layers-neural-network.png&quot; alt=&quot;Generic 3 Layer Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Generic 3 Layers Neural Network Architecture. Source: [Stanford CS231n] (https://cs231n.github.io/neural-networks-1/#nn)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;input-layer&quot;&gt;&lt;em&gt;Input Layer&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;A block of neurons can be called a layer. But note that although neurons are interconnected between layers, they do not have connections within the same layer. As shown in the figure above, the first layer of a Neural Network is the input layer. This has only the function of passing the system inputs to the next layer and does not perform any mathematical function.&lt;/p&gt;

&lt;h3 id=&quot;hidden-layers&quot;&gt;&lt;em&gt;Hidden Layers&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;This layer is responsible for one of the main functions of neural networks: to process the data and send it to the next layer. The value of each neuron is found by multiplying the weights W by the input X and adding a bias b. This value then goes through an activation function and is sent to the next layer, as shown in Fig. 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/hidden-layer-mathematics-en.png&quot; alt=&quot;Hidden Layer Mathematics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Mathematical operations within the neuron. Source: Source: [Stanford CS231n] (https://cs231n.github.io/neural-networks-1/#nn) Modified.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Thus, if we isolate the first neuron from the first hidden layer, the output value of the neuron will be equal to z1. Where &lt;em&gt;s1&lt;/em&gt; is the neuron’s input, where we multiply the weights by the inputs and add a bias b. After this operation, a transfer function &lt;em&gt;g&lt;/em&gt; is applied over &lt;em&gt;s1&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;It is important to note that &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;W&lt;/em&gt; in the first equation are matrices in this case and represent all inputs and all weights, respectively.&lt;/p&gt;

&lt;p&gt;We call this layer “Hidden Layer” because during the training of neural networks we have the inputs that are known and the outputs that are expected. But we don’t see what values are inside the neurons in that layer. This block can contain several hidden layers, and the more layers the “deeper” the neural network is, and the more patterns it can learn.&lt;/p&gt;

&lt;h3 id=&quot;output-layers&quot;&gt;&lt;em&gt;Output Layers&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;The output layer is responsible for showing the results obtained through the calculations made in the hidden layers. Usually, an activation function is used, as well as that of the neurons in the previous layers, to simplify the result.&lt;/p&gt;

&lt;h3 id=&quot;weights-and-bias&quot;&gt;&lt;em&gt;Weights and Bias&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Weights are responsible for defining how important that connection is to the neural network. As there are several connections within the ANN, this is how this architecture understands which patterns it should learn and which ones it should ignore. In addition, a value called bias is commonly used with weights and inputs. This value helps to fine-tune the neural network. Thus, if we have a neuron i in one layer and a neuron j in the next layer, we have a connection with the weight Wij and a bij bias.&lt;/p&gt;

&lt;h3 id=&quot;activation-functions&quot;&gt;&lt;em&gt;Activation Functions&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Also called the transfer function, it is the last mathematical processing step that takes place before the information leaves the neuron. This mathematical equation defines whether the neuron will be activated or not, which may be a step function, a linear function, or a non-linear function.&lt;/p&gt;

&lt;p&gt;The simplest activation function would be a step function. Where the neuron would activate only if the input was above a threshold, and the input signal would be fully reproduced at the node’s output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/step-function.png&quot; alt=&quot;Step Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can return values of 0 and 1, used in classification problems, or between 0 and 1, used in problems that we are more interested in knowing the probability of a certain entry being part of a certain class.&lt;/p&gt;

&lt;h2 id=&quot;main-types-of-artificial-neural-networks&quot;&gt;&lt;em&gt;Main Types of Artificial Neural Networks&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;There are two main types of Neural Networks: Feedforward Neural Networks and Recurrent Neural Networks.&lt;/p&gt;

&lt;h3 id=&quot;feedforward-neural-networks-fnn&quot;&gt;&lt;em&gt;Feedforward Neural Networks (FNN)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;This architecture is the most commonly found in the literature. In it, information moves only in one direction: from the inputs, through the hidden layer to the output node, and there are no cycles.&lt;/p&gt;

&lt;p&gt;The simplest unit of this topology is called Perceptron, the most simplistic neural network that is composed of just one node.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/perceptron.png&quot; alt=&quot;Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The Perceptron&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some simple problems can be solved with Perceptron, as it only works with linearly separable functions.&lt;/p&gt;

&lt;p&gt;With the need to solve more complex problems and from this basic unit, the &lt;strong&gt;Multilayer Perceptron (MLP)&lt;/strong&gt; was conceived. Composed of several layers of these nodes, being much more useful and being able to learn non-linear functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/multi-layer-perceptron.png&quot; alt=&quot;Multilayer Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Multilayer Perceptron Architecture. Source: [Advanced Methods for the Processing and Analysis of Multidimensional Signals: Application to Wind Speed] (https://www.researchgate.net/figure/Architecture-of-a-multilayer-perceptron-neural-network_fig5_316351306)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And finally, we have the &lt;strong&gt;Convolutional Neural Networks (CNN)&lt;/strong&gt;, which are the most common example of Feedforward Neural Networks. Inspired by the Visual Cortex, this topology divides data into small pieces and tries to learn essential patterns. This operation is called Convolution. More efficient than MLP, this topology is found widely in computer vision, video, and natural language applications. This topology has its own characteristic blocks, such as the convolution and pooling layers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/convolution-neural-network.png&quot; alt=&quot;Convolutional Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Convolutional Neural Network Example. Source: &lt;a href=&quot;https://missinglink.ai/guides/convolutional-neural-networks/convolutional-neural-network-tutorial-basic-advanced/&quot;&gt;Convolutional Neural Network Tutorial&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;recurring-neural-networks-rnn&quot;&gt;&lt;em&gt;Recurring Neural Networks (RNN)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Unlike Feedforward Neural Networks, in RNN information flows not only forward, but also backward, forming a cycle. For this, like CNN, they use several characteristical blocks, such as a memory block for example. This allows this topology to capture dynamic temporal patterns and be widely used in speech recognition problems and problems that require sequential linking.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/recorrent-neural-network.png&quot; alt=&quot;Recurrent Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Example of Recurrent Neural Network. Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg&quot;&gt;wikimedia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;types-of-activation-functions&quot;&gt;Types of Activation Functions&lt;/h2&gt;

&lt;p&gt;In addition to the step function, which I believe is not used in practice, there are several other activation functions. In addition to determining the model’s output, they also help with the accuracy of the results and the efficiency training. In practice, modern models use nonlinear activation functions, which are able to capture patterns in more complex data.&lt;/p&gt;

&lt;p&gt;The activation functions are used in two moments in the Neural Networks: to process the output of a single neuron, as we saw during the topic of hidden layers, and to process the output of the neural network as a whole.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/sigmoid.png&quot; alt=&quot;Sigmoid Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/tanh.png&quot; alt=&quot;Tanh Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/relu.png&quot; alt=&quot;Relu Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Normally Rectified Linear Unit (ReLU) functions are used in practice. The Sigmoid function is normally used to demonstrate how these elements work and is usually replaced by the Hyperbolic Tangent (TanH). Except in the case of the problem being a binary classification, in that case a Sigmoid function would be better in the model’s output as it would already deliver the result between 0 and 1.&lt;/p&gt;

&lt;p&gt;The choice of the activation function is motivated by the characteristics of the problem being solved. Sigmoid, for example, despite having a smoother gradient and normalizing the output between 0 and 1, has problems with vanish gradients and its output is not centered at zero. TanH has its center at zero, which facilitates the learning of the following layers, but disadvantages similar to Sigmoid.&lt;/p&gt;

&lt;p&gt;In addition to these, we can also highlight:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Leaky ReLU&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article we went through some of the main concepts of Artificial Neural Networks. After this review, I hope that you already have a more concrete idea of the basic concepts that involve one of the main topics of Deep Learning. Understanding the main building blocks of ANN, the main topologies and the most common activation functions, we can now move on to more advanced topics such as &lt;strong&gt;Forward and Backward Propagation&lt;/strong&gt; and &lt;strong&gt;Gradient Descent&lt;/strong&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="back-to-basics" />
      
        <category term="neural-networks" />
      

      
        <summary type="html">Learn the main concepts behind Neural Networks, one of Deep Learning’s pillars.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Batch vs Mini-batch vs Stochastic Gradient Descent with Code Examples</title>
      <link href="http://localhost:4000/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent" rel="alternate" type="text/html" title="Batch vs Mini-batch vs Stochastic Gradient Descent with Code Examples" />
      <published>2020-04-26T07:18:00-03:00</published>
      <updated>2020-04-26T07:18:00-03:00</updated>
      <id>http://localhost:4000/Batch%20vs%20Mini%20batch%20vs%20Stochastic%20Gradient%20Descent</id>
      <content type="html" xml:base="http://localhost:4000/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent">&lt;p&gt;What is the difference between these three Gradient Descent variants?&lt;/p&gt;

&lt;p&gt;One of the main questions that arises when studying Machine Learning and Deep Learning is the several types of Grandient Descent. Should I use Batch Gradient Descent? Mini-batch Gradient Descent or Stochastic Gradient Descent? In this post we are going to understand the difference between those concepts and take a look in code implementations from Gradient Descent, for the purpose of clarifying these methods.&lt;/p&gt;

&lt;p&gt;At this point, we know that our matrix of weights &lt;strong&gt;W&lt;/strong&gt; and our vector of bias &lt;strong&gt;b&lt;/strong&gt; are the core values of our Neural Networks (NN) (Check the Deep Learning Basics post). We can make an analogy with these concepts with the memory in which a NN stores patterns, and it is through tuning these parameters that we teach a NN. The acting of tuning this is done through the optimization algorithms, the amazing feature that allows NN to learn. After some time training the network, these patterns are learned and we have a set of weights and biases that hopefully correct classifies the inputs.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;One of the most common algorithms that help the NN to reach the correct values of weights and bias. The Gradient Descent (GD) is a method/algorithm to minimize the cost function J(W,b) in each step. It iteratively updates the weights and bias trying to reach the global minimum in a cost function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/gradient-descent.png&quot; alt=&quot;Gradient descent to minimize cost function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Minimizing the Cost Function, a Gradient Descent Illustration. Source: &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Stanford’s Andrew Ng’s MOOC Machine Learning Course&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Review this quickly, before we can compute the GD, first the inputs are taken and passed through all the nodes of a neural network, calculating the weighted sum of inputs, weights, and bias. This first pass is one of the main steps when calculating Gradient Descent and it is called &lt;strong&gt;Forward Propagation&lt;/strong&gt;. Once we have an output, we compare this output with the expected output and calculate how far it is from each other, the error. With this error, we can now propagate it backward, updating each and every weight and bias and trying to minimize this error. And this part is called, as you may anticipate, &lt;strong&gt;Backward Propagation&lt;/strong&gt;. The Backward Propagation step is calculated using derivatives and return the “gradients”, values that tell us in which direction we should follow to minimize the cost function.&lt;/p&gt;

&lt;p&gt;We are now ready to update the weight matrix W and the bias vector b. The gradient descent rule is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/f1.png&quot; alt=&quot;math1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In other words, the new weight/bias value will be the last one minus the gradient, moving it close to the global minimum value of the cost function. We also multiply this gradient to a learning rate alpha, which controls how big the step would be. For a more deep approach to Forward and Backward Propagation, Compute Losses,  Gradient Descent, check this post.&lt;/p&gt;

&lt;p&gt;This classic Gradient Descent is also called Batch Gradient Descent. In this method, every epoch runs through all the training dataset, to only then calculate the loss and update the W and b values. Although it provides stable convergence and a stable error, this method uses the entire training set; hence it is very slow for big datasets.&lt;/p&gt;

&lt;h2 id=&quot;mini-batch-gradient-descent&quot;&gt;&lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Imagine taking your dataset and dividing it into several chunks, or batches. So instead of waiting until the algorithm runs through the entire dataset to only after update the weights and bias, it updates at the end of each, so-called, mini-batch. This allows us to move quickly to the global minimum in the cost function and update the weights and biases multiple times per epoch now. The most common mini-batch sizes are 16, 32, 64, 128, 256, and 512. Most of the projects use Mini-batch GD because it is faster in larger datasets.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;minibatches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_mini_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mini_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

	    &lt;span class=&quot;c&quot;&gt;# Select a minibatch&lt;/span&gt;
	    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minibatch_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt;
	    &lt;span class=&quot;c&quot;&gt;# Forward propagation&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	    &lt;span class=&quot;c&quot;&gt;# Compute cost.&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	    &lt;span class=&quot;c&quot;&gt;# Backward propagation.&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	    &lt;span class=&quot;c&quot;&gt;# Update parameters.&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To prepare the mini-batches, one most apply some preprocessing steps: randomizing the dataset in order to random split the dataset and then partitioning it in the right number of chunks. But what happens if we chose to set the number of batches to 1 or equal to the number of training examples?&lt;/p&gt;

&lt;h2 id=&quot;batch-gradient-descent&quot;&gt;&lt;strong&gt;Batch Gradient Descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;As stated before, in this gradient descent, each batch is equal to the entire dataset. That is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/f2.png&quot; alt=&quot;math2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where {1} denotes the first batch from the mini-batch. The downside is that it takes too long per iteration. This method can be used to training datasets with less than 2000 training examples.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;(Batch) Gradient Descent&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Forward propagation&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Compute cost.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Backward propagation.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Update parameters.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;stochastic-gradient-descent&quot;&gt;&lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;On other hand, in this method, each batch is equal to one example from the training set. In this example, the first mini-batch is equal to the first training example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/f3.png&quot; alt=&quot;math3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where (1) denotes the first training example. Here the downside is that it loses the advantage gained from vectorization, has more oscilation but converges faster.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Forward propagation&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Compute cost&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Backward propagation&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Update parameters.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;It is essential to understand the difference between these optimization algorithms, as they compose a key function for Neural Networks. In summary, although Batch GD has higher accuracy than Stochastic GD, the latter is faster. The middle ground of the two and the most adopted, Mini-batch GD, combine both to deliver good accuracy and good performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gradient-descent-variants/batch-stochastic-mini-gd.png&quot; alt=&quot;Gradient descent variants&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Batch vs Stochastic vs Mini-batch Gradient Descent. Source: &lt;a href=&quot;https://www.coursera.org/learn/deep-neural-network/&quot;&gt;Stanford’s Andrew Ng’s MOOC Deep Learning Course&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is possible to use only the Mini-batch Gradient Descent code to implement all versions of Gradient Descent, you just need to set the mini_batch_size equals one to Stochastic GD or to the number of training examples to Batch GD. Thus, the main difference between Batch, Mini-batch and Stochastic Gradient Descent is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="back-to-basics" />
      
        <category term="gradient-descent" />
      

      
        <summary type="html">What is the difference between these three Gradient Descent variants?</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How to get started on Spacenet Challenge 6 using CosmiQ Baseline</title>
      <link href="http://localhost:4000/How-to-get-started-on-Spacenet-Challenge-6" rel="alternate" type="text/html" title="How to get started on Spacenet Challenge 6 using CosmiQ Baseline" />
      <published>2020-04-20T07:18:00-03:00</published>
      <updated>2020-04-20T07:18:00-03:00</updated>
      <id>http://localhost:4000/How%20to%20get%20started%20on%20Spacenet%20Challenge%206</id>
      <content type="html" xml:base="http://localhost:4000/How-to-get-started-on-Spacenet-Challenge-6">&lt;p&gt;Let’s implement the SpaceNet Baseline to join the Spacenet Challenge 6 and extract buildings footprints from satellite imagery.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://spacenet.ai/sn6-challenge/&quot;&gt;Spacenet Challagence 6&lt;/a&gt; has been launched almost a month ago and has been an amazing journey applying and understanding how to use Deep Learning in satellite images. I’ve been working with Machine/Deep Learning algorithms for remote sensing for a couple of months now and I’m amazed how deep and rich are the challenges one can find in this field.&lt;/p&gt;

&lt;p&gt;In this task, Spacenet challenges us to extract building footprints from satellite imagery using Synthetic Aperture Radar (SAR) and electro-optical (EO) imagery datasets. The area of interest (AOI) is Rotterdam, Netherlands, over 120 sq km and 48k building footprints labels. The high-resolution images are provided by Maxar’s WorldView 2 satellite. Although the participants could use both datasets (SAR and EO) for training, the test and scoring must be done using only the SAR dataset. This is intended to simulate real-world applications where one can not find matching data from both in the same location.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/663/0*vKaGlrZJP8YUVNyT.png&quot; alt=&quot;Test Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Example of EO and SAR image present in the dataset, with its inferred footprints and actual ground truth. Source: &lt;a href=&quot;https://medium.com/the-downlinq/the-spacenet-6-baseline-3b8ae8068351&quot;&gt;The SpaceNet 6 Baseline&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this tutorial, we are going to see how to implement the SpaceNet Baseline, as a first step to join the Spacenet Challenge 6. Implementing a baseline to enter a competition is not mandatory, but can help you in many ways. It can help you make sense of the dataset, have a benchmark to compare to, be a starting point or simply have a better feeling what your model’s output is supposed to look like.&lt;/p&gt;

&lt;p&gt;You can learn more about the Baseline &lt;a href=&quot;https://medium.com/the-downlinq/the-spacenet-6-baseline-3b8ae8068351&quot;&gt;here&lt;/a&gt;. In this baseline, CosmiQ implement a U-Net with a VGG-11 encoder. It is build using &lt;a href=&quot;https://github.com/CosmiQ/solaris&quot;&gt;Solaris&lt;/a&gt;, which is a CosmiQ Works Geospatial Machine Learning analysis toolkit based in Python, and can achieve a score of 0.21±.02 using the Jaccard Index, also called the Intersection-over-Union (IoU). You can check the full code here: &lt;a href=&quot;https://github.com/CosmiQ/CosmiQ_SN6_Baseline&quot;&gt;https://github.com/CosmiQ/CosmiQ_SN6_Baseline&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;walkthrough&quot;&gt;&lt;strong&gt;Walkthrough&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Although I think this tutorial can be used for any platform, at least to have a general overview, I’m currently working a Ubuntu Linux 18.04. So some of the commands and steps below are taking this into account.&lt;/p&gt;

&lt;p&gt;The CosmiQ Works provides the baseline using Docker, and not in a notebook to run on Jupyter or Colab, although you could try to “translate” it, I guess. Using docker is the best way to share the baseline to a vast number of people using different systems and keeping up all the libraries and dependencies together. So the first thing is to make sure you have the latest version of Docker installed. Although it would work with other versions, the latest version (Docker 19.03) natively supports NVIDIA GPUs as devices. Furthermore, we can use NVIDIA Container Toolkit to help us even more, since with this we don’t need to install the CUDA toolkit at the host. Allowing us to have multiple projects with multiple CUDA versions in your machine. You just need to install the NVIDIA driver at your host machine and the specific CUDA Toolkit inside your container.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/522/0*n3TB9NY4_3Xn7IOU.png&quot; alt=&quot;Test Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NVIDIA Container Toolkit. Source: &lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;nvidia-docker repository&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So go ahead and make sure you have installed in your system:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NVIDIA driver for your system&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Docker 19.03&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Git&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-datasets&quot;&gt;&lt;strong&gt;The Datasets&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The training data contains processed tiles of 450x450 m with its corresponding buildings footprints labels. The whole folder has about 39 GB. On the other hand, the test set has 17GB and each tile covers the same area. Both can be downloaded inside the container or in the host system. Either way, you are going to need an AWS account.&lt;/p&gt;

&lt;p&gt;Apparently, everyone with a normal Amazon account is already able to access AWS and get the credentials to download the images. If you don’t, you are going to need to create one and add a credit card. Once you have in hands your **&lt;a href=&quot;https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html&quot;&gt;Access key ID&lt;strong&gt; and &lt;/strong&gt;Secret access key**&lt;/a&gt;, you need to install the AWS CLI. This can be easily done in Ubuntu using pip or conda. I prefer using conda due to its intuitive virtual environments and features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using pip:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ sudo pip install awscli&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using conda:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ conda install -c conda-forge awscli&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now you need to configure the awscli using the credentials I mentioned before. So type this and fill with your keys:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ aws configure&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Awesome, now let’s download the images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training Data (~39GB)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ aws s3 cp s3://spacenet-dataset/spacenet/SN6_buildings/tarballs/SN6_buildings_AOI_11_Rotterdam_train.tar.gz .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test Data (~17GB)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ aws s3 cp s3://spacenet-dataset/spacenet/SN6_buildings/tarballs/SN6_buildings_AOI_11_Rotterdam_test_public.tar.gz .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you followed these steps inside the container, you are good to go. Otherwise, if you downloaded the files in your host system (which I think is better), you can transfer this data to your running container using:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ docker cp file_to_transfer.tar.gz ContainerID:/root/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will copy the dataset you choose to the same location where all the files of this project are.&lt;/p&gt;

&lt;p&gt;Furthermore, if you want to know more, have some questions on how to obtain the data or are using Windows, you can check &lt;a href=&quot;https://docs.google.com/document/d/1mkBKtSpeYlH3PxGTcLvzaXEarElUQCb1IThFclIXG0k/edit&quot;&gt;this guide&lt;/a&gt; to download these datasets using AWS.&lt;/p&gt;

&lt;h2 id=&quot;running-the-baseline-inside-the-container&quot;&gt;&lt;strong&gt;Running the baseline inside the container&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Now it is time to spin up this docker container and start using the baseline. Let’s go to the directory where we want to work on this project and clone the &lt;a href=&quot;https://github.com/CosmiQ/CosmiQ_SN6_Baseline&quot;&gt;baseline repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ git clone [https://github.com/CosmiQ/CosmiQ_SN6_Baseline.git](https://github.com/CosmiQ/CosmiQ_SN6_Baseline.git)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this folder, you will find several files that compose this baseline. The more important are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weights — A folder containing the best weights trained by CosmiQ.&lt;/li&gt;
  &lt;li&gt;baseline.py — Where all the magic happens. All the functions we need are here.&lt;/li&gt;
  &lt;li&gt;model.py — Holds the model, as it states. A U-Net and a VGG-11 written in Pytorch.&lt;/li&gt;
  &lt;li&gt;settings.sh, train.sh and test.sh — 3 shell scripts to configure, train and test the model.&lt;/li&gt;
  &lt;li&gt;Dockerfile — contains all the commands to assembly the Docker image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We then create the Docker image with the following command inside this folder:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ docker build --tag baseline .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then run it&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ sudo docker run -ti --name sb6 --gpus all baseline /bin/bash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are now inside the container, read to work in an environment prepared to develop our solution or work with the baseline to improve it. To run the baseline model in the test set and check the results you can simply run the test.sh script with two arguments: location of the test examples and where to save the .csv, which would be read to submission.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~$ ./test.sh /root/test_public/AOI_11_Rotterdam/ baseline_output.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command will search for the images in the root directory in the same structure from the .tar.gz file, and save the predictions .csv also in the root folder with the same “baseline_output”.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Unfortunately, my GPU is old (an NVIDIA GeForce 740M) and since version 0.3.1, Pytorch does not work with it because of Cuda capability. I then start a container without GPU to test it, it took 15 seconds for each inference, taking 8.5 hours to finish the 2000 test examples with an Intel(R) Core(TM) i5–3337U CPU @ 1.80GHz&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;You are now free to modify the files, make changes to the model, choose another encoder or try to improve this baseline.&lt;/p&gt;

&lt;p&gt;This tutorial was a walkthrough on how to join the Spacenet Challenge 6 using the baseline provided by CosmiQ. There are some minor details, like dependencies and drives, that slowed me down when I tried to implement this and I hope this tutorial helped you in some manner.&lt;/p&gt;

&lt;p&gt;Happy coding and I wish you the best of luck with this challenge!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="tutorials" />
      
        <category term="remote-sensing" />
      

      
        <summary type="html">Let’s implement the SpaceNet Baseline to join the Spacenet Challenge 6 and extract buildings footprints from satellite imagery.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Destaques de Maio em IA</title>
      <link href="http://localhost:4000/Destaques-de-Maio" rel="alternate" type="text/html" title="Destaques de Maio em IA" />
      <published>2000-05-28T07:15:00-03:00</published>
      <updated>2000-05-28T07:15:00-03:00</updated>
      <id>http://localhost:4000/Destaques%20de%20Maio</id>
      <content type="html" xml:base="http://localhost:4000/Destaques-de-Maio">&lt;p&gt;As 5 principais notícias e tendências da IA que cruzei em maio de 2020.&lt;/p&gt;

&lt;p&gt;English version &lt;a href=&quot;http://edgeaiguru.com/May-Highlights&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/google.gif&quot; alt=&quot;Aprendizado geral de representação visual do Google BiT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Modelo BiT sendo usado após o pré-treinamento e aplicado a outras tarefas com poucos exemplos rotulados&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-disponibiliza-bit-o-bert-para-visão-computacional&quot;&gt;Google disponibiliza BiT, o BERT para Visão Computacional&lt;/h2&gt;

&lt;p&gt;Em 21 de maio, a equipe de inteligência artificial do Google abriu o código do Big Transfer (BiT): um aprendizado geral sobre representação visual. Este é um esforço para ajudar aplicações de Visão Computacional a avançarem com modelos pré-treinados em larga escala. O uso do BiT é possível para obter desempenho de ponta em muitas aplicações, usando apenas alguns exemplos rotulados. Para fazer o ajuste fino desse modelo, eles também introduziram o “BiT-HyperRule”, uma abordagem heurística para selecionar hiper parâmetros, como taxa de aprendizado. Após a transferência de aprendizado, usando o conjunto de dados CIFAR-10, o modelo alcançou 64% e 95% de precisão média em 1 e 5 em &lt;em&gt;One Shot Learning&lt;/em&gt;, respectivamente. O Google também mencionou que “o BiT-L combina ou supera resultados […] em vários benchmarks de visão computacional, como Oxford &lt;a href=&quot;https://www.robots.ox.ac.uk / ~ vgg / data / pets /&quot;&gt;Pets&lt;/a&gt; e &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/flowers/&quot;&gt;Flowers&lt;/a&gt;, &lt;a href=&quot;https: //www.cs.toronto .edu / ~ kriz / cifar.html&quot;&gt;CIFAR&lt;/a&gt; etc. “. Você pode verificar o &lt;a href=&quot;https://arxiv.org/abs/1912.11370&quot;&gt;artigo&lt;/a&gt; e o &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;&gt;código&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/tf_logo_social.png&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-versão-220-do-tensorflow-chegou&quot;&gt;A versão 2.2.0 do Tensorflow chegou&lt;/h2&gt;

&lt;p&gt;Depois de quase quatro meses após o TF 2.1.0, este mês o Google também lançou o Tensorflow 2.2.0. As principais atualizações incluem um novo Profiler para CPU / GPU / TPU para ajudar a identificar gargalos de desempenho, melhorias de desempenho na computação entre dispositivos por meio de &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.distrbuite&lt;/code&gt;, um novo tipo escalar para strings (de&lt;code class=&quot;highlighter-rouge&quot;&gt; std :: string&lt;/code&gt; para &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow :: tstring&lt;/code&gt;) e a substituição do SWIG pelo pybind11 ao exportar funções C++. Também tivemos algumas melhorias no Keras, como o uso de lógicas de treinamento personalizadas com a substituição de &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.fit&lt;/code&gt; pelo ` Model.train_step` e o formato SavedModel que agora suporta todas as camadas internas do Keras (incluindo métricas, camadas de pré-processamento e RNN com monitoração de estado). Além disso, outro destaque é que eles abandonaram o suporte ao Python 2, que não é mantido desde janeiro de 2020. Por último, nesta versão, o TensorFlow requer a versão gast 0.3.3. Verifique o texto completo &lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/nvidia-a100.jpg&quot; alt=&quot;NVIDIA A100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-lança-a100-ai-gpu-o-melhor-instrumento-para-o-avanço-da-ia&quot;&gt;NVIDIA lança A100 AI GPU: o melhor instrumento para o avanço da IA&lt;/h2&gt;

&lt;p&gt;Anunciado em 14 de maio durante o evento Nvidia GTC 2020, o &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/a100/&quot;&gt;A100 AI Chip&lt;/a&gt; possui 54 bilhões de transistores e 5 petaflops de desempenho sendo a mais poderosa plataforma de IA e HPC para data centers, seguindo a tendência de HPC após &lt;a href=&quot;https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for- 6 a 9 bilhões&quot;&gt;adquirir a Mellanox em março&lt;/a&gt;. Com base na arquitetura Ampere, o chip de 7 nanômetros pode ser usado para computação científica, gráficos em nuvem e análise de dados. Aqui estão alguns números, 20 vezes mais potente que a GPU Tesla V100 anterior, o A100 alcança 19.5 de potência computacional com seus 6.912 núcleos CUDA, 40 GB de memória, 1,6 TB de largura de banda e a terceira geração de Tensors Core. Uma aplicação desse chip é o sistema &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;DGX&lt;/a&gt;, baseado em 8 NVIDIA A100 e direcionado ao setor de Inteligência Artificial, custa US$ 1 milhão. Podemos esperar ver o novo chip AI em serviços de nuvem em um futuro próximo. A Alibaba Cloud, AWS, Baidu Cloud, Google Cloud, Microsoft Azure, Oracle e Tencent Cloud já firmaram parceria com a NVIDIA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/fb.jpeg&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;facebook-ai-divulga-groknet-para-entendimento-avançado-do-produtos&quot;&gt;Facebook AI divulga GrokNet para entendimento avançado do produtos&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;“um sistema universal de visão computacional projetado para compras. Ele pode identificar atributos de produtos dentre bilhões de fotos - em diferentes categorias, como moda, automóveis e decoração de casa”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Diferente da maioria dos modelos que tem apenas uma categoria para aprender, o GrokNet foi treinado em sete conjuntos de dados, com vários tipos de supervisão e 83 funções de transferência, conseguiu prever categorias, atributos como cor e textura e prováveis consultas de pesquisa. O modelo do FB é treinado por um banco de dados colossal com cerca de 100 milhões de imagens retiradas da plataforma. A empresa também afirma que o sistema é capaz de identificar objetos que foram fotografados com condições de iluminação não muito boas e ângulos que não favorecem uma boa visualização do produto. Para dar ao modelo a capacidade de entender tarefas fáceis e difíceis, eles deram às tarefas uma combinação de peso e imagens para treinamento, ou seja, tarefas mais fáceis não precisam de muita supervisão e podem receber um peso menor. Comparado aos sistemas de atribuição de texto, o novo modelo pode prever anúncios de casas e jardins com precisão de 90%, contra 33% do antigo. O Facebook espera que “no futuro, o GrokNet possa ser usado para ajudar os clientes a encontrar facilmente exatamente o que estão procurando, receber sugestões personalizadas das vitrines sobre quais produtos são mais relevantes para eles e quais são compatíveis”. Verifique o anúncio oficial &lt;a href=&quot;https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/sudo.gif&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lançado-o-sudo-190-com-atualizações-de-segurança&quot;&gt;Lançado o Sudo 1.9.0 com atualizações de segurança&lt;/h2&gt;

&lt;p&gt;Comando básico em nossos trabalhos diários com nossos amados sistemas Unix, o sudo permite que os usuários executem programas como root. Em maio, o sudo recebeu uma atualização da versão 1.9.0 após 9 anos no &lt;em&gt;branch&lt;/em&gt; 1.8. Esta nova versão traz atualizações de segurança como, e centraliza o registro através do &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo_logsrvd&lt;/code&gt;, criando o sudo com a opção“ –enable-openssl ”, os dados são transmitidos através de um canal de comunicação criptografado (TLS). Agora também é possível usar softwares de terceiros para extrair dados de sessões sudo para auditoria. E embora o TF abandone o suporte ao Python 2, ele chega ao Sudo 1.9, onde os plugins podem ser escritos em python agora, que é ativado durante a compilação com a opção “–enable-python”. Verifique todas as alterações na versão completa &lt;a href=&quot;https://www.sudo.ws/stable.html#1.9.0&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Quer receber as notícias e tendências mais importantes da IA em apenas um e-mail a cada final de mês? Inscreva-se abaixo e vou te enviar as 10 notícias mais importantes em Inteligência Artificial.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="portuguese" />
      
        <category term="news" />
      

      
        <summary type="html">As 5 principais notícias e tendências da IA que cruzei em maio de 2020.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introdução a Redes Neurais</title>
      <link href="http://localhost:4000/Introdu%C3%A7%C3%A3o-a-Redes-Neurais" rel="alternate" type="text/html" title="Introdução a Redes Neurais" />
      <published>2000-05-03T07:18:00-03:00</published>
      <updated>2000-05-03T07:18:00-03:00</updated>
      <id>http://localhost:4000/Introdu%C3%A7%C3%A3o%20a%20Redes%20Neurais</id>
      <content type="html" xml:base="http://localhost:4000/Introdu%C3%A7%C3%A3o-a-Redes-Neurais">&lt;p&gt;Aprenda os principais conceitos por trás das Redes Neurais, um dos pilares do Deep Learning.&lt;/p&gt;

&lt;p&gt;English version &lt;a href=&quot;http://edgeaiguru.com/Introduction-to-Neural-Networks&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introdução&quot;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;A Inteligência Artificial vem revolucionando a indústria nos últimos anos e resolvendo problemas, que antes eram onerosos em tempo e dinheiro, de maneira muito mais eficaz. Problemas de visão computacional, processamento de linguagem natural e diversas outras aplicações só são possíveis graças aos avanços em Aprendizagem Profunda.&lt;/p&gt;

&lt;p&gt;As Redes Neurais Artificiais (RNA) são um dos principais pilares dessa tecnologia. Inspiradas no cérebro humano, as RNA levam esse nome pois tem conexões e motivações biológicas. Assim como no cérebro humano, onde unidade mais básica de processamento é o neurônio, as RNA possuem um elemento que processa impulsos, ou entradas, e que também é chamado de neurônio ou nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/artificial-vs-biological-neuron.png&quot; alt=&quot;Artificial vs Biological Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Neurônio Biológico vs Neurônio Artificial. Fonte: &lt;a href=&quot;https://www.datacamp.com/community/tutorials/deep-learning-python&quot;&gt;Keras Tutorial: Deep Learning in Python&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ambas estruturas compartilham o mesmo funcionamento para a transferência de informações: recebem uma entrada (impulso) que é carregada através do nó (corpo da célula) e ativam um certa saída (terminais axônicos). De mesma forma como nos neurônios biológicos, esse impulso nervoso que ativa o neurônios é reproduzida nas RNA através de funções de ativação.&lt;/p&gt;

&lt;p&gt;Logo, esse elemento básico das redes neurais podem ser representado pela seguinte figura, retirada do curso &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning&quot;&gt;Neural Networks and Deep Learning&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-neuron.png&quot; alt=&quot;Generic Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Onde, através do exemplo da necessidade de prever o preço de casas baseado no seu tamanho, podemos traçar um função que consiga representar esse problema. Nesse exemplo, uma função ReLU encaixa perfeitamente nos dados. Então, a mínima representação de um neurônio seria colocarmos na entrada a área de uma casa e, baseado na função matemática colocada “dentro” do neurônio, podemos estimar um preço para essa residência.&lt;/p&gt;

&lt;p&gt;Dessa forma, treinamos cada neurônio para ser ativado quando um certo padrão aparece. Assim, o agrupamento de diversos neurônios em série e em paralelo, permite as Redes Neurais a aprender a reconhecer padrões em imagens, textos, áudios e nas mais diversas formas de dados.&lt;/p&gt;

&lt;p&gt;Nesse artigo, será aprensentado os principais componentes das Redes Neurais Artificiais, algumas das principais arquiteturas, as funções de ativações mais comuns.&lt;/p&gt;

&lt;h2 id=&quot;redes-neurais-artificias&quot;&gt;Redes Neurais Artificias&lt;/h2&gt;

&lt;p&gt;Apesar das Redes Neurais terem algumas semelhanças com os neurônios do cérebro humano, essas são infinitamente mais simples do que seu correspondente biológico. Essas arquiteturas são compostas por blocos matemáticos que podem ser explicados utilizando álgebra e cálculo, muito diferentemente das diversas partes do cérebro que ainda não conseguimos entender.&lt;/p&gt;

&lt;p&gt;Os principais componentes das RNA são: a camada de entrada, as camadas ocultas e as camadas de saída. Essas camadas são ligadas através de conexões que têm pesos, esses definem o quão importante aquela conexão é para a rede. Além disso, como vimos anteriormente, na saída de cada neurônio existe um função de ativação que definirá se o neurônio irá ativar ou não.&lt;/p&gt;

&lt;h2 id=&quot;blocos-de-uma-rede-neural-artificial&quot;&gt;Blocos de uma Rede Neural Artificial&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-3-layers-neural-network.png&quot; alt=&quot;Generic 3 Layer Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Arquitetura de uma Rede Neural Genérica de 3 Camadas. Fonte: &lt;a href=&quot;https://cs231n.github.io/neural-networks-1/#nn&quot;&gt;Stanford CS231n&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;camada-de-entrada&quot;&gt;&lt;em&gt;Camada de Entrada&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Um bloco de neurônios pode ser chamado de camada. Mas perceba que apesar de os neurônios se interligarem entre camadas, eles não tem conexões dentro da mesma camada. Como mostra a figura acima, a primeira camada de uma Rede Neural é a camada de entrada. Esta tem apenas a função de passar as entradas do sistema para a próxima camada e não realiza nenhuma função matemática.&lt;/p&gt;

&lt;h3 id=&quot;camadas-ocultas&quot;&gt;&lt;em&gt;Camadas Ocultas&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Essa camada é responsável por uma das principais funções das redes neurais: processar os dados e enviá-los para a camada seguinte. O valor de cada neurônio é encontrado multiplicando o pesos W pela entrada X e somando um viés b. Esse valor então passa por uma função de ativação e é enviada a próxima camada, como mostra a Fig. 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/hidden-layer-mathematics.png&quot; alt=&quot;Hidden Layer Mathematics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Operações Matemáticas dentro do Neurônio. Fonte: Fonte: &lt;a href=&quot;https://cs231n.github.io/neural-networks-1/#nn&quot;&gt;Stanford CS231n&lt;/a&gt; Modificada.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Assim, se isolarmos o primeiro neurônio da primeira camada oculta, o valor de saída do neurônio será igual a z1. Onde &lt;em&gt;s1&lt;/em&gt; é a entrada do neurônio, onde multiplicamos os pesos pelas entradas e somamos um viés b. Após essa operação, é aplicada então uma função de transferência &lt;em&gt;g&lt;/em&gt; sobre o &lt;em&gt;s1.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;É importante notar que &lt;em&gt;X&lt;/em&gt; e &lt;em&gt;W&lt;/em&gt; na primeira equação são matrizes nesse caso, e representam todas as entradas e todos os pesos, respectivamente.&lt;/p&gt;

&lt;p&gt;Chamamos essa camada de “Camada Oculta” pois durante o treinamento de redes neurais temos as entradas que são conhecidas e as saídas que são esperadas. Mas não vemos quais os valores dentro dos neurônios dessa camada. Esse bloco pode conter diversas camadas ocultas, e quanto mais camadas mais “profunda” é a rede neural, e mais padrões ela consegue aprender.&lt;/p&gt;

&lt;h3 id=&quot;camadas-de-saída&quot;&gt;&lt;em&gt;Camadas de Saída&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;A camada de saída é a responsável por mostrar os resultados obtidos através dos cálculos feitos nas camadas ocultas. Normalmente é utilizada uma função de ativação, assim como a dos neurônios das camadas anteriores, para simplificar o resultado.&lt;/p&gt;

&lt;h3 id=&quot;pesos-e-viés&quot;&gt;&lt;em&gt;Pesos e Viés&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Os pesos são responsáveis por definir o quão importante aquela conexão é para a rede neural. Como existem diversas conexões dentro das RNA, é dessa forma que essa arquitetura entende quais padrões ela deve aprender e quais ela deve ignorar. Além disso, comumente é utilizado um valor chamado de viés junto aos pesos e as entradas. Esse valor ajuda a fazer um ajuste fino na rede neural. Dessa forma, se tivermos um neurônio i em uma camada e um neurônio j na camada seguinte, teremos um ligação com o peso Wij e um viés bij.&lt;/p&gt;

&lt;h3 id=&quot;funções-de-ativação&quot;&gt;&lt;em&gt;Funções de Ativação&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Também chamada de função de transferência, é o último processamento matemático que acontece antes da informação sair do neurônio. Esta equação matemática define se o neurônio será ativado ou não, podendo ser pode ser uma função degrau, uma função linear ou uma função não linear.&lt;/p&gt;

&lt;p&gt;A função de ativação mais simples seria a utilização de um degrau unitário. Onde o neurônio iria ativar somente caso a entrada fosse superior a um &lt;em&gt;threshold,&lt;/em&gt; e o sinal de entrada seria totalmente reproduzido na saída do nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/step-function.png&quot; alt=&quot;Step Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esta pode retornar valores de 0 e 1, utilizado em problemas de classificação, ou entre 0 e 1, utilizado em problemas que estamos mais interessados em saber a probabilidade de certa entrada fazer parte de certa classe.&lt;/p&gt;

&lt;h2 id=&quot;principais-tipos-de-redes-neurais-artificiais&quot;&gt;&lt;em&gt;Principais Tipos de Redes Neurais Artificiais&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Existem dois tipos principais de Redes Neurais: Redes Neurais Feedforward e Redes Neurais Recorrentes.&lt;/p&gt;

&lt;h3 id=&quot;redes-neurais-feedforward-rnf&quot;&gt;&lt;em&gt;Redes Neurais Feedforward (RNF)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Essa arquitetura é a mais comumente encontrada na literatura. Nela, a informação move-se em apenas uma direção: da entrada, passando pela camada oculta até o nós de saída, e não existem ciclos.&lt;/p&gt;

&lt;p&gt;A unidade mais simples dessa topologia é o Perceptron, a rede neural mais simples que é composta apenas por um nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/perceptron.png&quot; alt=&quot;Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;O Perceptron&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Alguns problemas simples podem ser resolvidos com o Perceptron, pois ele só funciona com funções linearmente separáveis.&lt;/p&gt;

&lt;p&gt;Com a necessidade de resolver problemas mais complexos e a partir dessa unidade básica, surge o &lt;strong&gt;Perceptron Multicamadas (MLP)&lt;/strong&gt;. Composto por diversas camadas desses nós, sendo muito mais úteis e podendo aprender funções não lineares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/multi-layer-perceptron.png&quot; alt=&quot;Multilayer Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Uma arquitetura de um Perceptron Multicamadas. Fonte:&lt;a href=&quot;https://www.researchgate.net/figure/Architecture-of-a-multilayer-perceptron-neural-network_fig5_316351306&quot;&gt;Advanced Methods for the Processing and Analysis of Multidimensional Signals: Application to Wind Speed&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;E por fim, temos as &lt;strong&gt;Redes Neurais Convolucionais (CNN)&lt;/strong&gt;, que são o exemplo mais comum das Redes Neurais Feedforward. Inspiradas no Córtex Visual, essa topologia divide os dados em pequenos pedaços e tentar aprender padrões essenciais. Essa operação é chamada de Convolução. Mais eficientes que os MLP, essa topologia é encontrada vastamente em aplicações de visão computacional, vídeo e linguagem natural. Essa topologia possui seus blocos característicos próprios, como as camadas de convolução e de &lt;em&gt;pooling.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/convolution-neural-network.png&quot; alt=&quot;Convolutional Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exemplo de Rede Neural Convolucional. Fonte: &lt;a href=&quot;https://missinglink.ai/guides/convolutional-neural-networks/convolutional-neural-network-tutorial-basic-advanced/&quot;&gt;Convolutional Neural Network Tutorial&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;redes-neurais-recorrentes-rnn&quot;&gt;&lt;em&gt;Redes Neurais Recorrentes (RNN)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Diferente das Redes Neurais Feedforward, nas RNN a informação flui não somente para frente, mas para trás também, formando um ciclo. Para isso, assim como as CNN, elas usam diversos blocos próprios, como um bloco de memória por exemplo. Isso permite essa topologia capturar padrões dinâmicos temporais e serem vastamente utilizados em problemas de reconhecimento de voz e problemas que exigem uma ligação sequencial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/recorrent-neural-network.png&quot; alt=&quot;Recurrent Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exemplo de Rede Neural Recorrente Fonte: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg&quot;&gt;wikimedia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;tipos-de-funções-de-ativação&quot;&gt;Tipos de Funções de Ativação&lt;/h2&gt;

&lt;p&gt;Além da função degrau unitário, que acredito não ser usada na prática, existem diversas outras funções de ativação. Além de determinarem a saída de um modelo, elas também ajudam na precisão dos resultados e na eficiência com que o modelo será treinado. Na prática, os modelos modernos usam funções de ativação não-linear, que são capazes de capturar padrões em dados mais complexos.&lt;/p&gt;

&lt;p&gt;As funções de ativação são usadas em dois momentos nas Redes Neurais: para processar a saída de um único neurônio, como vimos durante o tópico  de camadas ocultas, e para processar a saída da rede neural como um todo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/sigmoid.png&quot; alt=&quot;Sigmoid Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/tanh.png&quot; alt=&quot;Tanh Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/relu.png&quot; alt=&quot;Relu Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Normalmente são usadas funções &lt;em&gt;Rectified Linear Unit&lt;/em&gt; (ReLU) na prática. A função Sigmoid é normalmente utilizada para demostrar como esses elementos funcionam e normalmente é substituida pela Tangente Hiperbólica (TanH). Exceto no caso do problema tratar-se de uma classificação binária, nesse caso seria melhor uma função Sigmoid na saída do modelo pois está já entregaria o resultado entre 0 e 1.&lt;/p&gt;

&lt;p&gt;A escolha da função de ativação é motivada pelas características do problema que está sendo resolvido. A Sigmoid, por exemplo, apesar ter um gradient mais suave e normalizar a saída entre 0 e 1, tem problemas com &lt;em&gt;vanish gradients&lt;/em&gt; e sua saída não está centrada em zero. Já TanH tem o seu centro em zero, o que facilita o aprendizado das camadas seguintes, mas desvantagens parecidas com a Sigmoid.&lt;/p&gt;

&lt;p&gt;Além dessas, ainda podemos destacar:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Leaky ReLU&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Nesse artigo passamos por alguns dos principais conceitos de Redes Neurais Artificiais. Após essa revisão, espero que você já tenha uma idea mais concreta dos conceitos básicos que envolvem um dos principais tópicos de Aprendizagem Profunda. Entendendo os principais blocos construtores das RNA, as principais topologias e as funções de ativação mais comuns, podemos agora passar a tópicos mais avançados como &lt;strong&gt;Forward and Backward Propagation&lt;/strong&gt; e o &lt;strong&gt;Gradient Descent&lt;/strong&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="portuguese" />
      
        <category term="neural-networks" />
      

      
        <summary type="html">Aprenda os principais conceitos por trás das Redes Neurais, um dos pilares do Deep Learning.</summary>
      

      
      
    </entry>
  
</feed>
