<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Facial Keypoint Detection</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="The guidance through your Artificial Intelligence journey" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/Facial-Keypoint-Detection" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Edge AI Guru" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Facial Keypoint Detection" />
    <meta property="og:description" content="This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces. Let’s take a look at some examples of images and corresponding facial keypoints. Facial keypoints (also called facial landmarks) are the small" />
    <meta property="og:url" content="http://localhost:4000/Facial-Keypoint-Detection" />
    <meta property="og:image" content="http://localhost:4000/assets/images/facial-kpts/cover.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2020-06-15T07:15:00-03:00" />
    <meta property="article:modified_time" content="2020-06-15T07:15:00-03:00" />
    <meta property="article:tag" content="Projects" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Facial Keypoint Detection" />
    <meta name="twitter:description" content="This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces. Let’s take a look at some examples of images and corresponding facial keypoints. Facial keypoints (also called facial landmarks) are the small" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/images/facial-kpts/cover.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Edge AI Guru" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Projects" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Edge AI Guru",
        "logo": "http://localhost:4000/assets/images/blog-icon.png"
    },
    "url": "http://localhost:4000/Facial-Keypoint-Detection",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/images/facial-kpts/cover.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/Facial-Keypoint-Detection"
    },
    "description": "This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces. Let’s take a look at some examples of images and corresponding facial keypoints. Facial keypoints (also called facial landmarks) are the small"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Facial Keypoint Detection" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/"><img src="/assets/images/blog-icon.png" alt="Edge AI Guru" /></a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-tutorials" role="menuitem"><a href="/tag/tutorials/">Tutorials</a></li>
    <li class="nav-back-to-basics" role="menuitem"><a href="/tag/back-to-basics/">Back to Basics</a></li>
    <li class="nav-news" role="menuitem"><a href="/tag/news/">News</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Subscribe</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-projects post tag-projects ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="15 June 2020">15 June 2020</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/projects/'>PROJECTS</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">Facial Keypoint Detection</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/facial-kpts/cover.png)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.</p>

<p>Let’s take a look at some examples of images and corresponding facial keypoints.</p>

<p><img src="assets/images/facial-kpts/key_pts_example.png" alt="image-20200607150421443" /></p>

<p>Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and <strong>68 keypoints, with coordinates (x, y), for that face</strong>.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. This keypoint extraction project is part of the <a href="https://www.udacity.com/course/computer-vision-nanodegree--nd891">Udacity Computer Vision Nanodegree.</a> Here they are, numbered, and you can see that specific ranges of points match different portions of the face.</p>

<p><img src="assets/images/facial-kpts/landmarks_numbered.jpg" alt="image-20200607150421443" /></p>

<h2 id="tldr">TL;DR</h2>

<p>In this project, we defined and trained a CNN to identify and extract facial keypoints, founded in the <a href="https://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces Dataset</a>. Coded using Python and Pytorch, first we get familiar with the dataset and do all the processing and transformation using <code class="highlighter-rouge">torch.utils.data.Dataset</code> and <code class="highlighter-rouge">torch.utils.data.Dataloader</code>. Using a Custom LeNet-5 Architecture, we trained a model to identify such keypoints in the faces located by a OpenCV’s pre-trained Haar Cascade classifier. <a href="https://github.com/jacquesmats/facial_keypoint_detection">Code and tutorial here</a>.</p>

<h2 id="the-dataset">The Dataset</h2>

<p>The first step in working with any dataset is to become familiar with your data; we’ll need to load in the images of faces and their keypoints and visualize them! This set of image data has been extracted from the <a href="https://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces Dataset</a>, which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.</p>

<h4 id="training-and-testing-data">Training and Testing Data</h4>

<p>This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.</p>

<ul>
  <li>3462 of these images are training images, to create a model to predict keypoints.</li>
  <li>2308 are test images, which will be used to test the accuracy of the model.</li>
</ul>

<p>The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using <code class="highlighter-rouge">pandas</code>. When we read the training CSV and get the annotations, they are in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).</p>

<p>Below, in an output from the function <code class="highlighter-rouge">show_keypoints</code> that takes in an image and keypoints and displays them. As you look at this data, <strong>note that these images are not all of the same size</strong>, and neither are the faces! To eventually train a neural network on these images, we’ll need to standardize their shape.</p>

<p><img src="assets/images/facial-kpts/show_keypoints.png" alt="image-20200607150421443" /></p>

<p>In this project, to prepare our data for training, we used PyTorch’s Dataset class. Much of this this code is a modified version of what can be found in the <a href="http://pytorch.org/tutorials/beginner/data_loading_tutorial.html">PyTorch data loading tutorial</a>.</p>

<h4 id="dataset-class">Dataset class</h4>

<p><code class="highlighter-rouge">torch.utils.data.Dataset</code> is an abstract class representing a dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.</p>

<p>Our custom dataset should inherit <code class="highlighter-rouge">Dataset</code> and override the following methods:</p>

<ul>
  <li><code class="highlighter-rouge">__len__</code> so that <code class="highlighter-rouge">len(dataset)</code> returns the size of the dataset.</li>
  <li><code class="highlighter-rouge">__getitem__</code> to support the indexing such that <code class="highlighter-rouge">dataset[i]</code> can be used to get the i-th sample of image/keypoint data.</li>
</ul>

<p>Here we create a dataset class for our face keypoints dataset. We read the CSV file in <code class="highlighter-rouge">__init__</code> but leave the reading of images to <code class="highlighter-rouge">__getitem__</code>. This is memory efficient because all the images are not stored in the memory at once but read as required.</p>

<p>A sample of our dataset will be a dictionary <code class="highlighter-rouge">{'image': image, 'keypoints': key_pts}</code>. Our dataset will take an
optional argument <code class="highlighter-rouge">transform</code> so that any required processing can be applied on the sample.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FacialKeypointsDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="s">"""Face Landmarks dataset."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">root_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_pts_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_pts_frame</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">image_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">key_pts_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        
        <span class="n">image</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span>
        
        <span class="c"># if image has an alpha color channel, get rid of it</span>
        <span class="k">if</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
        
        <span class="n">key_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_pts_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
        <span class="n">key_pts</span> <span class="o">=</span> <span class="n">key_pts</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="p">{</span><span class="s">'image'</span><span class="p">:</span> <span class="n">image</span><span class="p">,</span> <span class="s">'keypoints'</span><span class="p">:</span> <span class="n">key_pts</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sample</span>
</code></pre></div></div>

<p>After we’ve defined this class, we can instantiate the dataset and display some images.</p>

<p><img src="assets/images/facial-kpts/sample0.png" alt="image-20200607150421443" /></p>

<h3 id="transforms">Transforms</h3>

<p>Now, the images in this dataset are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.</p>

<p>Therefore, we will need to write some pre-processing code. The following four transforms were created:</p>

<ul>
  <li><code class="highlighter-rouge">Normalize</code>: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]</li>
  <li><code class="highlighter-rouge">Rescale</code>: to rescale an image to a desired size.</li>
  <li><code class="highlighter-rouge">RandomCrop</code>: to crop an image randomly.</li>
  <li><code class="highlighter-rouge">ToTensor</code>: to convert numpy images to torch images.</li>
</ul>

<p>We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called. For this, we just implemented <code class="highlighter-rouge">__call__</code> method and (if we require parameters to be passed in), the <code class="highlighter-rouge">__init__</code> method.  We can then use a transform like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tx = Transform(params)
transformed_sample = tx(sample)
</code></pre></div></div>

<p>Observe below how these transforms are generally applied to both the image and its keypoints. Let’s test these transforms out to make sure they behave as expected. As you look at each transform, note that, in this case, <strong>order does matter</strong>. For example, you cannot crop a image using a value smaller than the original image (and the original images vary in size!), but, if you first rescale the original image, you can then crop it to any size smaller than the rescaled size.</p>

<p><img src="assets/images/facial-kpts/transform.png" alt="image-20200607150421443" /></p>

<p>After creating the transformed dataset, applying the transforms in order to get grayscale images of the same shape, verify that your transform works, we used <code class="highlighter-rouge">torch.utils.data.DataLoader</code> iterator which provides features like Batch the data, Shuffle the data and Load the data in parallel using <code class="highlighter-rouge">multiprocessing</code> workers.</p>

<p>Now that we load and transform the data, we are ready to build a neural network to train on this data.</p>

<h2 id="the-convolutional-neural-network">The Convolutional Neural Network</h2>

<p>After looking at the data we’re working with and, in this case, know the shapes of the images and of the keypoints, we can define a convolutional neural network that can <em>learn</em> from this data. Recall that CNN’s are defined by a few types of layers:</p>
<ul>
  <li>Convolutional layers</li>
  <li>Maxpooling layers</li>
  <li>Fully-connected layers</li>
</ul>

<h3 id="pytorch-neural-nets">PyTorch Neural Nets</h3>

<p>To define a neural network in PyTorch, we define the layers of a model in the function <code class="highlighter-rouge">__init__</code> and define the feedforward behavior of a network that employs those initialized layers in the function <code class="highlighter-rouge">forward</code>, which takes in an input image tensor, <code class="highlighter-rouge">x</code>. Also note that during training, PyTorch will be able to perform backpropagation by keeping track of the network’s feedforward behavior and using autograd to calculate the update to the weights in the network.</p>

<p>Best practice is to place any layers whose weights will change during the training process in <code class="highlighter-rouge">__init__</code> and refer to them in the <code class="highlighter-rouge">forward</code> function; any layers or functions that always behave in the same way, such as a pre-defined activation function, should appear <em>only</em> in the <code class="highlighter-rouge">forward</code> function.</p>

<p>The model used in this project was a Custom LeNet-5 Architecture, which you can check in the <a href="https://github.com/jacquesmats/facial_keypoint_detection">repository</a> under the file <code class="highlighter-rouge">models.py</code>, using 4 Convolutional Layers followed by a Poooling Layer and 2 Fully Connected. After reading the paper from <a href="https://arxiv.org/pdf/1710.00977.pdf">NaimishNet</a>, I realized, from Fig.21, that LeNet-5 had a good performance while with fewer layers than NaimishNet. I started if LeNet-5 and change it until reaching the final mode. Adding two more Conv. Layers, replacing AvgPool by MaxPool (since were more common use avg back then) and Tanh activations by ReLU, improvements made in this 20 years.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Net(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=25600, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=136, bias=True)
)
</code></pre></div></div>

<h3 id="feature-maps">Feature maps</h3>

<p>Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it’s convolutional kernels and when these kernels are applied to some input image, they produce a set of <strong>feature maps</strong>. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.</p>

<p><img src="assets/images/facial-kpts/feature_map_ex.png" alt="image-20200607150421443" /></p>

<h3 id="training">Training</h3>

<p>To prepare for training, create a transformed dataset of images and keypoints. In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so your model’s loss does not blow up during training, it is also suggested that you normalize the input images and keypoints. The necessary transforms have been defined in <code class="highlighter-rouge">data_load.py</code>.</p>

<p>Next, having defined the transformed dataset, we use PyTorch’s DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in <a href="http://pytorch.org/docs/master/data.html">this documentation</a>.</p>

<p>We also defined some functions:</p>

<ul>
  <li>net_sample_output(): test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints.</li>
  <li>show_all_keypoints(): displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided).</li>
  <li>visualize_output(): this function’s main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors <em>and</em> it undoes the keypoint normalization done in the Normalize() transform; it’s assumed that you applied these transformations when you loaded your test data.</li>
</ul>

<p>After testing the optimizer with Adam and SGD, I kept Adam because is faster and converges faster also. About the loss functions, MSE and L1Smooth had a similar performance with 5 epochs, so I kept because of the NaimishNet reference. In 60 epochs, the average loss was 0.000651, resulting in the following detection:</p>

<p><img src="assets/images/facial-kpts/download.png" alt="image-20200612307150421443" /></p>

<p><img src="assets/images/facial-kpts/download2.png" alt="image-20200602137150421443" /></p>

<p><img src="assets/images/facial-kpts/download3.png" alt="image-20200607121350421443" /></p>

<h2 id="face-and-facial-keypoint-detection">Face and Facial Keypoint detection</h2>

<p>After you’ve trained a neural network to detect facial keypoints, you can then apply this network to <em>any</em> image that includes faces. The neural network expects a Tensor of a certain size as input and, so, to detect any face, we first have to do some pre-processing. The following image will be used for test.</p>

<p><img src="assets/images/facial-kpts/obamas.jpg" alt="image-20200607150421443" /></p>

<h3 id="detect-all-faces-in-an-image">Detect all faces in an image</h3>

<p>Here used one of OpenCV’s pre-trained Haar Cascade classifiers, all of which can be found in the <code class="highlighter-rouge">detector_architectures/</code> directory, to find any faces in your selected image. In the figure below, we loop over each face in the original image and draw a red square on each face (in a copy of the original image, so as not to modify the original). You can even <a href="https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html">add eye detections</a> while using Haar detectors. An example of face detection is shown below.</p>

<p><img src="assets/images/facial-kpts/obamamichele.png" alt="image-2020060715210421443" /></p>

<h3 id="keypoint-detection">Keypoint detection</h3>

<p>Now, we loop over each detected face in an image (again!) only this time, you’ll transform those faces in Tensors that your CNN can accept as input images. First we convert the face from RGB to grayscale and normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]. So we rescale the detected face to be the expected square size for your CNN (224x224, suggested) and reshape the numpy image into a torch image.</p>

<p>And finally, after each face has been appropriately converted into an input Tensor for your network to see as input, we apply our <code class="highlighter-rouge">net</code> to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be “un-normalized” for display, and we write a helper function like <code class="highlighter-rouge">show_keypoints</code>. We end up with an image like the following with facial keypoints that closely match the facial features on each individual face:</p>

<p><img src="assets/images/facial-kpts/obama.png" alt="image-20200123607150421443" /></p>

<p><img src="assets/images/facial-kpts/michele.png" alt="image-20200607131250421443" /></p>

<hr />

<p>Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence</p>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Edge AI Guru</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <!--
<form method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  /><input class="location" type="hidden" name="location"  /><input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email"  placeholder="youremail@example.com" />
    </div>
    <button class="" type="submit" disabled><span>Subscribe</span></button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>
Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<div id="mc_embed_signup">
<form action="https://live.us19.list-manage.com/subscribe/post?u=1a75f176c221a80d91de5e939&amp;id=671c7f7a46" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="subscribe-email" id="mce-EMAIL" placeholder="youremail@example.com" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1a75f176c221a80d91de5e939_671c7f7a46" tabindex="-1" value=""></div>
    <!--<button class="" type="submit" disabled id="mc-embedded-subscribe"><span>Subscribe</span></button> -->
    <div class=""><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div> 
    </div>
</form>
</div>

<!--End mc_embed_signup-->
                </section>
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/images/matheus.png" alt="matheus" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/matheus">Matheus Jacques</a></h4>
                                
                                    <p>Engineer and entrepreneur with 4 years in R&D, 2 years as a Startup Founder and 2 year dealing with Business/Data Analytics. Programming since 2010.</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/matheus">Read More</a>
                        </div>
                    
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            this.page.url = 'http://localhost:4000/Facial-Keypoint-Detection';
                            this.page.identifier = '/Facial Keypoint Detection';
                            this.page.title = 'Facial Keypoint Detection';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://edgeaiguru.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/images/blog-cover.jpg)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Edge AI Guru &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/projects/">Projects</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/3D-Object-Tracking">Tracking 3D objects with Lidar and Camera</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Lidar-Obstacles-Detection">Lidar Obstacles Detection</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Image-Captioning">Image Captioning</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/projects/">
                                
                                    See all 5 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Slam">
                <div class="post-card-image" style="background-image: url(/assets/images/slam/cover.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Slam">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Projects</span>
                            
                        
                    

                    <h2 class="post-card-title">Landmark Detection & Robot Tracking (SLAM)</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p></p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/matheus.png" alt="Matheus Jacques" />
                        
                        <span class="post-card-author">
                            <a href="/author/matheus/">Matheus Jacques</a>
                        </span>
                    
                
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Feedforward-and-Backpropagation">
                <div class="post-card-image" style="background-image: url(/assets/images/feed-back-prop/cover.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Feedforward-and-Backpropagation">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Back-to-basics</span>
                            
                        
                    

                    <h2 class="post-card-title">Gradient Descent, Feedforward and Backpropagation</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p></p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/matheus.png" alt="Matheus Jacques" />
                        
                        <span class="post-card-author">
                            <a href="/author/matheus/">Matheus Jacques</a>
                        </span>
                    
                
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
                <img src="/assets/images/favicon.png" alt="Edge AI Guru icon" />
            
            <span>Edge AI Guru</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Facial Keypoint Detection</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Facial+Keypoint+Detection&amp;url=Facial-Keypoint-Detection"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=Facial-Keypoint-Detection"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">Edge AI Guru</a> &copy; 2020</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                    <img class="subscribe-overlay-logo" src="/assets/images/blog-icon.png" alt="Edge AI Guru" />
                
                <h1 class="subscribe-overlay-title">Subscribe to Edge AI Guru</h1>
                <p class="subscribe-overlay-description">Stay up to date! Get all the latest &amp; greatest posts delivered straight to your inbox</p>
                <!--
<form method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  /><input class="location" type="hidden" name="location"  /><input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email"  placeholder="youremail@example.com" />
    </div>
    <button class="" type="submit" disabled><span>Subscribe</span></button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>
Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<div id="mc_embed_signup">
<form action="https://live.us19.list-manage.com/subscribe/post?u=1a75f176c221a80d91de5e939&amp;id=671c7f7a46" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="subscribe-email" id="mce-EMAIL" placeholder="youremail@example.com" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1a75f176c221a80d91de5e939_671c7f7a46" tabindex="-1" value=""></div>
    <!--<button class="" type="submit" disabled id="mc-embedded-subscribe"><span>Subscribe</span></button> -->
    <div class=""><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div> 
    </div>
</form>
</div>

<!--End mc_embed_signup-->
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-166032640-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
