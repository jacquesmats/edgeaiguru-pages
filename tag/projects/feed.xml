<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/projects/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-02-16T20:27:07+01:00</updated>
  <id>http://localhost:4000/tag/projects/feed.xml</id>

  
  
  

  
    <title type="html">Edge AI Guru | </title>
  

  
    <subtitle>The guidance through your Artificial Intelligence journey</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">O impacto da Inteligência Artificial no Controle de Qualidade</title>
      <link href="http://localhost:4000/Inspe%C3%A7%C3%A3o-usando-CV" rel="alternate" type="text/html" title="O impacto da Inteligência Artificial no Controle de Qualidade" />
      <published>2021-02-02T11:15:00+01:00</published>
      <updated>2021-02-02T11:15:00+01:00</updated>
      <id>http://localhost:4000/%20Inspe%C3%A7%C3%A3o%20usando%20CV</id>
      <content type="html" xml:base="http://localhost:4000/Inspe%C3%A7%C3%A3o-usando-CV">&lt;p&gt;Abordamos o impulsionamento da indústria com a integração da IA e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante&lt;/p&gt;

&lt;p&gt;A Inteligência Artificial vem revolucionando a indústria nos últimos anos e resolvendo problemas, que antes eram onerosos em tempo e dinheiro, de maneira muito mais eficaz. Problemas de visão computacional, processamento de linguagem natural e diversas outras aplicações só são possíveis graças aos diversos avanços na área.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/ai-ml-dl.png&quot; alt=&quot;ai-ml-dl&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A inteligência artificial e suas subáreas.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Inteligência artificial pode ser definida como qualquer método que permite computadores imitar a inteligência humana. Já a parte de Aprendizado de Máquina (Machine Learning) é a subárea de Inteligência Artificial que aprende com os dados utilizando métodos estatísticos sem ser diretamente programada para isso. Parte do Aprendizado de Máquina, o Aprendizado Profundo (Deep Learning) é a área em que programas aprendem sozinhos sobre características importantes sobre os dados, viabilizando tarefas como reconhecimento de padrões em voz ou imagens.&lt;/p&gt;

&lt;p&gt;As Redes Neurais Artificiais (RNA) são um dos principais pilares dessa tecnologia. Inspiradas no cérebro humano, as RNA levam esse nome pois tem conexões e motivações biológicas. Assim como no cérebro humano, onde unidade mais básica de processamento é o neurônio, as RNA possuem um elemento que processa impulsos, ou entradas, e que também é chamado de neurônio ou nó.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inteligência Artificial na Indústria como forma de reduzir custos&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Podemos utilizar todo o espectro da Inteligência Artificial na indústria, desde Machine Learning até Aprendizado Profundo. Com ela, é possível otimizar processos com sistemas de automação inteligentes, aumentar o trabalho humano e impulsionar inovações. Sistemas de predição usando Machine Learning já são encontrados em áreas como manutenção preventiva, segurança de rede, processo de logística, gestão de inventário, gestão de ativos e gestão da cadeia de abastecimento. Por outro lado, recentemente com os avanços dos métodos de aprendizado profundo se tornaram mais comum em aplicações de visão computacional, principalmente no controle de qualidade. Essa tecnologia dá às máquinas sentidos mais humanos, como diferenciar e identificar objetos. Aqui estão alguns exemplos de aplicação de aprendizagem profunda na indústria que otimizam processos já existentes: identificação de produtos com defeito, detecção de anomalias e manutenção preditiva com redes neurais. Quando falamos diretamente de custos para o cliente, modelos são criados com o objetivo de realizar uma tarefa com precisão e também de otimizar um processo que permite a economia de dinheiro aos cofres da empresa. Segundo um &lt;a href=&quot;https://www.accenture.com/pt-pt/insight-ai-industry-growth&quot;&gt;estudo feito pela Accenture Research&lt;/a&gt;, o crescimento da margem de lucro na indústria americana vem diminuindo desde 2010. Isso acontece devido a fatores como como a falta de investimento em inovação e P&amp;amp;D. Por outro lado, a inovação é o que ajudará a indústria a voltar a lucrar. Com o aumento do número de sensores nas fábricas, a queda do preço de poder computacional e avanços em campos coma a Inteligências Artificial, a tecnologia será cada vez mais comum em processos industriais. De fato, na manufatura, a Inteligência Artificial encontra um ambiente favorável para inovar, já que essa área tem precursores como a Internet das Coisas (IoT).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/ind-grow.png&quot; alt=&quot;ind-grow&quot; /&gt;&lt;em&gt;Taxas de crescimento anuais em 2035 do valor bruto acrescentado (aproximadas ao PIB), comparando a linha base de crescimento em 2035 com um cenário de inteligência artificial, onde a IA foi absorvida pela economia. Fonte: Accenture e Frontier Economics&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Normalmente atribuído a humanos, trabalhos repetitivos são os que geram um maior número de erros e muita vezes também são os com maiores riscos. Assim, a substituição da mão de obra humana por processos automatizados ajuda reduzir significativamente o número e erros e os riscos na produção, diminuindo assim os custos. A pesquisa da Accenture ainda mostra que a IA tem potencial para aumentar as taxas de lucratividade em uma média de 38% até 2035 e levar a um aumento econômico de US $ 14 trilhões em 16 setores em 12 economias até 2035. O impacto das tecnologias de IA nos negócios é projetado para aumentar a produtividade do trabalho em até 40 por cento - e permitir que as pessoas façam um uso mais eficiente de seu tempo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aplicação de Deep Learning em Sistemas de Controle de Qualidade:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O uso da tecnologia de visão por máquina mostra-se como um verdadeiro aliado no controle de inspeção de qualidade de produtos em linhas de produção. Durante décadas, existem implementações que possibilitam analisar diversos produtos através de câmeras com algoritmos de processamento de imagem, onde um funcionário escolhe quais características são importantes para o problema, como arestas, cantos, cores, etc. A partir disso, definem-se regras através da extração manual de características que definem se há ou não problema no produto visualizado, tornando-se um método simples e consideravelmente eficaz.&lt;/p&gt;

&lt;p&gt;Com a revolução da Indústria 4.0, o desenvolvimento de sistemas autônomos e automatizados vem ganhando espaço. Dessa forma, foram desenvolvidos algoritmos de &lt;em&gt;Deep Learning&lt;/em&gt;, que é uma ferramenta poderosa que possibilita alavancar os sistemas de inspeção de qualidade. Esse método possui uma característica única: aprender a partir dos próprios dados. O processo utiliza das fotografias capturadas pelas câmeras, e cria regras próprias, customizadas e adaptadas para cada problema, sem necessidade da presença humana.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/process.png&quot; alt=&quot;process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Comparação entre sistemas de visão de máquina existentes e a simplificação através do uso de Deep Learning.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Além disso, essa tecnologia possui a flexibilidade de utilizar modelos treinados em problemas similares como um ponto de início para resolver um novo problema. Dessa forma, é possível otimizar as métricas de precisão e também economizar tempo de treino, visto que é necessário entregar ao modelo menos imagens para o aprendizado. Outro aliado da visão computacional são os algoritmos de detecção de objetos, que também partem de modelos pré-treinados em competições, e são necessários para localizar precisamente defeitos em objetos. Um exemplo de aplicação disso é o controle de qualidade em garrafas de plástico que implementamos na Fox IoT, em que o modelo consegue encontrar precisamente a localização de um rótulo defeituoso e consequentemente classifica-o corretamente.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/inspection/cola.png&quot; alt=&quot;cola&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Garrafa com rótulo defeituoso e inferência do algoritmo de inspeção visual.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Além do rótulo defeituoso, nossa inspeção de qualidade também é capaz de identificar quando a garrafa está amassada, sem tampa, sem rótulo ou com preenchimento de líquido incorreto, tornando uma solução completa para a indústria manufatureira. Devido à flexibilidade do serviço, podemos também fazer a contagem  de produtos na linha de produção e abrangir para outras manufaturas, desde integridade de caixas até falhas em confecções de tecidos.&lt;/p&gt;

&lt;p&gt;Assim explicado de forma geral o impulsionamento da indústria com a integração da inteligência artificial e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante, percebe-se que esses processos otimizados geram cada vez mais valor ao cliente final, possibilitando a redução de tempo e custos na produção.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Abordamos o impulsionamento da indústria com a integração da IA e uma demonstração de um serviço de inspeção de qualidade em garrafas de refrigerante</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Tracking 3D objects with Lidar and Camera</title>
      <link href="http://localhost:4000/3D-Object-Tracking" rel="alternate" type="text/html" title="Tracking 3D objects with Lidar and Camera" />
      <published>2020-07-23T12:15:00+02:00</published>
      <updated>2020-07-23T12:15:00+02:00</updated>
      <id>http://localhost:4000/3D%20Object%20Tracking</id>
      <content type="html" xml:base="http://localhost:4000/3D-Object-Tracking">&lt;p&gt;Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)&lt;/p&gt;

&lt;p&gt;
    Autonomous vehicles technology has been on my radar (no pun intended) for a while now. I was excited to apply AI to some very complex problems and this project was for sure one of the most amazing I ever did. More than Machine and Deep Learning techniques, solving autonomous driving requires several technologies and methods. All this is only possible through fusing sensors to make sense of the environment.
&lt;/p&gt;
&lt;p&gt;
    To fusion two sensors was a completely new challenge for me and seeing the results is awesome. After some introduction on how camera technology works and how optics are applied in modern systems, some image processing, and computer vision applications were covered,  to better understand how to fusion Lidar and Camera data. From this point on, I tried to apply this knowledge to a collision avoidance project for vehicles. 
&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T2UWLYwpuv4?controls=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/structure.png&quot; alt=&quot;Structure&quot;&gt;&lt;/p&gt;

&lt;i&gt;Project Structure. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;



&lt;p&gt;
    From cameras we detect images key points, such as tail lights, extract these keypoints descriptors and match them between successive images and observe its distances, computing Camera TCC. From Lidar we take a 3D Point Cloud, crop the points(discard road and bad measurements) and cluster these into objects, identified using Neural Networks, and provided with bounding boxes. We than fusion both information, connecting them into space and tracking 3D object bounding boxes to compute the Lidar TCC on the object in front of the car. Conducting various tests with the framework, we tried to identify the most suitable detector/descriptor combination for TTC estimation and also to search for problems that can lead to faulty measurements by the camera or Lidar sensor.
&lt;/p&gt;

&lt;h2&gt;The Technology&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;	
    To make it happen, everything was coded in C++ using the &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti&quot;&gt;KITTI dataset&lt;/a&gt;. Besides, the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;github&lt;/a&gt;. This project is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;.
&lt;/p&gt;
&lt;blockquote cite=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;
    &lt;p&gt;Our recording platform is a Volkswagen Passat B6, which has been modified with actuators for the pedals (acceleration and brake) and the steering wheel. The data is recorded using an eight core i7 computer equipped with a RAID system, running Ubuntu Linux and a real-time database. We use the following sensors:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;1 Inertial Navigation System (GPS/IMU): OXTS RT 3003&lt;/li&gt;
        &lt;li&gt;1 Laserscanner: Velodyne HDL-64E&lt;/li&gt;
        &lt;li&gt;2 Grayscale cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3M-C)&lt;/li&gt;
        &lt;li&gt;2 Color cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3C-C)&lt;/li&gt;
        &lt;li&gt;4 Varifocal lenses, 4-8 mm: Edmund Optics NT59-917&lt;/li&gt;
    &lt;/ul&gt;
&lt;/blockquote&gt;


&lt;br&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/passat_sensors.png&quot; alt=&quot;Passat&quot;&gt;&lt;/p&gt;
&lt;i&gt;KITTI Passat Sensor Setup. Source: &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;





&lt;h2&gt;The Project&lt;/h2&gt;
&lt;br&gt;
&lt;p&gt;
    Cameras are one mature technology and the main sensor in autonomous vehicles. In a world made by humans, everything is adapted to our senses and driving would not be different. Therefore, its main advantage is to &quot;simulate&quot; the human vision being able to interpret 2D information as road signs and lanes. At the same time, it has challenges as we humans: bad performance in darkness or bad weather conditions. Thus, alone in some applications, the camera will fail, hence the need to add a second sensor such as Radar or Lidar. To fill the gap, Lidar has high resolution and can reconstruct 3D objects while Radar has a greater range and can detect velocity more accurately.
&lt;/p&gt;
&lt;p&gt;
    In this project, the main objective was to estimate the Time to Collision(TCC) using a camera-based object classification to cluster Lidar points and from 3D bounding boxes compute TCC. Human reflection time is around 1 second, so the system has to warn us way before and starting breaking 2 to 3 seconds before the collision. But how do we tell a machine a collision is near? How do we compute such time? The first concern is to choose a Motion Model: Constant Velocity(CVM) or Constant Acceleration(CAM). While CAM models best real-world situations, CVM assumes that the velocity does not change and it was chosen for simplicity, leaving space for us to focus more on the computer vision aspects.  
&lt;/p&gt;
&lt;p&gt;
    From that, the problem can be divided into two main parts: estimate TTC from Lidar points and estimate it using camera successive images. The former is pretty straightforward, addressing only the preceding car and calculating the time from the X component in the Point Cloud.  Based on the CVM, we can compute the velocity based in two Lidar measurements over time, taking the closest Lidar point in the path of the car and observing how it changes in a time window. To avoid erroneous measurements that would lead to false TCC, we perform a &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule&quot;&gt;IQR range values&lt;/a&gt; to remove outliers. Due to some measurement accuracy based on the amount of light reflected from an object, when using Lidar for TCC is advised to remove measurement of the road surface and low reflectivity points, making it easy to measure the distance of the preceding vehicle. Furthermore, instead of estimating the velocity after computing the vehicle velocity and two distance measurements, that could be noisy, we could use a Radar sensor that measures directly the relative speed. 
&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/keypoints.jpg&quot; alt=&quot;Keypoints&quot;&gt;&lt;/p&gt;

&lt;i&gt;Keypoints detection and tracking using descriptors. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;    
    In another hand, it is hard to measure metric distances using only one camera, some companies achieve measuring distance using a stereo camera setup, like &lt;a href=&quot;http://www.6d-vision.com/&quot;&gt;Mercedes-Benz which pioneered in this technology&lt;/a&gt;. With two pictures of the same place, taken from two different points of view, is possible to locate points of interest in both images and calculate its distance using geometry and perspective projection. Using a mono camera we can estimate TCC only by observing relative height change (or scale change) in the preceding car, for example, without distance measurement. Here enters Deep Learning, allowing us to identify cars in images. As showed in the figure above, it is also used CV techniques to find key points and track them using descriptors from one frame to the next, estimating the scale change in the object. Tracking how these descriptors change over time, we can estimate TTC.
&lt;/p&gt;
&lt;p&gt;
    First, we focus on loading images, setting up data structures, and putting everything into a ring buffer to optimize memory load. Then, integrated several keypoint detectors such as HARRIS, FAST, BRISK, and SIFT and compare them with regard to the number of key points and speed. For the descriptor extraction and matching, we used brute force and also the FLANN approach and tested the various algorithms in different combinations and compare them with regard to some performance measures. 
    Counting the number of key points on the preceding vehicle for 10 images and taking note of the distribution of their neighborhood size, doing this for several detectors implemented, we noted that FAST, BRISK, and AKASE were the detectors that identified the larger number of keypoints.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/comparision_table.png&quot; alt=&quot;Table&quot;&gt;&lt;/p&gt;
&lt;i&gt;Comparision table between several detectors to identify keypoints&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;
    During the matching step, the Brute Force approach is used with the descriptor distance ratio set to 0.8, and the time it takes for keypoint detection and descriptor extraction was logged. A helper function was created to iterate through all the possible detectors and descriptors, saving its matches and times. From this was identified that the best Detector for this application is FAST, and the best Descriptors are BRIEF and ORB.
&lt;/p&gt;
&lt;p&gt;
    Then we detected and classify images using the YOLO framework, which gives us a set of bounding boxes and augmenting them associating each bounding box to its respective Lidar points. Finally, we track these bounding boxes over time to estimate Time To Collision with Lidar and Camera measurements. When taking into account both TCC, twenty pairs of Detector/Descriptors were analyzed and AZAKE/FREAK was chosen based on the average error when compared to Lidar's TCC.
&lt;/p&gt;
&lt;p&gt;
    To check the complete code with comments, tasks and the results, please check the &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Lidar Obstacles Detection</title>
      <link href="http://localhost:4000/Lidar-Obstacles-Detection" rel="alternate" type="text/html" title="Lidar Obstacles Detection" />
      <published>2020-07-18T12:15:00+02:00</published>
      <updated>2020-07-18T12:15:00+02:00</updated>
      <id>http://localhost:4000/Lidar%20Obstacles%20Detection</id>
      <content type="html" xml:base="http://localhost:4000/Lidar-Obstacles-Detection">&lt;p&gt;In this project, we create a simple 3d highway enviroment using Point Cloud Library for exploring self-driving car sensors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/obstacledetectionfps.gif&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main objective was to implement three essential algorithms used to identify obstacles using Lidar: RANSAC, KD-Tree, and Euclidean clustering, as these are required as part of the processing pipeline: filtering, segmentation, clustering, and bounding boxes. And use it to detect car and trucks on a narrow street using lidar. Although PCL’s built in segmentation and clustering functions were not used, they were useful for testing.&lt;/p&gt;

&lt;p&gt;In this object detection problem, we were concern about bounding boxes enclosing appropriate objects, as vehicles and the poles, where there is only one box per detected object. And objects being consistently detected across frames in the video. Most bounding boxes can be followed through the lidar stream, and major objects don’t lose or gain bounding boxes in the middle of the lidar stream. Segmentation was also implemented in the project, the code used for segmentation uses the 3D RANSAC algorithm. As well as clustering, using for clustering the Euclidean clustering algorithm along with the KD-Tree.&lt;/p&gt;

&lt;p&gt;For code efficiency, was kept in mind not running the exact same calculation repeatedly when you can run it once, storing the value and then reuse the value later. Avoid loops that run too many times. And the creation of unnecessarily complex data structures when simpler structures work equivalently.&lt;/p&gt;

&lt;p&gt;C++ was used in this whole project, which is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt; and the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/Lidar_Obstacle_Detection&quot;&gt;Github&lt;/a&gt;. Images and text are credit to &lt;a href=&quot;https://github.com/awbrown90&quot;&gt;Aaron Brown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.cpp&lt;/code&gt; we can found the main function, with the city block creator and how camera should start. At &lt;code class=&quot;highlighter-rouge&quot;&gt;processPointClouds.cpp&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;processPointClouds.h&lt;/code&gt; all the necessary function are implemented:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;filtering the cloud, with voxel grid point reduction and region based filtering&lt;/li&gt;
  &lt;li&gt;separating cloud, one cloud with obstacles and other with segmented plane&lt;/li&gt;
  &lt;li&gt;segmenting plane, applying RANSAC, find inliers and segment the largest planar component&lt;/li&gt;
  &lt;li&gt;clustering, performing euclidean clustering to group detected obstacles, creating the KdTree object for the search method of the extraction&lt;/li&gt;
  &lt;li&gt;findind bounding box for one of the clusters&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementing-ransac&quot;&gt;Implementing RANSAC&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac-linie-animiert.gif&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RANSAC stands fors Randomly Sample Consensus, and is a method for detecting outliers in data. RANSAC runs a for a max number of iterations, and returns the model with the best fit. Apart from the 3D RANSAC implemented in this project, a  RANSAC for fitting a line in 2D point data with outliers were also tested. The code is located in &lt;code class=&quot;highlighter-rouge&quot;&gt;src/quiz/ransac2d.cpp&lt;/code&gt; and the function to fill out is &lt;code class=&quot;highlighter-rouge&quot;&gt;Ransac&lt;/code&gt; which takes in arguments for a point cloud, max iterations to run, and distance tolerance. The point cloud is actually &lt;code class=&quot;highlighter-rouge&quot;&gt;pcl::PointXYZ&lt;/code&gt; but the z component will be set to zero to make things easy to visualize. The image below shows how the data looks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac2d.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The data was generated by creating a line with points slightly scattered, and then outliers were added by randomly placing points in the scene. We want to be able to identify which points belong to the line that was originally generated and which points are outliers. To do this you will randomly sample two points from the cloud and fit a line between the points. A helpful line equation for this problem can be seen below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ax + By + C = 0
for point1 (x1, y1), and point2 (x2, y2)
(y1  y2)x + (x2  x1)y + (x1y2  x2y1) = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After fitting the line you can then iterate through all the points and determine if they are inliers by measuring how far away the point is from the line. You can do this for each iteration keeping track of which fitted line had the highest number of inliers, the line with the most inliers will be the best model. The equation for calculating distance between a point and line is shown below. For further details see &lt;a href=&quot;https://brilliant.org/wiki/dot-product-distance-between-point-and-a-line/&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;line is Ax + By + C = 0
point (x,y)
d = |A*x+B*y+C|/sqrt(A^2+B^2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the results of doing RANSAC to fit a line from the data above. Inliers are green while outliers are red, the function had a max iteration size of 50, and a distance tolerance of 0.5. The max iteration size depends on the ratio of inliers to the total number of points. The more inliers our data contains the higher the probability of selecting inliers to fit the line to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/ransac2dfitted.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The method above was for fitting a line, you can do the same thing for fitting plane in a 3D point cloud by using the equation for a plane from three points, and the distance formula for a point to a plane.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plane is Ax + By + Cz + D = 0
for point1 (x1, y1, z1)
for point2 (x2, y2, z2)
for point3 (x3, y3, z3)

Use point1 as reference and define two vectors on the plane v1, and v2

vector v1 travels from point1 to point2
vector v2 travels from point1 to point3

v1 = &amp;lt; x2 - x1, y2 - y1, z2 - z1 &amp;gt;
v2 = &amp;lt; x3 - x1, y3 - y1, z3 - z1 &amp;gt;

Find normal vector of plane by taking cross product of v1 x v2.

v1 x v2 = &amp;lt;(y2-y1)(z3-z1)-(z2-z1)(y3-y1), (z2-z1)(x3-x1)-(x2-x1)(z3-z1), (x2-x1)(y3-y1)-(y2-y1)(x3-x1)&amp;gt;

To simplify notation we can write it in the form 
v1 x v2 = &amp;lt; i, j, k &amp;gt;

then ,

i(x-x1)+j(y-y1)+k(z-z1) = 0,
ix + jy + kz -( ix1 + jy1 + kz1 ) = 0

A = i
B = j
C = k
D = -( ix1 + jy1 + kz1 )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the distance formula from a point to the plane is then,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plane is Ax + By + Cz + D = 0
point (x,y,z)
d = |A*x+B*y+C*z+D|/sqrt(A^2+B^2+C^2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;implementing-kd-tree-and-euclidean-clustering&quot;&gt;Implementing KD-Tree and Euclidean Clustering&lt;/h2&gt;

&lt;p&gt;A KD-Tree is a binary tree that splits points between alternating axes. By seperating space by splitting regions, it can make it much faster to do nearest neighbor search when using an algorithm like euclidean clustering. Here we will be looking at a 2D example, so the the tree will be a 2D-Tree. The code can be found at &lt;code class=&quot;highlighter-rouge&quot;&gt;src/quiz/cluster/kdtree.h&lt;/code&gt; and using the function &lt;code class=&quot;highlighter-rouge&quot;&gt;insert&lt;/code&gt; which take a 2D point represented by a vector of 2 floats, and a point ID, this is just a way to uniquely identify points and a way to tell which index they are from the overall point cloud. To understand the &lt;code class=&quot;highlighter-rouge&quot;&gt;insert&lt;/code&gt; function let’s first talk about how a KD-Tree splits information.&lt;/p&gt;

&lt;h3 id=&quot;inserting-points-into-the-tree&quot;&gt;Inserting Points into the Tree&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/2dpoints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows what the 2d points look like, in this simple example there is only 11 points, and there is 3 clusters where points are in close proximity to each other that you will be finding. The image below shows what the tree looks like after all 11 points have been inserted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now lets talk about how exactly the tree is created. At the very beginning when the tree is empty, root is NULL, the point inserted becomes the root, and splits the x region. Here is what this visually looks like, after inserting the first point (-6.2, 7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree1.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next point is (-6.3,8.4), since -6.3 is less than -6.2 this Node will be created and be apart of root’s left node, and now the point (-6.3,8.4) will split the region in the y dimension. The root was at depth 0, and split the x region, the next point become the left child of root and had a depth of 1, and split the y region. A point at depth 2 will split the x region again, so the split can actually be calculated as depth % 2, where 2 is the number of dimensions we are working with. The image below show how the tree looks after inserting the second point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree2.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then here is what the tree looks like after inserting two more points (-5.2,7.1), (-5.7,6.3), and having another x split division from point (-5.7,6.3) being at depth 2 in the tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree4.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image below shows so far what the tree looks like after inserting those 4 points. The labeled nodes A, B, C, D, and E are all NULL but if the next point (7.2,6.1) is inserted, whill of those 5 nodes will it be assigned to ?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/kdtree5.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The answer is D. Let’s look at why this is. First the root (-6.2, 7) and the point(7.2, 6.1) x region will be compared. 7.2 is greater than -6.2 so the new point will branch off to the right to (-5.2, 7.1). Next the y region will be compared, 6.1 is less than 7.1 so the new point will branch off to the left to (-5.7,6.3). Last the x region will be compared again, 7.2 is greater than -5.7 so the new point will branch to the right and will be Node D.&lt;/p&gt;

&lt;p&gt;Logically this is how points are inserted, how about doing this in C++? Implementing a recursive helper function to insert points can be a very nice way to update Nodes. The basic idea is the tree is traversed until the Node is arrives at is NULL in which case a new Node is created and assigned at that current NULL Node. For assinging a Node, one idea is to use a double pointer, you could pass in a pointer to the node, starting at root, and then when you want to assign that node, derefrence pointer and assign it to the newly created Node. Another way of achieving this is by using a pointer reference as well.&lt;/p&gt;

&lt;h3 id=&quot;improving-the-tree-structure&quot;&gt;Improving the Tree Structure&lt;/h3&gt;

&lt;p&gt;Having a balanced tree that evenly splits regions improves the search time for finding points later. To improve the tree insert points that alternate between splitting the x region and the y region. To do this pick the median of sorted x and y points. For instance if you are inserting the first four points that we used above (-6.3,8.4),(-6.2,7),(-5.2,7.1),(-5.7,6.3) we would first insert (-5.2,7.1) since its the median for the x points, if there is an even number of elements the lower median is chosen. The next point to be insorted would be (-6.2,7) the median of the three points for y. Followed by (-5.7,6.3) the lower median between the two for x, and then finally (-6.3,8.4). This ordering will allow the tree to more evenly split the region space and improving searching later.&lt;/p&gt;

&lt;h4 id=&quot;searching-nearby-points-in-the-tree&quot;&gt;Searching Nearby Points in the Tree&lt;/h4&gt;

&lt;p&gt;Once points are able to be inserted into the tree, the next step is being able to search for nearby points (points within a distance of distanceTol) inside the tree compared to a given pivot point. The kd-tree is able to split regions and allows certain regions to be completly ruled out, speeding up the process of finding nearby neighbors. The naive approach of finding nearby neighbors is to go through every single point in the tree and compare its distance with the pivots, selecting point indices that fall with in the distance tolerance. Instead with the kd-tree we can compare distance within a boxed square that is 2 x distanceTol for length, centered around the pivot point. If the current node point is within this box then we can directly calculate the distance and see if we add it to the list of &lt;code class=&quot;highlighter-rouge&quot;&gt;ids&lt;/code&gt;. Then we see if our box crosses over the node division region and if it does compare that next node. We do this recursively, with the advantage being that if the box region is not inside some division region we completly skip that branch.&lt;/p&gt;

&lt;h3 id=&quot;euclidean-clustering&quot;&gt;Euclidean Clustering&lt;/h3&gt;

&lt;p&gt;Once the kd-tree method for searching for nearby points is implemented, its not difficult to implement a euclidean clustering method that groups indidual cluster indicies based on their proximity. Inside &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster.cpp&lt;/code&gt; there is a function called &lt;code class=&quot;highlighter-rouge&quot;&gt;euclideanCluster&lt;/code&gt; which returns a vector of vector ints, this is the list of each cluster’s indices. To perform the clustering, iterate through each point in the cloud and keep track of which points have been processed already. For each point add it to a cluster group then get a list of all the points in proximity to that point. For each point in proximity if it hasn’t already been processed add it to the cluster group and repeat the process of calling proximity points. Once the recursion stops for the first cluster group, create a new cluster and move through the point list. Once all the points have been processed there will be a certain number of cluster groups found.&lt;/p&gt;

&lt;p&gt;There are three clusters found, using a distance tolerance of 3.0. Each cluster group is colored a differently, red, green, and blue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lidar-obstacles/clusterkdtree.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we create a simple 3d highway enviroment using Point Cloud Library for exploring self-driving car sensors.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Image Captioning</title>
      <link href="http://localhost:4000/Image-Captioning" rel="alternate" type="text/html" title="Image Captioning" />
      <published>2020-07-07T12:15:00+02:00</published>
      <updated>2020-07-07T12:15:00+02:00</updated>
      <id>http://localhost:4000/Image%20Captioning</id>
      <content type="html" xml:base="http://localhost:4000/Image-Captioning">&lt;p&gt;In this project, we used a dataset of image-caption pairs to train a CNN-RNN model to automatically generate captions from images.&lt;/p&gt;

&lt;p&gt;The Dataset&lt;/p&gt;

&lt;p&gt;Downloaded using  &lt;a href=&quot;https://github.com/cocodataset/cocoapi&quot;&gt;COCO API&lt;/a&gt;, the Microsoft &lt;strong&gt;C&lt;/strong&gt;ommon &lt;strong&gt;O&lt;/strong&gt;bjects in &lt;strong&gt;CO&lt;/strong&gt;ntext (MS COCO) dataset is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms. Furthermore you can find more about the dataset on the &lt;a href=&quot;http://cocodataset.org/#home&quot;&gt;website&lt;/a&gt; or in the &lt;a href=&quot;https://arxiv.org/pdf/1405.0312.pdf&quot;&gt;research paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/img-captioning/coco-examples.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we used a dataset of image-caption pairs to train a CNN-RNN model to automatically generate captions from images.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Landmark Detection &amp;amp; Robot Tracking (SLAM)</title>
      <link href="http://localhost:4000/Slam" rel="alternate" type="text/html" title="Landmark Detection &amp; Robot Tracking (SLAM)" />
      <published>2020-06-25T12:15:00+02:00</published>
      <updated>2020-06-25T12:15:00+02:00</updated>
      <id>http://localhost:4000/Slam</id>
      <content type="html" xml:base="http://localhost:4000/Slam">&lt;p&gt;In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.&lt;/p&gt;

&lt;h2 id=&quot;simultaneous-localization-and-mapping&quot;&gt;Simultaneous Localization and Mapping&lt;/h2&gt;

&lt;p&gt;SLAM gives us a way to both localize a robot and build up a map of its environment as a robot moves and senses in real-time. This is an active area of research in the fields of robotics and autonomous systems. Since this localization and map-building relies on the visual sensing of landmarks, this is also a computer vision problem.&lt;/p&gt;

&lt;p&gt;To perform SLAM, we’ll collect a series of robot sensor measurements and motions, in that order, over a defined period of time. Then we’ll use only this data to re-construct the map of the world with the robot and landmark locations. Robot motion and sensors have some uncertainty associated with them. For example, imagine a car driving up hill and down hill; the speedometer reading will likely overestimate the speed of the car going up hill and underestimate the speed of the car going down hill because it cannot perfectly account for gravity. Similarly, we cannot perfectly predict the &lt;em&gt;motion&lt;/em&gt; of a robot. A robot is likely to slightly overshoot or undershoot a target location.&lt;/p&gt;

&lt;p&gt;First, we create a robot and move it around a 2D grid world and then, define a &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function for this robot that allows it to sense landmarks in a given world. It’s important that you understand how this robot moves, senses, and how it keeps track of different landmarks that it sees in a 2D grid world, so that you can work with it’s movement and sensor data.&lt;/p&gt;

&lt;p&gt;This project is part of the &lt;a href=&quot;https://www.udacity.com/course/computer-vision-nanodegree--nd891&quot;&gt;Udacity Computer Vision Nanodegree&lt;/a&gt; and all code can be found in &lt;a href=&quot;https://github.com/jacquesmats/slam_implementation&quot;&gt;my Github&lt;/a&gt;.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;  in robot class takes in a number of parameters including a world size and some values that indicate the sensing and movement capabilities of the robot. Furthermore, using the visualization function, &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt;, that will display a grid world in a plot and draw a red &lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt; at the location of our robot, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;. The details of how this function works can be found in the &lt;code class=&quot;highlighter-rouge&quot;&gt;helpers.py&lt;/code&gt; file in the home directory. In the following image, we define a small 10x10 square world, a measurement range that is half that of the world and small values for motion and measurement noise. These values will typically be about 10 times larger, but we want to demonstrate this behavior on a small scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/gridworld.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s create landmarks, which are measurable features in the map. You can think of landmarks as things like notable buildings, or something smaller such as a tree, rock, or other feature. The robot class has a function &lt;code class=&quot;highlighter-rouge&quot;&gt;make_landmarks&lt;/code&gt; which randomly generates locations for the number of specified landmarks. We have to pass these locations as a third argument to the &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt; function and the list of landmark locations is accessed similar to how we find the robot position &lt;code class=&quot;highlighter-rouge&quot;&gt;r.landmarks&lt;/code&gt;.  Each landmark is displayed as a purple &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; in the grid world, and we also print out the exact &lt;code class=&quot;highlighter-rouge&quot;&gt;[x, y]&lt;/code&gt; locations of these landmarks at the end of this cell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/landmark.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once we have some landmarks to sense, we need to be able to tell our robot to &lt;em&gt;try&lt;/em&gt; to sense how far they are away from it. It will be up t you to code the &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function in our robot class. The &lt;code class=&quot;highlighter-rouge&quot;&gt;sense&lt;/code&gt; function uses only internal class parameters and returns a list of the the measured/sensed x and y distances to the landmarks it senses within the specified &lt;code class=&quot;highlighter-rouge&quot;&gt;measurement_range&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;''' This function does not take in any parameters, instead it references internal variables
            (such as self.landamrks) to measure the distance between the robot and any landmarks
            that the robot can see (that are within its measurement range).
            This function returns a list of landmark indices, and the measured distances (dx, dy)
            between the robot's position and said landmarks.
            This function should account for measurement_noise and measurement_range.
            One item in the returned list should be in the form: [landmark_index, dx, dy].
            '''&lt;/span&gt;
           
        &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;## Iterate through all of the landmarks in a world&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;                 
            &lt;span class=&quot;c&quot;&gt;## For each landmark&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 1. compute dx and dy, the distances between the robot and the landmark&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 2. account for measurement noise by *adding* a noise component to dx and dy&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;## 3. If either of the distances, dx or dy, fall outside of the internal var, measurement_range&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;##    then we cannot record them; if they do fall in the range, then add them to the measurements list&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;##    as list.append([index, dx, dy]), this format is important for data creation done later&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;land_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;  
            
            &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;land_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dx: {}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; dy: {}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; range: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;omega-and-xi&quot;&gt;Omega and Xi&lt;/h2&gt;

&lt;p&gt;To implement Graph SLAM, a matrix and a vector (omega and xi, respectively) are introduced. The matrix is square and labeled with all the robot poses (xi) and all the landmarks (Li). Every time you make an observation, for example, as you move between two poses by some distance &lt;code class=&quot;highlighter-rouge&quot;&gt;dx&lt;/code&gt; and can relate those two positions, you can represent this as a numerical relationship in these matrices.&lt;/p&gt;

&lt;p&gt;It’s easiest to see how these work in an example. Below you can see a matrix representation of omega and a vector representation of xi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/omega_xi.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s look at a simple example that relates 3 poses to one another.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When you start out in the world most of these values are zeros or contain only values from the initial robot position&lt;/li&gt;
  &lt;li&gt;In this example, you have been given constraints, which relate these poses to one another&lt;/li&gt;
  &lt;li&gt;Constraints translate into matrix values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/omega_xi_constraints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have ever solved linear systems of equations before, this may look familiar.&lt;/p&gt;

&lt;h3 id=&quot;solving-for-x&quot;&gt;Solving for x&lt;/h3&gt;

&lt;p&gt;To “solve” for all these x values, we can use linear algebra; all the values of x are in the vector &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; which can be calculated as a product of the inverse of omega times xi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/solution.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;motion-constraints-and-landmarks&quot;&gt;Motion Constraints and Landmarks&lt;/h2&gt;

&lt;p&gt;Now, let’s look at how motion (and similarly, sensor measurements) can be used to create constraints and fill up the constraint matrices, omega and xi. Let’s start with empty/zero matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/initial_constraints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This example also includes relationships between poses and landmarks. Say we move from x0 to x1 with a displacement &lt;code class=&quot;highlighter-rouge&quot;&gt;dx&lt;/code&gt; of 5. Then we have created a motion constraint that relates x0 to x1, and we can start to fill up these matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/motion_constraint.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, the one constraint equation can be written in two ways. So, the motion constraint that relates x0 and x1 by the motion of 5 has affected the matrix, adding values for &lt;em&gt;all&lt;/em&gt; elements that correspond to x0 and x1.&lt;/p&gt;

&lt;h2 id=&quot;generating-an-environment&quot;&gt;Generating an environment&lt;/h2&gt;

&lt;p&gt;In a real SLAM problem, one may be given a map that contains information about landmark locations, and in this project, we will make our own data using the &lt;code class=&quot;highlighter-rouge&quot;&gt;make_data&lt;/code&gt; function, which generates a world grid with landmarks in it and then generates data by placing a robot in that world and moving and sensing over some number of time steps.  The data is collected as an instantiated robot moves and senses in a world. The SLAM function will take in this data as input. So, first we create this data and explore how it represents the movement and sensor measurements that our robot takes.&lt;/p&gt;

&lt;h3 id=&quot;create-the-world&quot;&gt;Create the world&lt;/h3&gt;

&lt;p&gt;We used the &lt;code class=&quot;highlighter-rouge&quot;&gt;make_data&lt;/code&gt; function to generate a world of a specified size with randomly generated landmark locations. &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; holds the sensors measurements and motion of your robot over time. It stores the measurements as &lt;code class=&quot;highlighter-rouge&quot;&gt;data[i][0]&lt;/code&gt; and the motion as &lt;code class=&quot;highlighter-rouge&quot;&gt;data[i][1]&lt;/code&gt;. One of the most challenging tasks were to create and modify the constraint matrix and vector: omega and xi. In the images above, you saw an example of how omega and xi could hold all the values the define the relationships between robot poses &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; and landmark positions &lt;code class=&quot;highlighter-rouge&quot;&gt;Li&lt;/code&gt; in a 1D world.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;this&lt;/em&gt; project, we implemented constraints for a 2D world. We are referring to robot poses as &lt;code class=&quot;highlighter-rouge&quot;&gt;Px, Py&lt;/code&gt; and landmark positions as &lt;code class=&quot;highlighter-rouge&quot;&gt;Lx, Ly&lt;/code&gt;, and one way to approach this challenge is to add &lt;em&gt;both&lt;/em&gt; x and y locations in the constraint matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/constraints2D.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;slam-inputs&quot;&gt;SLAM inputs&lt;/h3&gt;

&lt;p&gt;In addition to &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt;, the slam function will also take in:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;N -   The number of time steps that a robot will be moving and sensing&lt;/li&gt;
  &lt;li&gt;num_landmarks - The number of landmarks in the world&lt;/li&gt;
  &lt;li&gt;world_size - The size (w/h) of the world&lt;/li&gt;
  &lt;li&gt;motion_noise - The noise associated with motion; the update confidence for motion should be &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/motion_noise&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;measurement_noise - The noise associated with measurement/sensing; the update weight for measurement should be &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/measurement_noise&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recall that &lt;code class=&quot;highlighter-rouge&quot;&gt;omega&lt;/code&gt; holds the relative “strengths” or weights for each position variable, and you can update these weights by accessing the correct index in omega &lt;code class=&quot;highlighter-rouge&quot;&gt;omega[row][col]&lt;/code&gt; and &lt;em&gt;adding/subtracting&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;1.0/noise&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;noise&lt;/code&gt; is measurement or motion noise. &lt;code class=&quot;highlighter-rouge&quot;&gt;Xi&lt;/code&gt; holds actual position values, and so to update &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; you’ll do a similar addition process only using the actual value of a motion or measurement. So for a vector index &lt;code class=&quot;highlighter-rouge&quot;&gt;xi[row][0]&lt;/code&gt; you will end up adding/subtracting one measurement or motion divided by their respective &lt;code class=&quot;highlighter-rouge&quot;&gt;noise&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;implement-graph-slam&quot;&gt;Implement Graph SLAM&lt;/h3&gt;

&lt;p&gt;With a 2D omega and xi structure as shown above, we have to be mindful about how you update the values in these constraint matrices to account for motion and measurement constraints in the x and y directions. Recall that the solution to these matrices (which holds all values for robot poses &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; and landmark locations &lt;code class=&quot;highlighter-rouge&quot;&gt;L&lt;/code&gt;) is the vector, &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt;, which can be computed at the end of the construction of omega and xi as the inverse of omega times xi.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## slam takes in 6 arguments and returns mu, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## mu is the entire path traversed by a robot (all x,y poses) *and* all landmarks locations&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;slam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;## Use initilization to create constraint matrices, omega and xi&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_constraints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_landmarks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Which index starts the Landmarks?&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;first_landm_at&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;## Iterate through each time step in the data&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## get all the motion and measurement data as it iterates&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## update the constraint matrix/vector to account for all *measurements*&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## this should be a series of additions that take into account the measurement noise &lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;    

            &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            
            &lt;span class=&quot;c&quot;&gt;### MAIN DIAGONAL&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# X and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Lx and Ly&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;### OTHER DIAGONAL&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Lx and X&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Ly and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;                     
            &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;



            &lt;span class=&quot;c&quot;&gt;# Xi for X and Y&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Xi for Lx and Ly&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_L&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement_noise&lt;/span&gt;
                    
    &lt;span class=&quot;c&quot;&gt;## update the constraint matrix/vector to account for all *motion* and motion noise           &lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;motion&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;#X_current&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# X_next&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Y_current&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#Y_next&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;motion_noise&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;## After iterating through all the data&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## Compute the best estimate of poses and landmark positions&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## using the formula, omega_inverse * Xi&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;omega_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;omega_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;omega&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;run-slam&quot;&gt;Run SLAM&lt;/h2&gt;

&lt;p&gt;Once we completed the implementation of &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;, lets check what &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; it returns for different world sizes and different landmarks! We expect the robot to start with an estimated pose in the very center of your square world, whose size is defined by &lt;code class=&quot;highlighter-rouge&quot;&gt;world_size&lt;/code&gt;. We specify the number, &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;, the time steps that the robot was expected to move and the &lt;code class=&quot;highlighter-rouge&quot;&gt;num_landmarks&lt;/code&gt; in the world (which your implementation of &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;) should see and estimate a position for.&lt;/p&gt;

&lt;p&gt;With these values in mind, you should expect to see a result that displays two lists:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Estimated poses&lt;/strong&gt;, a list of (x, y) pairs that is exactly &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; in length since this is how many motions your robot has taken. The very first pose should be the center of your world, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;[50.000, 50.000]&lt;/code&gt; for a world that is 100.0 in square size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Estimated landmarks&lt;/strong&gt;, a list of landmark positions (x, y) that is exactly &lt;code class=&quot;highlighter-rouge&quot;&gt;num_landmarks&lt;/code&gt; in length.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For visualizing the constructed world, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;display_world&lt;/code&gt; code from the &lt;code class=&quot;highlighter-rouge&quot;&gt;helpers.py&lt;/code&gt; file (which was also used in the first notebook), we can actually visualize what you have coded with &lt;code class=&quot;highlighter-rouge&quot;&gt;slam&lt;/code&gt;: the final position of the robot and the positon of landmarks, created from only motion and measurement data!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/fullgrid.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To confirm that the slam code works, we run it on some test data and cases. The output should be &lt;strong&gt;close-to or exactly&lt;/strong&gt; identical to the given results. If there are minor discrepancies it could be a matter of floating point accuracy or in the calculation of the inverse matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam/final_result.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">In this project, we’ll be localizing a robot in a 2D grid world. The basis for simultaneous localization and mapping (SLAM) is to gather information from a robot’s sensors and motions over time, and then use information about measurements and motion to re-construct a map of the world.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Facial Keypoint Detection</title>
      <link href="http://localhost:4000/Facial-Keypoint-Detection" rel="alternate" type="text/html" title="Facial Keypoint Detection" />
      <published>2020-06-15T12:15:00+02:00</published>
      <updated>2020-06-15T12:15:00+02:00</updated>
      <id>http://localhost:4000/Facial%20Keypoint%20Detection</id>
      <content type="html" xml:base="http://localhost:4000/Facial-Keypoint-Detection">&lt;p&gt;This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some examples of images and corresponding facial keypoints.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/key_pts_example.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and &lt;strong&gt;68 keypoints, with coordinates (x, y), for that face&lt;/strong&gt;.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. This keypoint extraction project is part of the &lt;a href=&quot;https://www.udacity.com/course/computer-vision-nanodegree--nd891&quot;&gt;Udacity Computer Vision Nanodegree.&lt;/a&gt; Here they are, numbered, and you can see that specific ranges of points match different portions of the face.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/landmarks_numbered.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;In this project, we defined and trained a CNN to identify and extract facial keypoints, founded in the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;. Coded using Python and Pytorch, first we get familiar with the dataset and do all the processing and transformation using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataloader&lt;/code&gt;. Using a Custom LeNet-5 Architecture, we trained a model to identify such keypoints in the faces located by a OpenCV’s pre-trained Haar Cascade classifier. &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;Code and tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;The first step in working with any dataset is to become familiar with your data; we’ll need to load in the images of faces and their keypoints and visualize them! This set of image data has been extracted from the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;, which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.&lt;/p&gt;

&lt;h4 id=&quot;training-and-testing-data&quot;&gt;Training and Testing Data&lt;/h4&gt;

&lt;p&gt;This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3462 of these images are training images, to create a model to predict keypoints.&lt;/li&gt;
  &lt;li&gt;2308 are test images, which will be used to test the accuracy of the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas&lt;/code&gt;. When we read the training CSV and get the annotations, they are in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).&lt;/p&gt;

&lt;p&gt;Below, in an output from the function &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt; that takes in an image and keypoints and displays them. As you look at this data, &lt;strong&gt;note that these images are not all of the same size&lt;/strong&gt;, and neither are the faces! To eventually train a neural network on these images, we’ll need to standardize their shape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/show_keypoints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this project, to prepare our data for training, we used PyTorch’s Dataset class. Much of this this code is a modified version of what can be found in the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/data_loading_tutorial.html&quot;&gt;PyTorch data loading tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;dataset-class&quot;&gt;Dataset class&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; is an abstract class representing a dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.&lt;/p&gt;

&lt;p&gt;Our custom dataset should inherit &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt; and override the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__len__&lt;/code&gt; so that &lt;code class=&quot;highlighter-rouge&quot;&gt;len(dataset)&lt;/code&gt; returns the size of the dataset.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt; to support the indexing such that &lt;code class=&quot;highlighter-rouge&quot;&gt;dataset[i]&lt;/code&gt; can be used to get the i-th sample of image/keypoint data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here we create a dataset class for our face keypoints dataset. We read the CSV file in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; but leave the reading of images to &lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt;. This is memory efficient because all the images are not stored in the memory at once but read as required.&lt;/p&gt;

&lt;p&gt;A sample of our dataset will be a dictionary &lt;code class=&quot;highlighter-rouge&quot;&gt;{'image': image, 'keypoints': key_pts}&lt;/code&gt;. Our dataset will take an
optional argument &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; so that any required processing can be applied on the sample.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FacialKeypointsDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;# if image has an alpha color channel, get rid of it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'float'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'keypoints'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After we’ve defined this class, we can instantiate the dataset and display some images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/sample0.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transforms&quot;&gt;Transforms&lt;/h3&gt;

&lt;p&gt;Now, the images in this dataset are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.&lt;/p&gt;

&lt;p&gt;Therefore, we will need to write some pre-processing code. The following four transforms were created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Normalize&lt;/code&gt;: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Rescale&lt;/code&gt;: to rescale an image to a desired size.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RandomCrop&lt;/code&gt;: to crop an image randomly.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ToTensor&lt;/code&gt;: to convert numpy images to torch images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called. For this, we just implemented &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; method and (if we require parameters to be passed in), the &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; method.  We can then use a transform like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tx = Transform(params)
transformed_sample = tx(sample)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Observe below how these transforms are generally applied to both the image and its keypoints. Let’s test these transforms out to make sure they behave as expected. As you look at each transform, note that, in this case, &lt;strong&gt;order does matter&lt;/strong&gt;. For example, you cannot crop a image using a value smaller than the original image (and the original images vary in size!), but, if you first rescale the original image, you can then crop it to any size smaller than the rescaled size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/transform.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After creating the transformed dataset, applying the transforms in order to get grayscale images of the same shape, verify that your transform works, we used &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt; iterator which provides features like Batch the data, Shuffle the data and Load the data in parallel using &lt;code class=&quot;highlighter-rouge&quot;&gt;multiprocessing&lt;/code&gt; workers.&lt;/p&gt;

&lt;p&gt;Now that we load and transform the data, we are ready to build a neural network to train on this data.&lt;/p&gt;

&lt;h2 id=&quot;the-convolutional-neural-network&quot;&gt;The Convolutional Neural Network&lt;/h2&gt;

&lt;p&gt;After looking at the data we’re working with and, in this case, know the shapes of the images and of the keypoints, we can define a convolutional neural network that can &lt;em&gt;learn&lt;/em&gt; from this data. Recall that CNN’s are defined by a few types of layers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convolutional layers&lt;/li&gt;
  &lt;li&gt;Maxpooling layers&lt;/li&gt;
  &lt;li&gt;Fully-connected layers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-neural-nets&quot;&gt;PyTorch Neural Nets&lt;/h3&gt;

&lt;p&gt;To define a neural network in PyTorch, we define the layers of a model in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and define the feedforward behavior of a network that employs those initialized layers in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt;, which takes in an input image tensor, &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;. Also note that during training, PyTorch will be able to perform backpropagation by keeping track of the network’s feedforward behavior and using autograd to calculate the update to the weights in the network.&lt;/p&gt;

&lt;p&gt;Best practice is to place any layers whose weights will change during the training process in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and refer to them in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function; any layers or functions that always behave in the same way, such as a pre-defined activation function, should appear &lt;em&gt;only&lt;/em&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;The model used in this project was a Custom LeNet-5 Architecture, which you can check in the &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;repository&lt;/a&gt; under the file &lt;code class=&quot;highlighter-rouge&quot;&gt;models.py&lt;/code&gt;, using 4 Convolutional Layers followed by a Poooling Layer and 2 Fully Connected. After reading the paper from &lt;a href=&quot;https://arxiv.org/pdf/1710.00977.pdf&quot;&gt;NaimishNet&lt;/a&gt;, I realized, from Fig.21, that LeNet-5 had a good performance while with fewer layers than NaimishNet. I started if LeNet-5 and change it until reaching the final mode. Adding two more Conv. Layers, replacing AvgPool by MaxPool (since were more common use avg back then) and Tanh activations by ReLU, improvements made in this 20 years.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=25600, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=136, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;feature-maps&quot;&gt;Feature maps&lt;/h3&gt;

&lt;p&gt;Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it’s convolutional kernels and when these kernels are applied to some input image, they produce a set of &lt;strong&gt;feature maps&lt;/strong&gt;. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/feature_map_ex.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;To prepare for training, create a transformed dataset of images and keypoints. In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so your model’s loss does not blow up during training, it is also suggested that you normalize the input images and keypoints. The necessary transforms have been defined in &lt;code class=&quot;highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, having defined the transformed dataset, we use PyTorch’s DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in &lt;a href=&quot;http://pytorch.org/docs/master/data.html&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also defined some functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;net_sample_output(): test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints.&lt;/li&gt;
  &lt;li&gt;show_all_keypoints(): displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided).&lt;/li&gt;
  &lt;li&gt;visualize_output(): this function’s main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors &lt;em&gt;and&lt;/em&gt; it undoes the keypoint normalization done in the Normalize() transform; it’s assumed that you applied these transformations when you loaded your test data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After testing the optimizer with Adam and SGD, I kept Adam because is faster and converges faster also. About the loss functions, MSE and L1Smooth had a similar performance with 5 epochs, so I kept because of the NaimishNet reference. In 60 epochs, the average loss was 0.000651, resulting in the following detection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download.png&quot; alt=&quot;image-20200612307150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download2.png&quot; alt=&quot;image-20200602137150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download3.png&quot; alt=&quot;image-20200607121350421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;face-and-facial-keypoint-detection&quot;&gt;Face and Facial Keypoint detection&lt;/h2&gt;

&lt;p&gt;After you’ve trained a neural network to detect facial keypoints, you can then apply this network to &lt;em&gt;any&lt;/em&gt; image that includes faces. The neural network expects a Tensor of a certain size as input and, so, to detect any face, we first have to do some pre-processing. The following image will be used for test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamas.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;detect-all-faces-in-an-image&quot;&gt;Detect all faces in an image&lt;/h3&gt;

&lt;p&gt;Here used one of OpenCV’s pre-trained Haar Cascade classifiers, all of which can be found in the &lt;code class=&quot;highlighter-rouge&quot;&gt;detector_architectures/&lt;/code&gt; directory, to find any faces in your selected image. In the figure below, we loop over each face in the original image and draw a red square on each face (in a copy of the original image, so as not to modify the original). You can even &lt;a href=&quot;https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html&quot;&gt;add eye detections&lt;/a&gt; while using Haar detectors. An example of face detection is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamamichele.png&quot; alt=&quot;image-2020060715210421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;keypoint-detection&quot;&gt;Keypoint detection&lt;/h3&gt;

&lt;p&gt;Now, we loop over each detected face in an image (again!) only this time, you’ll transform those faces in Tensors that your CNN can accept as input images. First we convert the face from RGB to grayscale and normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]. So we rescale the detected face to be the expected square size for your CNN (224x224, suggested) and reshape the numpy image into a torch image.&lt;/p&gt;

&lt;p&gt;And finally, after each face has been appropriately converted into an input Tensor for your network to see as input, we apply our &lt;code class=&quot;highlighter-rouge&quot;&gt;net&lt;/code&gt; to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be “un-normalized” for display, and we write a helper function like &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt;. We end up with an image like the following with facial keypoints that closely match the facial features on each individual face:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obama.png&quot; alt=&quot;image-20200123607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/michele.png&quot; alt=&quot;image-20200607131250421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Predicting Buyers for Food Delivery Platforms</title>
      <link href="http://localhost:4000/Predicting-Buyers-for-Food-Delivery-Platforms" rel="alternate" type="text/html" title="Predicting Buyers for Food Delivery Platforms" />
      <published>2019-02-18T11:15:00+01:00</published>
      <updated>2019-02-18T11:15:00+01:00</updated>
      <id>http://localhost:4000/Predicting%20Buyers%20for%20Food%20Delivery%20Platforms</id>
      <content type="html" xml:base="http://localhost:4000/Predicting-Buyers-for-Food-Delivery-Platforms">&lt;p&gt;This project has the objective to create a Machine Learning Model to predict whether a user that just registered will buy or not. Predicting such action could influence from user experience until predicting demands for restaurants.&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;This project has the objective to create a Machine Learning Model to predict whether a user that just registered will buy or not. Predicting such action could influence from user experience until predicting demands for restaurants. The metrics used to evaluate this project was mainly Area Under the Curve (AUC), complemented by Accuracy and F-Score. A benchmark with five classifiers was created and a Light Gradient Boost Model was chosen. After tuning and reducing the number of features, the model achieves 99.0% of accuracy, followed by 98.2% and 99.1% of F-score and ROC-AUC, respectively. 
The predictor aims to be implemented in production to help the company understand better customer behavior. With that in mind, the first step was made moving the best classifier to the AWS SageMaker. There, it will be connected with the streaming data from the database and predict the customer next steps in real time.&lt;/p&gt;

&lt;h2 id=&quot;project-overview&quot;&gt;Project Overview&lt;/h2&gt;

&lt;p&gt;Immerse in a sector that expects to exceed $ 250 billion by 2022 [1], every improvement put you ahead of the competition. The FoodTech Industry is transforming the way we eat and it is being pulled by a generation which demands far more from services. In the US, 63% of Americans abandoned their cart due to bad customer experience [2].  Furthermore, because of the sectors wideness and variety, not only start ups are joining the race but corporate food companies as well. From the farm, passing through logistics and ending up in the customer’s table, the food sector is a rich soil to apply cutting edge technology. 
Being part of the last mile agents, the food delivery apps are the very end of this food chain. Working in direct contact with the customers, it is the bridge between clients, drivers and restaurants. In the food delivery branch the platform is sticky: 80\% of consumers rarely or never change to a new application [4]. So retention techniques as reorder buttons, discounts and promotions are the core to get and keep an active base of digital consumers.
Knowing your customer and predicting their next steps becomes essential in a such rapid pace environment. Predicting a customer purchase can influence directly in his user experience. It is possible to suggest his favorite cuisine, understand how logistics can scale or offer a discount code as an incentive. Thus, this project aims to create a classifier which is capable of predicting whether or not a customer will place an order soon after registration.&lt;/p&gt;

&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;p&gt;Taking action upon belief can be naive in a fully connected measurable world. With an average transaction order between $ 25 to $ 50 [3], a discount coupon of $ 5 represents 10 to 20% of its value. At the same time, use discounts to convincing new customers to order is a common practice. Furthermore, to give discount code to everyone which is logging for the first time, could be a big waste of budget. From the dataset, is possible to notice that 36% of customers tend to buy in the same day of registration. Meaning that every improvement in this number increases directly the company’s revenue. In a scenario where we have 100.000 new orders a day and (using) a discount code of $ 5, if we have a solution that gives discount only to customers that will probably not place an order in the same day, it could represent savings of $ 180k/day.
The solution for the problem stated before consist in identifying potential buyers and taking action to delivery a better user experience. And at the same time, instigate non buyers to place an order. This can be done using the biographic and demographic data present in the main dataset to understand these customers types.&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;For binary classification problems the most common metric to evaluate model’s performance is the Area Under the Curve (AUC). It can tell us how good a classifier identify two classes. The AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. As scale-invariant, it can measure how well the predictions are ranked and does not consider its absolute values. Together with AUC, the use of Accuracy and F-score complement the evaluation step.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;For the inputs, we have two datasets: the main one which contains demographic (age, population, etc) and order information (date, payment type, delivery method). Together they have 464.969 lines where each one is an order. This dataset contains two years of data from part of the company’s operation. And we have a second dataset which translates peoples name to gender, called here “names.csv”. This is necessary because the raw information in the database is not complete. 
The main dataset was obtained with consent from food delivery startup and is composed of the following dimensions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/featureset.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1. Type of variable associated with each feature&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;While the first 10 columns tell us information about the client, the second part digs deeper into consumer habits. All the dimensions could influence the client desire to buy, hence all of them was used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/datasets_head.png&quot; alt=&quot;image-2020060715042144332&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. Datasets used to train the model&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The names to gender dataset were download from a public website [5] and are based in the 2010 Brazilian Sense. It is provided information about the environment such as city size, number of orders in this particular city (that may reveal that people in cities with more orders are likely to buy) and country’s region. We have timewise data, which indicate the likelihood o buy in some hours and days. And the logistic data telling if the client used a coupon, which kind of cuisine this kind of customer order, which payment type they prefer. All to try to find a pattern in people who buy soon after registration. 
The binary classifier to classify each user is put manualy using Python during the code, in a column named ‘Buyer’ where 1 stands for True.&lt;/p&gt;

&lt;h3 id=&quot;exploratory-visualization&quot;&gt;Exploratory Visualization&lt;/h3&gt;

&lt;p&gt;In this section, we will have a closer look in the dataset and try to identify patterns already. A great way to start is to look a the datasets age:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/male_female_buyers.png&quot; alt=&quot;image-20200607151230421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/ageplot.png&quot; alt=&quot;image-2020060127150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3. Age and Gender Exploration&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Instantly we notice that there are several outliers. This may happen because of the input method in the registration form or by users deliberately choosing to put the wrong date of birth. A outliers removal technic is needed to improve the classifier accuracy.&lt;/p&gt;

&lt;p&gt;Another feature that comes to mind is gender. Gender may influence in customer desire to buy. But from Figure 3 we can not see a major influence upon putting a order or not. We can conclude that we have around 20\% more women in this dataset, but the target buyers are present in both genders equally.&lt;/p&gt;

&lt;p&gt;33&lt;img src=&quot;assets/images/pred-buyers/orderpophue.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4. Order and Population from Target Users&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Taking a look in the relationship between number of orders and population, we can see that it isn’t a linear function. Most of the data is concentrated in cities with population less than 200.000 and with less than 25 thousand orders. With this in mind, we can look at our target users in this range, as in Figure 4, to try to find a pattern. Clearly this users are in small cities, where competition is not so fierce and the number of orders are small, indicating that the app is new in the town.,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/orderspoprelation.png&quot; alt=&quot;image-20200607112250421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5. Relationship between number of orders and population&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In other hand, we can identify a pattern looking to the scatter plot below (Figure \ref{fig:scatagehour}) of hour by age. There is two well defined groups: people younger than 20 years and seniors from 40 to 80 years old. This two groups tend to order their first order between 10 AM and 8 PM. The gap between 20 and 40 years old, are mostly compose of people that do some research before place an order, or are impacted by some marketing campaign but do not buy instantly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/scatter_agehour.png&quot; alt=&quot;image-2020060217150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6. Identifying buyers per age and hour&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;algorithms-and-techniques&quot;&gt;Algorithms and Techniques&lt;/h3&gt;

&lt;p&gt;The first step is to prepare the data. We have two datasets: one containing information about customers who bought in the same day of registration and other containing customers that ordered the following day or after. These two must be merged as a main file. The main dataset does not contain buyers gender, which is main feature to predict customer behavior. So a name to gender dataset is needed as part of feature engineering, together with the creation of additional features tha would help the algoritm. Furthermore, a step to drop non relevant features as names and IDs is needed.&lt;/p&gt;

&lt;p&gt;Once the main dataset is put together, is proceed with preprocessing the data. Transforming skewed continuous features, such as orders and population that has vastly large numbers, as well as normalizing and hot-encoding variables performs a critical step. Although some algorithms can handle categorical features internally, this step will be performed to avoid future trouble. At the same time, outliers and null values must be removed, hence algorithms can be very sensitive to such numbers. We have seen already the need to remove outliers while looking to the age, population and orders range of the data.&lt;/p&gt;

&lt;p&gt;With all data prepared, is time to start understanding all features and which algorithm would be used. We start by defining a Naive Predictor, as stated before, which would set our lower boundary. Next we will explore three models: Random Forest, GradientBoost and LightGBM (LGBM).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/decision-tree-example.png&quot; alt=&quot;image-2020060217150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 7. Decision Tree Example&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This three algorithms are based in Decision Trees. Decision Trees works by creating questions and answering them. Here each node is a feature, each link is a class and each leaf is the output of such a branch, as stated below. For example, from Figure 7, we notice that people under 20 years old tend not to buy on business days. These models are part of the ensemble learning methods and predict by combining the outputs from multiple individual trees. While Random Forest Method create trees independently using random slices from data and choose from the best, the GradientBoost builds one tree at the time and improves over the last one in an iterative method, fixing errors from the previous. Although LGBM is also a tree based algorithm, it grows tree leaf-wise, maximizing classes and not deep-wise. Which can reduce the losses, choosing the leaf with max delta loss. \par
For selecting the best model, first we need a training pipeline, which would provide an automated way for training multiple classifiers. Then, three models should be defined taking into consideration the data and the goal of the project. Since this project aims to go to production, prediction time is the key evaluation metric here. Also, recall is more important than precision, because we are more interested in find all possible customers instead of find only buyers that would buy. \par
The next step is to improve the chosen model to obtain a better ROC Curve, F-score and Accuracy. To do this, is possible to use GridSearch from Sklearn library. With the best classifier in hands, we must then save it to use it later. It is possible too, to create a model with fewer features, in order to simplify a real-time predictor.&lt;/p&gt;

&lt;h3 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h3&gt;

&lt;p&gt;The most simple benchmark is called the Naive Predictor. The Naive Predictor can be explained as follows: if we take the whole customer base, and instead of guessing which of them would buy in the same day of registration, we admitted that all of them are potential buyer. How often would we be right? Accordingly to the dataset, only 34.6% of the time we would be right. It is proposed also 3 more models to serve as benchmark.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;I’ll skip the methodology used and go directly to the results achieved. Data preprocessing, implementation metrics and refinements were excluded from this blog post to keep it short. This items and more can be found at the &lt;a href=&quot;https://drive.google.com/file/d/13iXMpgMFs8mrL064gvzE7wOw6InA_FfF/view?usp=sharing&quot;&gt;full article&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;model-evaluation-and-validation&quot;&gt;Model Evaluation and Validation&lt;/h3&gt;

&lt;p&gt;Using the training pipeline stated before, the results for the three supervised learners are showed below. The dotted line in the image show the Naive Predictor performance for comparation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/trainingmodelds.png&quot; alt=&quot;image-2020060217150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 8. Performance of the three models&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Looking to the training part, we can notice that the Gradient Boost Classifier takes up 20 times more time to train in the full training set. Furthermore, it has the worst performance when compared with the others on the accuracy and the F-score metrics. In other hand, the Random Forest Classifier seems to overfit the data, with accuracy close to 100\% on training. 
The LGBM which, in turn, has the fastest training time, also has a more balanced accuracy and f-score performance. The downside of this model is that it has the longer time to predict if a customer is a truly target user, around 0,35 seconds.  But it owns the better general performance in the test set. 
Once chosen the model is time to improve its performance through tuning the hyperparameters.  Using the &lt;em&gt;gridParams&lt;/em&gt; and the scoring method as &lt;code class=&quot;highlighter-rouge&quot;&gt;scoring = 'AUC': 'roc_auc'&lt;/code&gt; the following configuration was the best one achieved. The comparison before and after the Grid Search is showed below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_estimator_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LGBMClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boosting_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbdt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;importance_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'split'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_child_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_child_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;min_data_in_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_split_gain&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_leaves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reg_alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg_lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;silent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;subsample_for_bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It could improve the ROC-AUC metric in 2,09%, which is the main metric for this project. Moreover, improvements of 2,89\% and 2,14% can be seen in accuracy and f-score.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Unoptimized&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;------&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9730&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9633&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;ROC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AUC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9737&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Optimized&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;------&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Final&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9944&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Final&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9922&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Final&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ROC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AUC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9946&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Last but not least, the ROC-AUC curve for the LGDM Classifier is showed in Figure 9. This plot show us the True Positives Rate (TPR) on the y-axis ax and the False Positive Rate (FPR) on x-axis. Showing the trade-off between sensitivity (TPR) and specificity (1 – FPR). From this analysis we can see that the model has a prediction rate of false positives closer to 0\% and 99\% to true positives. Meaning how well the model can distinguish from the two classes, buyers and non buyers. The blue line shows what we should expect from a random classifier, meaning the classifier can’t separate between classes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/roccurve.png&quot; alt=&quot;image-2020060217150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 9. LGDM Classifier’s ROC-AUC Curve&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;justification&quot;&gt;Justification&lt;/h3&gt;

&lt;p&gt;After careful consideration, the Light GBM was chosen. It has proven to have the best performance of all models. Despite its predicting time, it is still suitable for the application. The table below show the results of all models serving as a benchmark for this project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/pred-buyers/roc-auc.png&quot; alt=&quot;image-2020060217150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Tuned Light GBM is definitely the best solution for the problem proposed. It can clearly distinguish possible buyers from people that are only checking the platform. This model is only the first layer in a possible chain of predictors to better understand consumers behavior and add features to food delivery platforms. 
Further, using PCA and Feature Importance, a model containing only 5 features were trained. Using these 5 features: ‘id’,’density’,’month’,’classification’,’day_of_week’, an Accuracy of 99.02%, F-Score of 98.42% and a ROC-AUC of 99.10% were achieved.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The process to start this project began choosing a real problem to solve. With my experiences in grocery/food delivery platforms, I thought that a good feature to have would be predicting if a consumer would buy or not. Besides improving the customer experience, it could help the platform convert more users into buyers. The first step was to choose between a model that could generalize an identify if old and new customers would buy or just to understand new users behavior.  The last was chosen due its simplicity and probably better accuracy.
A big challenge was to gather the data from the database. More than that, choose the right period of time to extract and which information would be more important. Once this was decided, the preprocessing phase was by far the most time-consuming. It is the most delicate step and can change the entire output from the model, if done incorrectly. I forgot to remove duplicate UserID once, which give the model accuracy as low as the naive predictor. 
From then the project was pretty straight forward. The training pipeline and the use of GridSearch, although time-consuming as well, was simple to handle. With the LGBM tuned and working, I tried to use some feature reduction to see how much of its accuracy we could keep. I expect to deploy this classifier to production and hope to see applied in some feature in the future.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/13iXMpgMFs8mrL064gvzE7wOw6InA_FfF/view?usp=sharing&quot;&gt;Read the full article here.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">This project has the objective to create a Machine Learning Model to predict whether a user that just registered will buy or not. Predicting such action could influence from user experience until predicting demands for restaurants.</summary>
      

      
      
    </entry>
  
</feed>
