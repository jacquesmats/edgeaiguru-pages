<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/projects/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-08-17T00:11:17-03:00</updated>
  <id>http://localhost:4000/tag/projects/feed.xml</id>

  
  
  

  
    <title type="html">Edge AI Guru | </title>
  

  
    <subtitle>The guidance through your Artificial Intelligence journey</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Tracking 3D objects with Lidar and Camera</title>
      <link href="http://localhost:4000/3D-Object-Tracking" rel="alternate" type="text/html" title="Tracking 3D objects with Lidar and Camera" />
      <published>2020-07-23T07:15:00-03:00</published>
      <updated>2020-07-23T07:15:00-03:00</updated>
      <id>http://localhost:4000/3D%20Object%20Tracking</id>
      <content type="html" xml:base="http://localhost:4000/3D-Object-Tracking">&lt;p&gt;Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)&lt;/p&gt;

&lt;p&gt;
    Autonomous vehicles technology has been on my radar (no pun intended) for a while now. I was excited to apply AI to some very complex problems and this project was for sure one of the most amazing I ever did. More than Machine and Deep Learning techniques, solving autonomous driving requires several technologies and methods. All this is only possible through fusing sensors to make sense of the environment.
&lt;/p&gt;
&lt;p&gt;
    To fusion two sensors was a completely new challenge for me and seeing the results is awesome. After some introduction on how camera technology works and how optics are applied in modern systems, some image processing, and computer vision applications were covered,  to better understand how to fusion Lidar and Camera data. From this point on, I tried to apply this knowledge to a collision avoidance project for vehicles. 
&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T2UWLYwpuv4?controls=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/structure.png&quot; alt=&quot;Structure&quot;&gt;&lt;/p&gt;

&lt;i&gt;Project Structure. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;



&lt;p&gt;
    From cameras we detect images key points, such as tail lights, extract these keypoints descriptors and match them between successive images and observe its distances, computing Camera TCC. From Lidar we take a 3D Point Cloud, crop the points(discard road and bad measurements) and cluster these into objects, identified using Neural Networks, and provided with bounding boxes. We than fusion both information, connecting them into space and tracking 3D object bounding boxes to compute the Lidar TCC on the object in front of the car. Conducting various tests with the framework, we tried to identify the most suitable detector/descriptor combination for TTC estimation and also to search for problems that can lead to faulty measurements by the camera or Lidar sensor.
&lt;/p&gt;

&lt;h2&gt;The Technology&lt;/h2&gt;
&lt;br&gt;

&lt;p&gt;	
    To make it happen, everything was coded in C++ using the &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti&quot;&gt;KITTI dataset&lt;/a&gt;. Besides, the whole code can be found in my &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;github&lt;/a&gt;. This project is part of the &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;.
&lt;/p&gt;
&lt;blockquote cite=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;
    &lt;p&gt;Our recording platform is a Volkswagen Passat B6, which has been modified with actuators for the pedals (acceleration and brake) and the steering wheel. The data is recorded using an eight core i7 computer equipped with a RAID system, running Ubuntu Linux and a real-time database. We use the following sensors:&lt;/p&gt;
    &lt;ul&gt;
        &lt;li&gt;1 Inertial Navigation System (GPS/IMU): OXTS RT 3003&lt;/li&gt;
        &lt;li&gt;1 Laserscanner: Velodyne HDL-64E&lt;/li&gt;
        &lt;li&gt;2 Grayscale cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3M-C)&lt;/li&gt;
        &lt;li&gt;2 Color cameras, 1.4 Megapixels: Point Grey Flea 2 (FL2-14S3C-C)&lt;/li&gt;
        &lt;li&gt;4 Varifocal lenses, 4-8 mm: Edmund Optics NT59-917&lt;/li&gt;
    &lt;/ul&gt;
&lt;/blockquote&gt;


&lt;br&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/passat_sensors.png&quot; alt=&quot;Passat&quot;&gt;&lt;/p&gt;
&lt;i&gt;KITTI Passat Sensor Setup. Source: &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/setup.php&quot;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;





&lt;h2&gt;The Project&lt;/h2&gt;
&lt;br&gt;
&lt;p&gt;
    Cameras are one mature technology and the main sensor in autonomous vehicles. In a world made by humans, everything is adapted to our senses and driving would not be different. Therefore, its main advantage is to &quot;simulate&quot; the human vision being able to interpret 2D information as road signs and lanes. At the same time, it has challenges as we humans: bad performance in darkness or bad weather conditions. Thus, alone in some applications, the camera will fail, hence the need to add a second sensor such as Radar or Lidar. To fill the gap, Lidar has high resolution and can reconstruct 3D objects while Radar has a greater range and can detect velocity more accurately.
&lt;/p&gt;
&lt;p&gt;
    In this project, the main objective was to estimate the Time to Collision(TCC) using a camera-based object classification to cluster Lidar points and from 3D bounding boxes compute TCC. Human reflection time is around 1 second, so the system has to warn us way before and starting breaking 2 to 3 seconds before the collision. But how do we tell a machine a collision is near? How do we compute such time? The first concern is to choose a Motion Model: Constant Velocity(CVM) or Constant Acceleration(CAM). While CAM models best real-world situations, CVM assumes that the velocity does not change and it was chosen for simplicity, leaving space for us to focus more on the computer vision aspects.  
&lt;/p&gt;
&lt;p&gt;
    From that, the problem can be divided into two main parts: estimate TTC from Lidar points and estimate it using camera successive images. The former is pretty straightforward, addressing only the preceding car and calculating the time from the X component in the Point Cloud.  Based on the CVM, we can compute the velocity based in two Lidar measurements over time, taking the closest Lidar point in the path of the car and observing how it changes in a time window. To avoid erroneous measurements that would lead to false TCC, we perform a &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule&quot;&gt;IQR range values&lt;/a&gt; to remove outliers. Due to some measurement accuracy based on the amount of light reflected from an object, when using Lidar for TCC is advised to remove measurement of the road surface and low reflectivity points, making it easy to measure the distance of the preceding vehicle. Furthermore, instead of estimating the velocity after computing the vehicle velocity and two distance measurements, that could be noisy, we could use a Radar sensor that measures directly the relative speed. 
&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/keypoints.jpg&quot; alt=&quot;Keypoints&quot;&gt;&lt;/p&gt;

&lt;i&gt;Keypoints detection and tracking using descriptors. Source: &lt;a href=&quot;https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313&quot;&gt;Udacity Sensor Fusion Nanodegree&lt;/a&gt;&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;    
    In another hand, it is hard to measure metric distances using only one camera, some companies achieve measuring distance using a stereo camera setup, like &lt;a href=&quot;http://www.6d-vision.com/&quot;&gt;Mercedes-Benz which pioneered in this technology&lt;/a&gt;. With two pictures of the same place, taken from two different points of view, is possible to locate points of interest in both images and calculate its distance using geometry and perspective projection. Using a mono camera we can estimate TCC only by observing relative height change (or scale change) in the preceding car, for example, without distance measurement. Here enters Deep Learning, allowing us to identify cars in images. As showed in the figure above, it is also used CV techniques to find key points and track them using descriptors from one frame to the next, estimating the scale change in the object. Tracking how these descriptors change over time, we can estimate TTC.
&lt;/p&gt;
&lt;p&gt;
    First, we focus on loading images, setting up data structures, and putting everything into a ring buffer to optimize memory load. Then, integrated several keypoint detectors such as HARRIS, FAST, BRISK, and SIFT and compare them with regard to the number of key points and speed. For the descriptor extraction and matching, we used brute force and also the FLANN approach and tested the various algorithms in different combinations and compare them with regard to some performance measures. 
    Counting the number of key points on the preceding vehicle for 10 images and taking note of the distribution of their neighborhood size, doing this for several detectors implemented, we noted that FAST, BRISK, and AKASE were the detectors that identified the larger number of keypoints.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/3d-obj-tracking/comparision_table.png&quot; alt=&quot;Table&quot;&gt;&lt;/p&gt;
&lt;i&gt;Comparision table between several detectors to identify keypoints&lt;/i&gt;
&lt;br&gt;
&lt;p&gt;
    During the matching step, the Brute Force approach is used with the descriptor distance ratio set to 0.8, and the time it takes for keypoint detection and descriptor extraction was logged. A helper function was created to iterate through all the possible detectors and descriptors, saving its matches and times. From this was identified that the best Detector for this application is FAST, and the best Descriptors are BRIEF and ORB.
&lt;/p&gt;
&lt;p&gt;
    Then we detected and classify images using the YOLO framework, which gives us a set of bounding boxes and augmenting them associating each bounding box to its respective Lidar points. Finally, we track these bounding boxes over time to estimate Time To Collision with Lidar and Camera measurements. When taking into account both TCC, twenty pairs of Detector/Descriptors were analyzed and AZAKE/FREAK was chosen based on the average error when compared to Lidar's TCC.
&lt;/p&gt;
&lt;p&gt;
    To check the complete code with comments, tasks and the results, please check the &lt;a href=&quot;https://github.com/jacquesmats/3D_Object_Tracking&quot;&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">Integrating Lidar Point Clouds with Camera Images to compute Time To Collision (TTC)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Facial Keypoint Detection</title>
      <link href="http://localhost:4000/Facial-Keypoint-Detection" rel="alternate" type="text/html" title="Facial Keypoint Detection" />
      <published>2020-06-15T07:15:00-03:00</published>
      <updated>2020-06-15T07:15:00-03:00</updated>
      <id>http://localhost:4000/Facial%20Keypoint%20Detection</id>
      <content type="html" xml:base="http://localhost:4000/Facial-Keypoint-Detection">&lt;p&gt;This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some examples of images and corresponding facial keypoints.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/key_pts_example.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and &lt;strong&gt;68 keypoints, with coordinates (x, y), for that face&lt;/strong&gt;.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. This keypoint extraction project is part of the &lt;a href=&quot;https://www.udacity.com/course/computer-vision-nanodegree--nd891&quot;&gt;Udacity Computer Vision Nanodegree.&lt;/a&gt; Here they are, numbered, and you can see that specific ranges of points match different portions of the face.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/landmarks_numbered.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;In this project, we defined and trained a CNN to identify and extract facial keypoints, founded in the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;. Coded using Python and Pytorch, first we get familiar with the dataset and do all the processing and transformation using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataloader&lt;/code&gt;. Using a Custom LeNet-5 Architecture, we trained a model to identify such keypoints in the faces located by a OpenCV’s pre-trained Haar Cascade classifier. &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;Code and tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;The first step in working with any dataset is to become familiar with your data; we’ll need to load in the images of faces and their keypoints and visualize them! This set of image data has been extracted from the &lt;a href=&quot;https://www.cs.tau.ac.il/~wolf/ytfaces/&quot;&gt;YouTube Faces Dataset&lt;/a&gt;, which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.&lt;/p&gt;

&lt;h4 id=&quot;training-and-testing-data&quot;&gt;Training and Testing Data&lt;/h4&gt;

&lt;p&gt;This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3462 of these images are training images, to create a model to predict keypoints.&lt;/li&gt;
  &lt;li&gt;2308 are test images, which will be used to test the accuracy of the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas&lt;/code&gt;. When we read the training CSV and get the annotations, they are in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).&lt;/p&gt;

&lt;p&gt;Below, in an output from the function &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt; that takes in an image and keypoints and displays them. As you look at this data, &lt;strong&gt;note that these images are not all of the same size&lt;/strong&gt;, and neither are the faces! To eventually train a neural network on these images, we’ll need to standardize their shape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/show_keypoints.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this project, to prepare our data for training, we used PyTorch’s Dataset class. Much of this this code is a modified version of what can be found in the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/data_loading_tutorial.html&quot;&gt;PyTorch data loading tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;dataset-class&quot;&gt;Dataset class&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; is an abstract class representing a dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.&lt;/p&gt;

&lt;p&gt;Our custom dataset should inherit &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt; and override the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__len__&lt;/code&gt; so that &lt;code class=&quot;highlighter-rouge&quot;&gt;len(dataset)&lt;/code&gt; returns the size of the dataset.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt; to support the indexing such that &lt;code class=&quot;highlighter-rouge&quot;&gt;dataset[i]&lt;/code&gt; can be used to get the i-th sample of image/keypoint data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here we create a dataset class for our face keypoints dataset. We read the CSV file in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; but leave the reading of images to &lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt;. This is memory efficient because all the images are not stored in the memory at once but read as required.&lt;/p&gt;

&lt;p&gt;A sample of our dataset will be a dictionary &lt;code class=&quot;highlighter-rouge&quot;&gt;{'image': image, 'keypoints': key_pts}&lt;/code&gt;. Our dataset will take an
optional argument &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; so that any required processing can be applied on the sample.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FacialKeypointsDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;# if image has an alpha color channel, get rid of it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_pts_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'float'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'keypoints'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After we’ve defined this class, we can instantiate the dataset and display some images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/sample0.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transforms&quot;&gt;Transforms&lt;/h3&gt;

&lt;p&gt;Now, the images in this dataset are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.&lt;/p&gt;

&lt;p&gt;Therefore, we will need to write some pre-processing code. The following four transforms were created:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Normalize&lt;/code&gt;: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Rescale&lt;/code&gt;: to rescale an image to a desired size.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RandomCrop&lt;/code&gt;: to crop an image randomly.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ToTensor&lt;/code&gt;: to convert numpy images to torch images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it’s called. For this, we just implemented &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; method and (if we require parameters to be passed in), the &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; method.  We can then use a transform like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tx = Transform(params)
transformed_sample = tx(sample)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Observe below how these transforms are generally applied to both the image and its keypoints. Let’s test these transforms out to make sure they behave as expected. As you look at each transform, note that, in this case, &lt;strong&gt;order does matter&lt;/strong&gt;. For example, you cannot crop a image using a value smaller than the original image (and the original images vary in size!), but, if you first rescale the original image, you can then crop it to any size smaller than the rescaled size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/transform.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After creating the transformed dataset, applying the transforms in order to get grayscale images of the same shape, verify that your transform works, we used &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt; iterator which provides features like Batch the data, Shuffle the data and Load the data in parallel using &lt;code class=&quot;highlighter-rouge&quot;&gt;multiprocessing&lt;/code&gt; workers.&lt;/p&gt;

&lt;p&gt;Now that we load and transform the data, we are ready to build a neural network to train on this data.&lt;/p&gt;

&lt;h2 id=&quot;the-convolutional-neural-network&quot;&gt;The Convolutional Neural Network&lt;/h2&gt;

&lt;p&gt;After looking at the data we’re working with and, in this case, know the shapes of the images and of the keypoints, we can define a convolutional neural network that can &lt;em&gt;learn&lt;/em&gt; from this data. Recall that CNN’s are defined by a few types of layers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convolutional layers&lt;/li&gt;
  &lt;li&gt;Maxpooling layers&lt;/li&gt;
  &lt;li&gt;Fully-connected layers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-neural-nets&quot;&gt;PyTorch Neural Nets&lt;/h3&gt;

&lt;p&gt;To define a neural network in PyTorch, we define the layers of a model in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and define the feedforward behavior of a network that employs those initialized layers in the function &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt;, which takes in an input image tensor, &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;. Also note that during training, PyTorch will be able to perform backpropagation by keeping track of the network’s feedforward behavior and using autograd to calculate the update to the weights in the network.&lt;/p&gt;

&lt;p&gt;Best practice is to place any layers whose weights will change during the training process in &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; and refer to them in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function; any layers or functions that always behave in the same way, such as a pre-defined activation function, should appear &lt;em&gt;only&lt;/em&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;The model used in this project was a Custom LeNet-5 Architecture, which you can check in the &lt;a href=&quot;https://github.com/jacquesmats/facial_keypoint_detection&quot;&gt;repository&lt;/a&gt; under the file &lt;code class=&quot;highlighter-rouge&quot;&gt;models.py&lt;/code&gt;, using 4 Convolutional Layers followed by a Poooling Layer and 2 Fully Connected. After reading the paper from &lt;a href=&quot;https://arxiv.org/pdf/1710.00977.pdf&quot;&gt;NaimishNet&lt;/a&gt;, I realized, from Fig.21, that LeNet-5 had a good performance while with fewer layers than NaimishNet. I started if LeNet-5 and change it until reaching the final mode. Adding two more Conv. Layers, replacing AvgPool by MaxPool (since were more common use avg back then) and Tanh activations by ReLU, improvements made in this 20 years.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=25600, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=136, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;feature-maps&quot;&gt;Feature maps&lt;/h3&gt;

&lt;p&gt;Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it’s convolutional kernels and when these kernels are applied to some input image, they produce a set of &lt;strong&gt;feature maps&lt;/strong&gt;. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/feature_map_ex.png&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;To prepare for training, create a transformed dataset of images and keypoints. In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so your model’s loss does not blow up during training, it is also suggested that you normalize the input images and keypoints. The necessary transforms have been defined in &lt;code class=&quot;highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next, having defined the transformed dataset, we use PyTorch’s DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in &lt;a href=&quot;http://pytorch.org/docs/master/data.html&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also defined some functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;net_sample_output(): test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints.&lt;/li&gt;
  &lt;li&gt;show_all_keypoints(): displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided).&lt;/li&gt;
  &lt;li&gt;visualize_output(): this function’s main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors &lt;em&gt;and&lt;/em&gt; it undoes the keypoint normalization done in the Normalize() transform; it’s assumed that you applied these transformations when you loaded your test data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After testing the optimizer with Adam and SGD, I kept Adam because is faster and converges faster also. About the loss functions, MSE and L1Smooth had a similar performance with 5 epochs, so I kept because of the NaimishNet reference. In 60 epochs, the average loss was 0.000651, resulting in the following detection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download.png&quot; alt=&quot;image-20200612307150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download2.png&quot; alt=&quot;image-20200602137150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/download3.png&quot; alt=&quot;image-20200607121350421443&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;face-and-facial-keypoint-detection&quot;&gt;Face and Facial Keypoint detection&lt;/h2&gt;

&lt;p&gt;After you’ve trained a neural network to detect facial keypoints, you can then apply this network to &lt;em&gt;any&lt;/em&gt; image that includes faces. The neural network expects a Tensor of a certain size as input and, so, to detect any face, we first have to do some pre-processing. The following image will be used for test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamas.jpg&quot; alt=&quot;image-20200607150421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;detect-all-faces-in-an-image&quot;&gt;Detect all faces in an image&lt;/h3&gt;

&lt;p&gt;Here used one of OpenCV’s pre-trained Haar Cascade classifiers, all of which can be found in the &lt;code class=&quot;highlighter-rouge&quot;&gt;detector_architectures/&lt;/code&gt; directory, to find any faces in your selected image. In the figure below, we loop over each face in the original image and draw a red square on each face (in a copy of the original image, so as not to modify the original). You can even &lt;a href=&quot;https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html&quot;&gt;add eye detections&lt;/a&gt; while using Haar detectors. An example of face detection is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obamamichele.png&quot; alt=&quot;image-2020060715210421443&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;keypoint-detection&quot;&gt;Keypoint detection&lt;/h3&gt;

&lt;p&gt;Now, we loop over each detected face in an image (again!) only this time, you’ll transform those faces in Tensors that your CNN can accept as input images. First we convert the face from RGB to grayscale and normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]. So we rescale the detected face to be the expected square size for your CNN (224x224, suggested) and reshape the numpy image into a torch image.&lt;/p&gt;

&lt;p&gt;And finally, after each face has been appropriately converted into an input Tensor for your network to see as input, we apply our &lt;code class=&quot;highlighter-rouge&quot;&gt;net&lt;/code&gt; to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be “un-normalized” for display, and we write a helper function like &lt;code class=&quot;highlighter-rouge&quot;&gt;show_keypoints&lt;/code&gt;. We end up with an image like the following with facial keypoints that closely match the facial features on each individual face:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/obama.png&quot; alt=&quot;image-20200123607150421443&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/facial-kpts/michele.png&quot; alt=&quot;image-20200607131250421443&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Would you like to  receive the most important news and trends in AI in just one email every end of the month? Subscribe below and I’ll send you the 10 most important news in Artificial Intelligence&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="projects" />
      

      
        <summary type="html">This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.</summary>
      

      
      
    </entry>
  
</feed>
