<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/portuguese/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-08-20T14:37:41-03:00</updated>
  <id>http://localhost:4000/tag/portuguese/feed.xml</id>

  
  
  

  
    <title type="html">Edge AI Guru | </title>
  

  
    <subtitle>The guidance through your Artificial Intelligence journey</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Destaques de Maio em IA</title>
      <link href="http://localhost:4000/Destaques-de-Maio" rel="alternate" type="text/html" title="Destaques de Maio em IA" />
      <published>2000-05-28T07:15:00-03:00</published>
      <updated>2000-05-28T07:15:00-03:00</updated>
      <id>http://localhost:4000/Destaques%20de%20Maio</id>
      <content type="html" xml:base="http://localhost:4000/Destaques-de-Maio">&lt;p&gt;As 5 principais notícias e tendências da IA que cruzei em maio de 2020.&lt;/p&gt;

&lt;p&gt;English version &lt;a href=&quot;http://edgeaiguru.com/May-Highlights&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/google.gif&quot; alt=&quot;Aprendizado geral de representação visual do Google BiT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Modelo BiT sendo usado após o pré-treinamento e aplicado a outras tarefas com poucos exemplos rotulados&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-disponibiliza-bit-o-bert-para-visão-computacional&quot;&gt;Google disponibiliza BiT, o BERT para Visão Computacional&lt;/h2&gt;

&lt;p&gt;Em 21 de maio, a equipe de inteligência artificial do Google abriu o código do Big Transfer (BiT): um aprendizado geral sobre representação visual. Este é um esforço para ajudar aplicações de Visão Computacional a avançarem com modelos pré-treinados em larga escala. O uso do BiT é possível para obter desempenho de ponta em muitas aplicações, usando apenas alguns exemplos rotulados. Para fazer o ajuste fino desse modelo, eles também introduziram o “BiT-HyperRule”, uma abordagem heurística para selecionar hiper parâmetros, como taxa de aprendizado. Após a transferência de aprendizado, usando o conjunto de dados CIFAR-10, o modelo alcançou 64% e 95% de precisão média em 1 e 5 em &lt;em&gt;One Shot Learning&lt;/em&gt;, respectivamente. O Google também mencionou que “o BiT-L combina ou supera resultados […] em vários benchmarks de visão computacional, como Oxford &lt;a href=&quot;https://www.robots.ox.ac.uk / ~ vgg / data / pets /&quot;&gt;Pets&lt;/a&gt; e &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/flowers/&quot;&gt;Flowers&lt;/a&gt;, &lt;a href=&quot;https: //www.cs.toronto .edu / ~ kriz / cifar.html&quot;&gt;CIFAR&lt;/a&gt; etc. “. Você pode verificar o &lt;a href=&quot;https://arxiv.org/abs/1912.11370&quot;&gt;artigo&lt;/a&gt; e o &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;&gt;código&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/tf_logo_social.png&quot; alt=&quot;Tensorflow Logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-versão-220-do-tensorflow-chegou&quot;&gt;A versão 2.2.0 do Tensorflow chegou&lt;/h2&gt;

&lt;p&gt;Depois de quase quatro meses após o TF 2.1.0, este mês o Google também lançou o Tensorflow 2.2.0. As principais atualizações incluem um novo Profiler para CPU / GPU / TPU para ajudar a identificar gargalos de desempenho, melhorias de desempenho na computação entre dispositivos por meio de &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.distrbuite&lt;/code&gt;, um novo tipo escalar para strings (de&lt;code class=&quot;highlighter-rouge&quot;&gt; std :: string&lt;/code&gt; para &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow :: tstring&lt;/code&gt;) e a substituição do SWIG pelo pybind11 ao exportar funções C++. Também tivemos algumas melhorias no Keras, como o uso de lógicas de treinamento personalizadas com a substituição de &lt;code class=&quot;highlighter-rouge&quot;&gt;Model.fit&lt;/code&gt; pelo ` Model.train_step` e o formato SavedModel que agora suporta todas as camadas internas do Keras (incluindo métricas, camadas de pré-processamento e RNN com monitoração de estado). Além disso, outro destaque é que eles abandonaram o suporte ao Python 2, que não é mantido desde janeiro de 2020. Por último, nesta versão, o TensorFlow requer a versão gast 0.3.3. Verifique o texto completo &lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/nvidia-a100.jpg&quot; alt=&quot;NVIDIA A100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-lança-a100-ai-gpu-o-melhor-instrumento-para-o-avanço-da-ia&quot;&gt;NVIDIA lança A100 AI GPU: o melhor instrumento para o avanço da IA&lt;/h2&gt;

&lt;p&gt;Anunciado em 14 de maio durante o evento Nvidia GTC 2020, o &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/a100/&quot;&gt;A100 AI Chip&lt;/a&gt; possui 54 bilhões de transistores e 5 petaflops de desempenho sendo a mais poderosa plataforma de IA e HPC para data centers, seguindo a tendência de HPC após &lt;a href=&quot;https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for- 6 a 9 bilhões&quot;&gt;adquirir a Mellanox em março&lt;/a&gt;. Com base na arquitetura Ampere, o chip de 7 nanômetros pode ser usado para computação científica, gráficos em nuvem e análise de dados. Aqui estão alguns números, 20 vezes mais potente que a GPU Tesla V100 anterior, o A100 alcança 19.5 de potência computacional com seus 6.912 núcleos CUDA, 40 GB de memória, 1,6 TB de largura de banda e a terceira geração de Tensors Core. Uma aplicação desse chip é o sistema &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;DGX&lt;/a&gt;, baseado em 8 NVIDIA A100 e direcionado ao setor de Inteligência Artificial, custa US$ 1 milhão. Podemos esperar ver o novo chip AI em serviços de nuvem em um futuro próximo. A Alibaba Cloud, AWS, Baidu Cloud, Google Cloud, Microsoft Azure, Oracle e Tencent Cloud já firmaram parceria com a NVIDIA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/fb.jpeg&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;facebook-ai-divulga-groknet-para-entendimento-avançado-do-produtos&quot;&gt;Facebook AI divulga GrokNet para entendimento avançado do produtos&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;“um sistema universal de visão computacional projetado para compras. Ele pode identificar atributos de produtos dentre bilhões de fotos - em diferentes categorias, como moda, automóveis e decoração de casa”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Diferente da maioria dos modelos que tem apenas uma categoria para aprender, o GrokNet foi treinado em sete conjuntos de dados, com vários tipos de supervisão e 83 funções de transferência, conseguiu prever categorias, atributos como cor e textura e prováveis consultas de pesquisa. O modelo do FB é treinado por um banco de dados colossal com cerca de 100 milhões de imagens retiradas da plataforma. A empresa também afirma que o sistema é capaz de identificar objetos que foram fotografados com condições de iluminação não muito boas e ângulos que não favorecem uma boa visualização do produto. Para dar ao modelo a capacidade de entender tarefas fáceis e difíceis, eles deram às tarefas uma combinação de peso e imagens para treinamento, ou seja, tarefas mais fáceis não precisam de muita supervisão e podem receber um peso menor. Comparado aos sistemas de atribuição de texto, o novo modelo pode prever anúncios de casas e jardins com precisão de 90%, contra 33% do antigo. O Facebook espera que “no futuro, o GrokNet possa ser usado para ajudar os clientes a encontrar facilmente exatamente o que estão procurando, receber sugestões personalizadas das vitrines sobre quais produtos são mais relevantes para eles e quais são compatíveis”. Verifique o anúncio oficial &lt;a href=&quot;https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/may-news/sudo.gif&quot; alt=&quot;Sudo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lançado-o-sudo-190-com-atualizações-de-segurança&quot;&gt;Lançado o Sudo 1.9.0 com atualizações de segurança&lt;/h2&gt;

&lt;p&gt;Comando básico em nossos trabalhos diários com nossos amados sistemas Unix, o sudo permite que os usuários executem programas como root. Em maio, o sudo recebeu uma atualização da versão 1.9.0 após 9 anos no &lt;em&gt;branch&lt;/em&gt; 1.8. Esta nova versão traz atualizações de segurança como, e centraliza o registro através do &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo_logsrvd&lt;/code&gt;, criando o sudo com a opção“ –enable-openssl ”, os dados são transmitidos através de um canal de comunicação criptografado (TLS). Agora também é possível usar softwares de terceiros para extrair dados de sessões sudo para auditoria. E embora o TF abandone o suporte ao Python 2, ele chega ao Sudo 1.9, onde os plugins podem ser escritos em python agora, que é ativado durante a compilação com a opção “–enable-python”. Verifique todas as alterações na versão completa &lt;a href=&quot;https://www.sudo.ws/stable.html#1.9.0&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Quer receber as notícias e tendências mais importantes da IA em apenas um e-mail a cada final de mês? Inscreva-se abaixo e vou te enviar as 10 notícias mais importantes em Inteligência Artificial.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="portuguese" />
      
        <category term="news" />
      

      
        <summary type="html">As 5 principais notícias e tendências da IA que cruzei em maio de 2020.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introdução a Redes Neurais</title>
      <link href="http://localhost:4000/Introdu%C3%A7%C3%A3o-a-Redes-Neurais" rel="alternate" type="text/html" title="Introdução a Redes Neurais" />
      <published>2000-05-03T07:18:00-03:00</published>
      <updated>2000-05-03T07:18:00-03:00</updated>
      <id>http://localhost:4000/Introdu%C3%A7%C3%A3o%20a%20Redes%20Neurais</id>
      <content type="html" xml:base="http://localhost:4000/Introdu%C3%A7%C3%A3o-a-Redes-Neurais">&lt;p&gt;Aprenda os principais conceitos por trás das Redes Neurais, um dos pilares do Deep Learning.&lt;/p&gt;

&lt;p&gt;English version &lt;a href=&quot;http://edgeaiguru.com/Introduction-to-Neural-Networks&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introdução&quot;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;A Inteligência Artificial vem revolucionando a indústria nos últimos anos e resolvendo problemas, que antes eram onerosos em tempo e dinheiro, de maneira muito mais eficaz. Problemas de visão computacional, processamento de linguagem natural e diversas outras aplicações só são possíveis graças aos avanços em Aprendizagem Profunda.&lt;/p&gt;

&lt;p&gt;As Redes Neurais Artificiais (RNA) são um dos principais pilares dessa tecnologia. Inspiradas no cérebro humano, as RNA levam esse nome pois tem conexões e motivações biológicas. Assim como no cérebro humano, onde unidade mais básica de processamento é o neurônio, as RNA possuem um elemento que processa impulsos, ou entradas, e que também é chamado de neurônio ou nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/artificial-vs-biological-neuron.png&quot; alt=&quot;Artificial vs Biological Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Neurônio Biológico vs Neurônio Artificial. Fonte: &lt;a href=&quot;https://www.datacamp.com/community/tutorials/deep-learning-python&quot;&gt;Keras Tutorial: Deep Learning in Python&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ambas estruturas compartilham o mesmo funcionamento para a transferência de informações: recebem uma entrada (impulso) que é carregada através do nó (corpo da célula) e ativam um certa saída (terminais axônicos). De mesma forma como nos neurônios biológicos, esse impulso nervoso que ativa o neurônios é reproduzida nas RNA através de funções de ativação.&lt;/p&gt;

&lt;p&gt;Logo, esse elemento básico das redes neurais podem ser representado pela seguinte figura, retirada do curso &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning&quot;&gt;Neural Networks and Deep Learning&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-neuron.png&quot; alt=&quot;Generic Neuron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Onde, através do exemplo da necessidade de prever o preço de casas baseado no seu tamanho, podemos traçar um função que consiga representar esse problema. Nesse exemplo, uma função ReLU encaixa perfeitamente nos dados. Então, a mínima representação de um neurônio seria colocarmos na entrada a área de uma casa e, baseado na função matemática colocada “dentro” do neurônio, podemos estimar um preço para essa residência.&lt;/p&gt;

&lt;p&gt;Dessa forma, treinamos cada neurônio para ser ativado quando um certo padrão aparece. Assim, o agrupamento de diversos neurônios em série e em paralelo, permite as Redes Neurais a aprender a reconhecer padrões em imagens, textos, áudios e nas mais diversas formas de dados.&lt;/p&gt;

&lt;p&gt;Nesse artigo, será aprensentado os principais componentes das Redes Neurais Artificiais, algumas das principais arquiteturas, as funções de ativações mais comuns.&lt;/p&gt;

&lt;h2 id=&quot;redes-neurais-artificias&quot;&gt;Redes Neurais Artificias&lt;/h2&gt;

&lt;p&gt;Apesar das Redes Neurais terem algumas semelhanças com os neurônios do cérebro humano, essas são infinitamente mais simples do que seu correspondente biológico. Essas arquiteturas são compostas por blocos matemáticos que podem ser explicados utilizando álgebra e cálculo, muito diferentemente das diversas partes do cérebro que ainda não conseguimos entender.&lt;/p&gt;

&lt;p&gt;Os principais componentes das RNA são: a camada de entrada, as camadas ocultas e as camadas de saída. Essas camadas são ligadas através de conexões que têm pesos, esses definem o quão importante aquela conexão é para a rede. Além disso, como vimos anteriormente, na saída de cada neurônio existe um função de ativação que definirá se o neurônio irá ativar ou não.&lt;/p&gt;

&lt;h2 id=&quot;blocos-de-uma-rede-neural-artificial&quot;&gt;Blocos de uma Rede Neural Artificial&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/generic-3-layers-neural-network.png&quot; alt=&quot;Generic 3 Layer Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Arquitetura de uma Rede Neural Genérica de 3 Camadas. Fonte: &lt;a href=&quot;https://cs231n.github.io/neural-networks-1/#nn&quot;&gt;Stanford CS231n&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;camada-de-entrada&quot;&gt;&lt;em&gt;Camada de Entrada&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Um bloco de neurônios pode ser chamado de camada. Mas perceba que apesar de os neurônios se interligarem entre camadas, eles não tem conexões dentro da mesma camada. Como mostra a figura acima, a primeira camada de uma Rede Neural é a camada de entrada. Esta tem apenas a função de passar as entradas do sistema para a próxima camada e não realiza nenhuma função matemática.&lt;/p&gt;

&lt;h3 id=&quot;camadas-ocultas&quot;&gt;&lt;em&gt;Camadas Ocultas&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Essa camada é responsável por uma das principais funções das redes neurais: processar os dados e enviá-los para a camada seguinte. O valor de cada neurônio é encontrado multiplicando o pesos W pela entrada X e somando um viés b. Esse valor então passa por uma função de ativação e é enviada a próxima camada, como mostra a Fig. 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/hidden-layer-mathematics.png&quot; alt=&quot;Hidden Layer Mathematics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Operações Matemáticas dentro do Neurônio. Fonte: Fonte: &lt;a href=&quot;https://cs231n.github.io/neural-networks-1/#nn&quot;&gt;Stanford CS231n&lt;/a&gt; Modificada.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Assim, se isolarmos o primeiro neurônio da primeira camada oculta, o valor de saída do neurônio será igual a z1. Onde &lt;em&gt;s1&lt;/em&gt; é a entrada do neurônio, onde multiplicamos os pesos pelas entradas e somamos um viés b. Após essa operação, é aplicada então uma função de transferência &lt;em&gt;g&lt;/em&gt; sobre o &lt;em&gt;s1.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;É importante notar que &lt;em&gt;X&lt;/em&gt; e &lt;em&gt;W&lt;/em&gt; na primeira equação são matrizes nesse caso, e representam todas as entradas e todos os pesos, respectivamente.&lt;/p&gt;

&lt;p&gt;Chamamos essa camada de “Camada Oculta” pois durante o treinamento de redes neurais temos as entradas que são conhecidas e as saídas que são esperadas. Mas não vemos quais os valores dentro dos neurônios dessa camada. Esse bloco pode conter diversas camadas ocultas, e quanto mais camadas mais “profunda” é a rede neural, e mais padrões ela consegue aprender.&lt;/p&gt;

&lt;h3 id=&quot;camadas-de-saída&quot;&gt;&lt;em&gt;Camadas de Saída&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;A camada de saída é a responsável por mostrar os resultados obtidos através dos cálculos feitos nas camadas ocultas. Normalmente é utilizada uma função de ativação, assim como a dos neurônios das camadas anteriores, para simplificar o resultado.&lt;/p&gt;

&lt;h3 id=&quot;pesos-e-viés&quot;&gt;&lt;em&gt;Pesos e Viés&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Os pesos são responsáveis por definir o quão importante aquela conexão é para a rede neural. Como existem diversas conexões dentro das RNA, é dessa forma que essa arquitetura entende quais padrões ela deve aprender e quais ela deve ignorar. Além disso, comumente é utilizado um valor chamado de viés junto aos pesos e as entradas. Esse valor ajuda a fazer um ajuste fino na rede neural. Dessa forma, se tivermos um neurônio i em uma camada e um neurônio j na camada seguinte, teremos um ligação com o peso Wij e um viés bij.&lt;/p&gt;

&lt;h3 id=&quot;funções-de-ativação&quot;&gt;&lt;em&gt;Funções de Ativação&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Também chamada de função de transferência, é o último processamento matemático que acontece antes da informação sair do neurônio. Esta equação matemática define se o neurônio será ativado ou não, podendo ser pode ser uma função degrau, uma função linear ou uma função não linear.&lt;/p&gt;

&lt;p&gt;A função de ativação mais simples seria a utilização de um degrau unitário. Onde o neurônio iria ativar somente caso a entrada fosse superior a um &lt;em&gt;threshold,&lt;/em&gt; e o sinal de entrada seria totalmente reproduzido na saída do nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/step-function.png&quot; alt=&quot;Step Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esta pode retornar valores de 0 e 1, utilizado em problemas de classificação, ou entre 0 e 1, utilizado em problemas que estamos mais interessados em saber a probabilidade de certa entrada fazer parte de certa classe.&lt;/p&gt;

&lt;h2 id=&quot;principais-tipos-de-redes-neurais-artificiais&quot;&gt;&lt;em&gt;Principais Tipos de Redes Neurais Artificiais&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Existem dois tipos principais de Redes Neurais: Redes Neurais Feedforward e Redes Neurais Recorrentes.&lt;/p&gt;

&lt;h3 id=&quot;redes-neurais-feedforward-rnf&quot;&gt;&lt;em&gt;Redes Neurais Feedforward (RNF)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Essa arquitetura é a mais comumente encontrada na literatura. Nela, a informação move-se em apenas uma direção: da entrada, passando pela camada oculta até o nós de saída, e não existem ciclos.&lt;/p&gt;

&lt;p&gt;A unidade mais simples dessa topologia é o Perceptron, a rede neural mais simples que é composta apenas por um nó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/perceptron.png&quot; alt=&quot;Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;O Perceptron&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Alguns problemas simples podem ser resolvidos com o Perceptron, pois ele só funciona com funções linearmente separáveis.&lt;/p&gt;

&lt;p&gt;Com a necessidade de resolver problemas mais complexos e a partir dessa unidade básica, surge o &lt;strong&gt;Perceptron Multicamadas (MLP)&lt;/strong&gt;. Composto por diversas camadas desses nós, sendo muito mais úteis e podendo aprender funções não lineares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/multi-layer-perceptron.png&quot; alt=&quot;Multilayer Perceptron&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Uma arquitetura de um Perceptron Multicamadas. Fonte:&lt;a href=&quot;https://www.researchgate.net/figure/Architecture-of-a-multilayer-perceptron-neural-network_fig5_316351306&quot;&gt;Advanced Methods for the Processing and Analysis of Multidimensional Signals: Application to Wind Speed&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;E por fim, temos as &lt;strong&gt;Redes Neurais Convolucionais (CNN)&lt;/strong&gt;, que são o exemplo mais comum das Redes Neurais Feedforward. Inspiradas no Córtex Visual, essa topologia divide os dados em pequenos pedaços e tentar aprender padrões essenciais. Essa operação é chamada de Convolução. Mais eficientes que os MLP, essa topologia é encontrada vastamente em aplicações de visão computacional, vídeo e linguagem natural. Essa topologia possui seus blocos característicos próprios, como as camadas de convolução e de &lt;em&gt;pooling.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/convolution-neural-network.png&quot; alt=&quot;Convolutional Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exemplo de Rede Neural Convolucional. Fonte: &lt;a href=&quot;https://missinglink.ai/guides/convolutional-neural-networks/convolutional-neural-network-tutorial-basic-advanced/&quot;&gt;Convolutional Neural Network Tutorial&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;redes-neurais-recorrentes-rnn&quot;&gt;&lt;em&gt;Redes Neurais Recorrentes (RNN)&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Diferente das Redes Neurais Feedforward, nas RNN a informação flui não somente para frente, mas para trás também, formando um ciclo. Para isso, assim como as CNN, elas usam diversos blocos próprios, como um bloco de memória por exemplo. Isso permite essa topologia capturar padrões dinâmicos temporais e serem vastamente utilizados em problemas de reconhecimento de voz e problemas que exigem uma ligação sequencial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/recorrent-neural-network.png&quot; alt=&quot;Recurrent Neural Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exemplo de Rede Neural Recorrente Fonte: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg&quot;&gt;wikimedia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;tipos-de-funções-de-ativação&quot;&gt;Tipos de Funções de Ativação&lt;/h2&gt;

&lt;p&gt;Além da função degrau unitário, que acredito não ser usada na prática, existem diversas outras funções de ativação. Além de determinarem a saída de um modelo, elas também ajudam na precisão dos resultados e na eficiência com que o modelo será treinado. Na prática, os modelos modernos usam funções de ativação não-linear, que são capazes de capturar padrões em dados mais complexos.&lt;/p&gt;

&lt;p&gt;As funções de ativação são usadas em dois momentos nas Redes Neurais: para processar a saída de um único neurônio, como vimos durante o tópico  de camadas ocultas, e para processar a saída da rede neural como um todo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/sigmoid.png&quot; alt=&quot;Sigmoid Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/tanh.png&quot; alt=&quot;Tanh Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/nn-intro/relu.png&quot; alt=&quot;Relu Formula and Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Normalmente são usadas funções &lt;em&gt;Rectified Linear Unit&lt;/em&gt; (ReLU) na prática. A função Sigmoid é normalmente utilizada para demostrar como esses elementos funcionam e normalmente é substituida pela Tangente Hiperbólica (TanH). Exceto no caso do problema tratar-se de uma classificação binária, nesse caso seria melhor uma função Sigmoid na saída do modelo pois está já entregaria o resultado entre 0 e 1.&lt;/p&gt;

&lt;p&gt;A escolha da função de ativação é motivada pelas características do problema que está sendo resolvido. A Sigmoid, por exemplo, apesar ter um gradient mais suave e normalizar a saída entre 0 e 1, tem problemas com &lt;em&gt;vanish gradients&lt;/em&gt; e sua saída não está centrada em zero. Já TanH tem o seu centro em zero, o que facilita o aprendizado das camadas seguintes, mas desvantagens parecidas com a Sigmoid.&lt;/p&gt;

&lt;p&gt;Além dessas, ainda podemos destacar:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Leaky ReLU&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Nesse artigo passamos por alguns dos principais conceitos de Redes Neurais Artificiais. Após essa revisão, espero que você já tenha uma idea mais concreta dos conceitos básicos que envolvem um dos principais tópicos de Aprendizagem Profunda. Entendendo os principais blocos construtores das RNA, as principais topologias e as funções de ativação mais comuns, podemos agora passar a tópicos mais avançados como &lt;strong&gt;Forward and Backward Propagation&lt;/strong&gt; e o &lt;strong&gt;Gradient Descent&lt;/strong&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matheus Jacques</name>
        
        
      </author>

      

      
        <category term="portuguese" />
      
        <category term="neural-networks" />
      

      
        <summary type="html">Aprenda os principais conceitos por trás das Redes Neurais, um dos pilares do Deep Learning.</summary>
      

      
      
    </entry>
  
</feed>
